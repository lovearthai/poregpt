#!/usr/bin/env python3
"""
å°†ç›®å½•ä¸‹æ¯ä¸ª .npy æ–‡ä»¶ï¼ˆå« dictï¼‰è½¬æ¢ä¸ºåŒåçš„ memmap å‹å¥½ .npyï¼Œ
å¹¶ç”Ÿæˆ shards.jsonï¼ˆæŒ‰ num_samples é™åºæ’åˆ—ï¼‰ã€‚
"""

import os
import sys
import json
import argparse
import numpy as np
from tqdm import tqdm
from multiprocessing import Pool, cpu_count


def convert_single_file(args):
    """
    è½¬æ¢å•ä¸ª .npy æ–‡ä»¶ã€‚
    è¿”å›: (filename, num_valid_chunks)
    """
    input_path, output_path, chunk_size, dtype = args
    print(input_path)
    try:
        data = np.load(input_path, allow_pickle=True)
        valid_chunks = []
        for item in tqdm(data):
            if 'chunk_data' in item and item['chunk_data'].shape[0] == chunk_size:
                valid_chunks.append(item['chunk_data'].astype(dtype))
        
        if not valid_chunks:
            arr = np.empty((0, chunk_size), dtype=dtype)
        else:
            arr = np.stack(valid_chunks, axis=0)
        
        np.save(output_path, arr)
        return (os.path.basename(output_path), len(valid_chunks))
    
    except Exception as e:
        print(f"âŒ Failed to process {input_path}: {e}", file=sys.stderr)
        return (os.path.basename(output_path), 0)


def main():
    parser = argparse.ArgumentParser(
        description="Convert each .npy file to memmap-friendly format and generate shards.json (sorted by num_samples descending)."
    )
    parser.add_argument("--input_dir", type=str, required=True, help="Input directory with original .npy files")
    parser.add_argument("--output_dir", type=str, required=True, help="Output directory for converted .npy files")
    parser.add_argument("--chunk_size", type=int, default=12000, help="Expected chunk length (default: 12000)")
    parser.add_argument("--dtype", type=str, default="float32", help="Output dtype (default: float32)")
    parser.add_argument("--num_workers", type=int, default=None, help="Number of parallel workers (default: auto)")
    args = parser.parse_args()

    # è§£æ dtype
    try:
        dtype = getattr(np, args.dtype) if isinstance(args.dtype, str) else args.dtype
    except AttributeError:
        raise ValueError(f"Invalid dtype: {args.dtype}")

    # è·å–è¾“å…¥æ–‡ä»¶åˆ—è¡¨ï¼ˆæ’åºä»¥ä¿è¯å¯é‡ç°ï¼‰
    input_files = sorted([
        f for f in os.listdir(args.input_dir)
        if f.endswith('.npy')
    ])
    if not input_files:
        raise ValueError(f"No .npy files found in {args.input_dir}")

    os.makedirs(args.output_dir, exist_ok=True)

    # æ„å»ºä»»åŠ¡
    tasks = [
        (
            os.path.join(args.input_dir, fname),
            os.path.join(args.output_dir, fname),
            args.chunk_size,
            dtype
        )
        for fname in input_files
    ]

    num_workers = args.num_workers or min(cpu_count(), len(tasks))

    print(f"ParallelGrouping {len(tasks)} files with {num_workers} workers...")
    
    with Pool(processes=num_workers) as pool:
        results = list(tqdm(
            pool.imap_unordered(convert_single_file, tasks),
            total=len(tasks),
            desc="Converting files"
        ))
    
    # æ„å»º shard_info å¹¶æŒ‰ num_samples é™åºæ’åº
    result_dict = dict(results)
    shard_info = []
    for fname in input_files:
        num_samples = result_dict.get(fname, 0)
        shard_info.append({
            "path": fname,
            "num_samples": num_samples
        })
    
    # ğŸ”» æŒ‰æ ·æœ¬æ•°é™åºæ’åˆ—ï¼ˆä»å¤§åˆ°å°ï¼‰ğŸ”»
    shard_info.sort(key=lambda x: x["num_samples"], reverse=True)

    total_samples = sum(info["num_samples"] for info in shard_info)

    # ä¿å­˜ shards.json
    meta_path = os.path.join(args.output_dir, "shards.json")
    with open(meta_path, 'w') as f:
        json.dump({
            "total_samples": total_samples,
            "chunk_size": args.chunk_size,
            "dtype": np.dtype(dtype).name,
            "shards": shard_info
        }, f, indent=2)

    print(f"\nğŸ‰ Conversion completed!")
    print(f"   Total files processed: {len(input_files)}")
    print(f"   Total samples:         {total_samples}")
    print(f"   Largest shard:         {shard_info[0]['num_samples']} samples")
    print(f"   Smallest shard:        {shard_info[-1]['num_samples']} samples")
    print(f"   Metadata saved to:     {meta_path}")


if __name__ == "__main__":
    main()
