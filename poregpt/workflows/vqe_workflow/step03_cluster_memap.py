# -*- coding: utf-8 -*-
"""
å¯¹ cnn_eval.py ç”Ÿæˆçš„ token embeddingsï¼ˆå•ä¸ª .npy memmap æ–‡ä»¶ï¼‰è¿›è¡Œ FAISS èšç±»ã€‚
- è¾“å…¥ï¼šä¸€ä¸ªå®Œæ•´çš„ .npy æ–‡ä»¶è·¯å¾„
- è‹¥ max_sampled_tokens == -1ï¼šåŠ è½½å…¨éƒ¨
- å¦åˆ™ï¼šåŠ è½½å‰ N ä¸ª tokens
"""

import os
import numpy as np
import faiss
import time
import argparse


def cluster_memmap_tokens(
    memmap_npy_path: str,
    output_prefix: str,
    max_sampled_tokens: int = -1,
    num_clusters: int = 16384,
    niter: int = 20,
    nredo: int = 100,
    max_points_per_centroid: int = 65536,
    seed: int = 42,
):
    print("ğŸ”§ Running with arguments:")
    print(f"    memmap_npy_path          = {memmap_npy_path}")
    print(f"    output_prefix            = {output_prefix}")
    print(f"    max_sampled_tokens       = {max_sampled_tokens} (-1 means load all)")
    print(f"    num_clusters             = {num_clusters}")
    print(f"    niter                    = {niter}")
    print(f"    nredo                    = {nredo}")
    print(f"    max_points_per_centroid  = {max_points_per_centroid}")
    print(f"    seed                     = {seed}")
    print("-" * 50)

    np.random.seed(seed)

    if not os.path.exists(memmap_npy_path):
        raise FileNotFoundError(f"âŒ File not found: {memmap_npy_path}")

    # æ‰“å¼€ memmapï¼ˆåªè¯»ï¼Œä¸åŠ è½½ï¼‰
    print(f"ğŸ“¥ Opening memmap file: {memmap_npy_path}")

    total_tokens = 1000000000      # â† å¿…é¡»æä¾›ï¼
    feature_dim =64       # â† å¿…é¡»æä¾›ï¼

    data = np.memmap(
        memmap_npy_path,
        dtype=np.float32,
        mode='r',
        shape=(total_tokens, feature_dim),
        order='C'
    )

    total_tokens, feature_dim = data.shape
    print(f"ğŸ“Š File shape: ({total_tokens:,}, {feature_dim})")

    # å†³å®šåŠ è½½æ•°é‡
    if max_sampled_tokens == -1:
        actual_total = total_tokens
        print("ğŸ”„ Loading ALL tokens (max_sampled_tokens = -1)")
    else:
        actual_total = min(total_tokens, max_sampled_tokens)
        print(f"ğŸ”„ Loading first {actual_total:,} tokens")

    # åŠ è½½åˆ°å†…å­˜ï¼ˆè½¬ä¸ºæ™®é€š arrayï¼‰
    #all_vectors = np.array(data[:actual_total], dtype=np.float32)
    #all_ids = np.arange(actual_total, dtype=np.int64)

    #ç›´æ¥åˆ‡ç‰‡å¹¶ç¡®ä¿æ˜¯ float32ï¼ˆmmap æœ¬èº«å·²æ˜¯ float32ï¼‰
    X = data[:actual_total]  # è¿™ä»ç„¶æ˜¯ä¸€ä¸ª memory-mapped array view
    print(f"âœ… Loaded {len(X):,} tokens into memory.")

# å…³é”®ï¼šFAISS éœ€è¦ C-contiguous array
# å¦‚æœ X ä¸æ˜¯ contiguousï¼ŒFAISS ä¼šæŠ¥é”™æˆ–é™é»˜å‡ºé”™
    if not X.flags.c_contiguous:
        print("âš ï¸  Data is not C-contiguous. Making a copy...")
        X = np.ascontiguousarray(X, dtype=np.float32)
    else:
    # ç¡®ä¿ dtype æ˜¯ float32ï¼ˆFAISS è¦æ±‚ï¼‰
       X = X.astype(np.float32, copy=False)

    print(f"âœ… Using {len(X):,} tokens for training (shape={X.shape}, contiguous={X.flags.c_contiguous})")
    # === L2 å½’ä¸€åŒ– ===
    norms = np.linalg.norm(X, axis=1, keepdims=True)
    print(f"Before normalization: mean norm = {np.mean(norms):.6f}")
    #all_vectors = all_vectors / norms
    #print(f"ğŸ“ After normalization: mean norm = {np.mean(np.linalg.norm(all_vectors, axis=1)):.6f}")

    # === FAISS KMeans èšç±» ===
    time1 = time.time()
    kmeans = faiss.Kmeans(
        d=feature_dim,
        k=num_clusters,
        niter=niter,
        nredo=nredo,
        verbose=True,
        gpu=True,
        spherical=False,
        max_points_per_centroid=max_points_per_centroid,
        min_points_per_centroid=1,
        seed=seed
    )

    print("ğŸš€ Training K-Means...")
    kmeans.train(X)
    time2 = time.time()
    print(f"â±ï¸  Training time: {time2 - time1:.2f}s")

    print("ğŸ” Assigning clusters...")
    distances, assignments = kmeans.assign(all_vectors)
    time3 = time.time()
    print(f"â±ï¸  Assignment time: {time3 - time2:.2f}s")

    # === ä¿å­˜ç»“æœ ===
    cluster_results = np.column_stack((all_ids, assignments, distances))
    output_file = f"{output_prefix}_clustered_k{num_clusters}.npy"
    np.save(output_file, cluster_results)
    print(f"ğŸ’¾ Cluster results saved to: {output_file}")

    centroids_file_npy = f"{output_prefix}_centroids_k{num_clusters}.npy"
    np.save(centroids_file_npy, kmeans.centroids)
    print(f"ğŸ’¾ Centroids saved to: {centroids_file_npy}")

    try:
        import h5py
        centroids_file_h5 = f"{output_prefix}_centroids_k{num_clusters}.h5"
        with h5py.File(centroids_file_h5, 'w') as f:
            f.create_dataset("centroids", data=kmeans.centroids)
        print(f"ğŸ’¾ Centroids also saved to: {centroids_file_h5}")
    except ImportError:
        pass

    print("ğŸ‰ Done!")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Cluster tokens from a single .npy memmap file.")
    parser.add_argument("--memmap_npy_path", type=str, required=True,
                        help="Full path to the .npy memmap file (e.g., /data/merged_features.npy)")
    parser.add_argument("--output_prefix", type=str, default="cluster",
                        help="Prefix for output files")
    parser.add_argument("--max_sampled_tokens", type=int, default=-1,
                        help="Number of tokens to load; -1 means load all")
    parser.add_argument("--num_clusters", type=int, default=16384)
    parser.add_argument("--niter", type=int, default=100)
    parser.add_argument("--nredo", type=int, default=10)
    parser.add_argument("--max_points_per_centroid", type=int, default=65536)
    parser.add_argument("--seed", type=int, default=42)

    args = parser.parse_args()
    cluster_memmap_tokens(**vars(args))
