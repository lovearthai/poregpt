# -*- coding: utf-8 -*-
"""
å¯¹ cnn_eval.py ç”Ÿæˆçš„ token embeddingsï¼ˆmemmap æ ¼å¼ï¼‰è¿›è¡Œ FAISS èšç±»ã€‚
ä¸å†é‡‡æ ·ï¼Œç›´æ¥ä½¿ç”¨æŒ‡å®šçš„ shards_xxx.json æ–‡ä»¶ä¸­çš„æ‰€æœ‰ shardsã€‚
ã€å·²ä¼˜åŒ–ï¼šé¢„åˆ†é…æ•°ç»„é¿å… list append + concatã€‘
"""

import os
import json
import numpy as np
import faiss
import time
import argparse
from tqdm import tqdm


def cluster_memmap_tokens_from_shards_json(
    feature_shards_dir: str,
    shards_json: str,
    output_prefix: str,
    max_sampled_tokens: int = 1000_000_000,
    num_clusters: int = 16384,
    niter: int = 20,
    nredo: int = 100,
    max_points_per_centroid: int = 65536,
    seed: int = 42,
):
    # === æ‰“å°å‚æ•° ===
    print("ğŸ”§ Running with arguments:")
    print(f"    feature_shards_dir       = {feature_shards_dir}")
    print(f"    shards_json              = {shards_json}")
    print(f"    output_prefix            = {output_prefix}")
    print(f"    max_sampled_tokens       = {max_sampled_tokens:,}")
    print(f"    num_clusters             = {num_clusters}")
    print(f"    niter                    = {niter}")
    print(f"    nredo                    = {nredo}")
    print(f"    max_points_per_centroid  = {max_points_per_centroid}")
    print(f"    seed                     = {seed}")
    print("-" * 50)

    np.random.seed(seed)
    rng = np.random.default_rng(seed)

    shards_json_path = os.path.join(feature_shards_dir, shards_json)
    assert os.path.exists(shards_json_path), f"âŒ Missing {shards_json_path}"

    with open(shards_json_path, 'r') as f:
        meta = json.load(f)

    total_tokens = meta["total_tokens"]
    feature_dim = meta["feature_dim"]
    shards_info = meta["shards"]

    print(f"ğŸ“Š Total tokens in {shards_json}: {total_tokens:,}, dim: {feature_dim}")

    # === è®¡ç®—å®é™…è¦åŠ è½½çš„ token æ€»æ•° ===
    actual_total = min(total_tokens, max_sampled_tokens)

    # é¢„åˆ†é…å¤§æ•°ç»„
    all_vectors = np.empty((actual_total, feature_dim), dtype=np.float32)
    all_ids = np.empty(actual_total, dtype=np.int64)

    offset = 0
    global_token_offset = 0
    pbar = tqdm(total=len(shards_info), desc="Loading shards")

    for shard in shards_info:
        if offset >= actual_total:
            break

        num_tokens_in_shard = shard["num_tokens"]
        shard_file = os.path.join(feature_shards_dir, shard["shard_file"])

        # åªåŠ è½½éœ€è¦çš„éƒ¨åˆ†ï¼ˆå¦‚æœæœ€åä¸€ shard è¶…å‡ºä¸Šé™ï¼‰
        tokens_needed = actual_total - offset
        tokens_to_load = min(num_tokens_in_shard, tokens_needed)

        # ç›´æ¥ä» memmap è¯»å–å­é›†ï¼ˆä¸è½¬æˆå®Œæ•´ arrayï¼‰
        data = np.memmap(shard_file, dtype='float32', mode='r', shape=(num_tokens_in_shard, feature_dim))
        all_vectors[offset:offset + tokens_to_load] = data[:tokens_to_load]
        all_ids[offset:offset + tokens_to_load] = np.arange(
            global_token_offset,
            global_token_offset + tokens_to_load,
            dtype=np.int64
        )

        offset += tokens_to_load
        global_token_offset += num_tokens_in_shard
        pbar.update(1)

    pbar.close()

    # è£å‰ªï¼ˆç†è®ºä¸Šä¸éœ€è¦ï¼Œä½†ä¿é™©ï¼‰
    if offset < actual_total:
        all_vectors = all_vectors[:offset]
        all_ids = all_ids[:offset]

    print(f"âœ… Loaded {len(all_vectors):,} tokens directly into pre-allocated array.")

    # === L2 å½’ä¸€åŒ–ï¼šä¸º spherical K-Means åšå‡†å¤‡ ===
    #norms = np.linalg.norm(all_vectors, axis=1, keepdims=True)


    # é˜²æ­¢é™¤é›¶ï¼šå°†æå°èŒƒæ•°è®¾ä¸º1ï¼ˆå®é™…ä¸­å¾ˆå°‘å‘ç”Ÿï¼Œä½†å®‰å…¨èµ·è§ï¼‰
    #norms = np.where(norms == 0, 1.0, norms)
    # æ‰“å°å½’ä¸€åŒ–å‰åçš„èŒƒæ•°ç»Ÿè®¡
    #print(f"Before normalization: mean norm = {np.mean(np.linalg.norm(all_vectors, axis=1)):.6f}")
    #all_vectors = all_vectors / norms
    #print(f"ğŸ“ L2 normalized vectors (mean norm: {np.mean(np.linalg.norm(all_vectors, axis=1)):.6f})")

    # === FAISS KMeans èšç±» ===
    time1 = time.time()
    kmeans = faiss.Kmeans(
        d=feature_dim,
        k=num_clusters,
        niter=niter,
        nredo=nredo,
        verbose=True,
        gpu=True,
        spherical=False,
        max_points_per_centroid=max_points_per_centroid,
        min_points_per_centroid=1,
        seed=seed
    )

    print("ğŸš€ Training K-Means...")
    kmeans.train(all_vectors)
    time2 = time.time()
    print(f"â±ï¸  Training time: {time2 - time1:.2f}s")

    print("ğŸ” Assigning clusters...")
    distances, assignments = kmeans.assign(all_vectors)
    time3 = time.time()
    print(f"â±ï¸  Assignment time: {time3 - time2:.2f}s")

    # === ä¿å­˜ç»“æœ ===
    cluster_results = np.column_stack((all_ids, assignments, distances))
    output_file = f"{output_prefix}_clustered_k{num_clusters}.npy"
    np.save(output_file, cluster_results)
    print(f"ğŸ’¾ Cluster results saved to: {output_file}")

    centroids_file_npy = f"{output_prefix}_centroids_k{num_clusters}.npy"
    np.save(centroids_file_npy, kmeans.centroids)
    print(f"ğŸ’¾ Centroids saved to: {centroids_file_npy}")

    try:
        import h5py
        centroids_file_h5 = f"{output_prefix}_centroids_k{num_clusters}.h5"
        with h5py.File(centroids_file_h5, 'w') as f:
            f.create_dataset("centroids", data=kmeans.centroids)
        print(f"ğŸ’¾ Centroids also saved to: {centroids_file_h5}")
    except ImportError:
        pass

    print("ğŸ‰ Done!")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Cluster tokens from a specified shards JSON file (no sampling).")
    parser.add_argument("--feature_shards_dir", type=str, required=True,
                        help="Directory containing shard_*.npy files")
    parser.add_argument("--shards_json", type=str, default="shards.json",
                        help="Name of the shards metadata file (e.g., shards_sampled_20p.json)")
    parser.add_argument("--output_prefix", type=str, default="cluster",
                        help="Prefix for output files")
    parser.add_argument("--max_sampled_tokens", type=int, default=10_000_000,
                        help="Maximum number of tokens to load into memory (default: 10M)")
    parser.add_argument("--num_clusters", type=int, default=16384,
                        help="Number of K-Means clusters")
    parser.add_argument("--niter", type=int, default=100)
    parser.add_argument("--nredo", type=int, default=10)
    parser.add_argument("--max_points_per_centroid", type=int, default=65536)
    parser.add_argument("--seed", type=int, default=42)

    args = parser.parse_args()
    cluster_memmap_tokens_from_shards_json(**vars(args))
