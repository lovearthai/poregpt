

def process_basecall_corpus(fast5_dir, csv_path, vqetokenizer, nanopore_signal_process_strategy="apple"):
    """
    è¯»å– CSV æ–‡ä»¶ï¼Œä» FAST5 æ–‡ä»¶ä¸­æå–ç‰‡æ®µï¼Œå¯¹å…¶è¿›è¡Œæ ‡è®°åŒ–ï¼Œ
    å¹¶å°†ç»“æœæŒ‰ FAST5 æ–‡ä»¶ååˆ†ç»„ä¿å­˜åˆ° JSONL.GZ æ–‡ä»¶ä¸­ã€‚

    Args:
        fast5_dir (str): åŒ…å« FAST5 æ–‡ä»¶çš„ç›®å½•è·¯å¾„ã€‚
        csv_path (str): è¾“å…¥ CSV æ–‡ä»¶çš„è·¯å¾„ã€‚
        vqetokenizer: ä¸€ä¸ªé¢„å®šä¹‰çš„ VQE tokenizer ç±»å®ä¾‹ï¼Œå…·æœ‰ tokenize_chunk æ–¹æ³•ã€‚
        nanopore_signal_process_strategy (str): ä¿¡å·å¤„ç†ç­–ç•¥ã€‚
    """

    # è¯»å– CSV æ–‡ä»¶
    print(f"ğŸ“– æ­£åœ¨è¯»å– CSV: {csv_path}")
    df = pd.read_csv(csv_path)
    print(f"ğŸ“Š ä» CSV åŠ è½½äº† {len(df)} è¡Œã€‚")

    # æŒ‰ fast5 æ–‡ä»¶ååˆ†ç»„ï¼Œä»¥ä¾¿é«˜æ•ˆå¤„ç†
    grouped_df = df.groupby('fast5')
    print(f"ğŸ“ æ‰¾åˆ° {len(grouped_df)} ä¸ªå”¯ä¸€çš„ FAST5 æ–‡ä»¶ã€‚")

    for fast5_filename, group in grouped_df:
        fast5_path = os.path.join(fast5_dir, fast5_filename)

        if not os.path.exists(fast5_path):
            print(f"âŒ æœªæ‰¾åˆ° FAST5 æ–‡ä»¶: {fast5_path}")
            continue # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è·³è¿‡

        output_jsonl_gz_path = os.path.join(fast5_dir, f"{os.path.splitext(fast5_filename)[0]}.jsonl.gz")
        print(f"ğŸ”„ æ­£åœ¨å¤„ç† FAST5: {fast5_filename} -> {os.path.basename(output_jsonl_gz_path)}")

        results_for_this_fast5 = []

        # ä¸ºè¯¥ç»„ä¸­çš„æ‰€æœ‰ reads ä¸€æ¬¡æ€§æ‰“å¼€ FAST5 æ–‡ä»¶
        with get_fast5_file(fast5_path, mode="r") as f5:
            for _, row in group.iterrows():
                read_id = row['read_id']
                chunk_start = int(row['chunk_start']) # ç¡®ä¿ä¸ºæ•´æ•°
                chunk_size = int(row['chunk_size'])   # ç¡®ä¿ä¸ºæ•´æ•°
                bases = row['bases']

                try:
                    # åœ¨ FAST5 æ–‡ä»¶ä¸­æŸ¥æ‰¾ç‰¹å®šçš„ read
                    read = f5.get_read(read_id)
                    if read is None:
                        print(f"    âš ï¸  åœ¨ {fast5_filename} ä¸­æœªæ‰¾åˆ° Read ID {read_id}ã€‚æ­£åœ¨è·³è¿‡ã€‚")
                        continue

                    # --- æå–åŸå§‹ä¿¡å· ---
                    channel_info = read.handle[read.global_key + 'channel_id'].attrs
                    offset = int(channel_info['offset'])
                    scaling = channel_info['range'] / channel_info['digitisation']
                    raw = read.handle[read.raw_dataset_name][:]
                    signal_raw = np.array(scaling * (raw + offset), dtype=np.float32)

                    # --- åº”ç”¨å¤„ç†ç­–ç•¥ ---
                    signal_processed = nanopore_process_signal(signal_raw, nanopore_signal_process_strategy)

                    # --- æå–ç‰‡æ®µ (Chunk) ---
                    # æ ¹æ®å¼€å§‹ä½ç½®å’Œå¤§å°è®¡ç®—ç»“æŸç´¢å¼•
                    chunk_end = chunk_start + chunk_size
                    # ç¡®ä¿ä¸è¶…å‡ºä¿¡å·é•¿åº¦èŒƒå›´
                    if chunk_end > len(signal_processed):
                         print(f"    âš ï¸  ç‰‡æ®µ ({chunk_start}:{chunk_end}) è¶…å‡ºä¿¡å·é•¿åº¦ ({len(signal_processed)})ï¼Œread ID ä¸º {read_id}ã€‚æ­£åœ¨è·³è¿‡ã€‚")
                         continue

                    chunk_signal = signal_processed[chunk_start:chunk_end]

                    # --- æ ‡è®°åŒ–ç‰‡æ®µ ---
                    # è°ƒç”¨æä¾›çš„ vqetokenizer å®ä¾‹çš„ tokenize_chunk æ–¹æ³•
                    print(f"    ğŸ”¤ æ­£åœ¨æ ‡è®°åŒ– read {read_id} çš„ç‰‡æ®µ {chunk_start}-{chunk_end} (é•¿åº¦: {len(chunk_signal)})")
                    text = vqetokenizer.tokenize_chunk(chunk_signal)

                    # --- å‡†å¤‡ç»“æœæ¡ç›® ---
                    result_entry = {
                        "fast5": fast5_filename,
                        "read_id": read_id,
                        "chunk_start": chunk_start,
                        "chunk_size": chunk_size,
                        "bases": bases,
                        "text": text
                    }
                    results_for_this_fast5.append(result_entry)

                except Exception as e:
                    print(f"    âŒ å¤„ç† {fast5_filename} ä¸­çš„ read {read_id} (ç‰‡æ®µ {chunk_start}-{chunk_start+chunk_size}) æ—¶å‡ºé”™: {e}")
                    continue # ç»§ç»­å¤„ç†æ­¤ FAST5 çš„ä¸‹ä¸€è¡Œ

        # --- å°†æ­¤ FAST5 çš„ç»“æœå†™å…¥ JSONL.GZ ---
        print(f"ğŸ’¾ æ­£åœ¨å°† {len(results_for_this_fast5)} æ¡ç»“æœå†™å…¥ {os.path.basename(output_jsonl_gz_path)}")
        with gzip.open(output_jsonl_gz_path, 'wt', encoding='utf-8') as gz_file:
            for item in results_for_this_fast5:
                gz_file.write(json.dumps(item) + '\n')

    print("ğŸ‰ æ‰€æœ‰å¤„ç†å®Œæˆï¼")
