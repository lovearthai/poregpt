
from .signal_normal import nanopore_normalize, nanopore_filter_signal
import faiss
import gzip
import json
from tqdm import tqdm
from ont_fast5_api.fast5_interface import get_fast5_file
import numpy as np

class KmeansTokenizer:
    """
    Nanopore RVQ Tokenizer å°è£…ç±»ã€‚

    åŠŸèƒ½ï¼š
        - åŠ è½½é¢„è®­ç»ƒ RVQ æ¨¡å‹
        - tokenize å•ä¸ª read / numpy ä¿¡å· / æ•´ä¸ª FAST5 ç›®å½•
    """

    def __init__(
        self,
        centroids_path: str,
    ):
        """
        åˆå§‹åŒ– tokenizerã€‚
        """
        data = np.load(centroids_path, allow_pickle=True).item()
        self.window_size = data["dimension"]
        self.stride = data["stride"]
        self.index = self._init_worker(data["centroids"])

    def _init_worker(self, centroids):
        d = centroids.shape[1]
        if hasattr(faiss, 'StandardGpuResources'):
        # === GPU æ¨¡å¼ ===
            print("ğŸš€ Initializing FAISS GPU index...")
            res = faiss.StandardGpuResources()  # GPU èµ„æºç®¡ç†å™¨
            cpu_index = faiss.IndexFlatL2(d)
            cpu_index.add(centroids) # type: ignore
            # å°† CPU ç´¢å¼•æ¬åˆ° GPUï¼ˆé»˜è®¤ device=0ï¼‰
            index = faiss.index_cpu_to_gpu(res, 0, cpu_index)
        else:
            # === CPU å›é€€æ¨¡å¼ ===
            print("ğŸ’» Using FAISS CPU index...")
            cpu_index = faiss.IndexFlatL2(d)
            cpu_index.add(centroids) # type: ignore
            index = cpu_index
        return index
    
    def _sliding_window_chunks(self, signal):
        """
        å¯¹ä¸€ç»´ä¿¡å·è¿›è¡Œæ»‘åŠ¨çª—å£åˆ‡ç‰‡ã€‚

        Args:
            signal (np.ndarray): ä¸€ç»´å½’ä¸€åŒ–ä¿¡å·
            window_size (int): çª—å£é•¿åº¦
            stride (int): æ­¥é•¿

        Returns:
            list of tuples: æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªä¸‰å…ƒç»„ (start, end, vector)ï¼Œå…¶ä¸­ï¼š
                            - start æ˜¯åˆ‡ç‰‡åœ¨åŸå§‹ä¿¡å·ä¸­çš„èµ·å§‹ç´¢å¼•
                            - end æ˜¯åˆ‡ç‰‡åœ¨åŸå§‹ä¿¡å·ä¸­çš„ç»“æŸç´¢å¼•ï¼ˆä¸åŒ…å«ï¼‰
                            - vector æ˜¯åˆ‡ç‰‡æœ¬èº«çš„å€¼
        """
        n_points = len(signal)
        if n_points < self.window_size:
            return []

        chunks_info = []
        start = 0
        while start + self.window_size <= n_points:
            end = start + self.window_size
            chunk = signal[start:end]
            chunks_info.append((start, end, chunk))
            start += self.stride
        return chunks_info

    def tokenize_data(self, signal: np.ndarray) -> str:
        # Normalize
        norm_sig_no_filter = nanopore_normalize(signal)
        norm_sig = nanopore_filter_signal(norm_sig_no_filter) # è¿›è¡Œå»å™ªå¤„ç†
        if norm_sig.size == 0:
            return ""
        vec_list = []
        chunks_info = self._sliding_window_chunks(norm_sig)
        for _, _, chunk in chunks_info:
            if chunk.size == 0:
                continue
            vec_list.append(chunk)
        if not vec_list:
            return ""
        try:
            X = np.stack(vec_list, axis=0).astype(np.float32)
        except Exception:
            return ""
        _, I = self.index.search(X, 1) # type: ignore
        cluster_ids = I[:, 0].tolist()

        tokens = ''.join(f"<|bwav:{int(cid)}|>" for cid in cluster_ids)

        return tokens


    def tokenize_read(self, read) -> str:
        """
        ç›´æ¥ tokenize ä¸€ä¸ª ont_fast5_api read å¯¹è±¡ï¼Œè¿”å›æ ¼å¼åŒ– token å­—ç¬¦ä¸²ã€‚

        Args:
            read: fast5 read object
            token_type: "L1", "L2", "L3", or "L4"

        Returns:
            str: formatted token string
        """
        # --- Scale ---
        channel_info = read.handle[read.global_key + 'channel_id'].attrs
        offset = int(channel_info['offset'])
        scaling = channel_info['range'] / channel_info['digitisation']
        raw = read.handle[read.raw_dataset_name][:]
        scaled = np.array(scaling * (raw + offset), dtype=np.float32)

        return self.tokenize_data(scaled)

 
    def tokenize_fast5(self, fast5_path: str, output_path: str):
        print(f"âœ… Process {fast5_path}")
        """å†…éƒ¨æ–¹æ³•ï¼šå¤„ç†å•ä¸ª FAST5 â†’ JSONL.GZ"""
        results = []
        with get_fast5_file(fast5_path, mode="r") as f5:
            for read in tqdm(f5.get_reads()):
                try:
                    token_str = self.tokenize_read(read)
    
                    results.append({
                        "id": read.read_id,
                        "text": token_str
                    })
                except Exception as e:
                    print(f"âŒ Error on read {read.read_id} in {fast5_path}: {e}")
                    continue
    
        # Save
        with gzip.open(output_path, 'wt', encoding='utf-8') as f:
            for item in results:
                f.write(json.dumps(item) + '\n')
        print(f"âœ… Wrote {len(results)} reads to {output_path}")
