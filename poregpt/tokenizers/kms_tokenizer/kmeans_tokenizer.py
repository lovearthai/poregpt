
from ...utils.signal import nanopore_process_signal
import faiss
import gzip
import json
from tqdm import tqdm
from ont_fast5_api.fast5_interface import get_fast5_file
import numpy as np
from abc import ABC, abstractmethod

# åŸºç±»ï¼šæŠ½è±¡ç±»
class InterfaceTokenizer(ABC):
    @abstractmethod
    def tokenize_data(self, signal: np.ndarray) -> list:
        """å°†åŸå§‹ä¿¡å·æ•°æ®è½¬æ¢ä¸º token å­—ç¬¦ä¸²"""
        pass

    @abstractmethod
    def tokenize_read(self, read, nanopore_signal_process_strategy="apple") -> list:
        """å°†æµ‹åºè¯»æ®µï¼ˆreadï¼‰å¯¹è±¡è½¬æ¢ä¸º token å­—ç¬¦ä¸²"""
        pass

    @abstractmethod
    def tokenize_fast5(self, fast5_path: str, output_path:str, nanopore_signal_process_strategy="apple"):
        """ä» FAST5 æ–‡ä»¶ä¸­è¯»å–ä¿¡å·å¹¶ä¿å­˜ token åˆ°è¾“å‡ºè·¯å¾„"""
        pass

class KmeansTokenizer(InterfaceTokenizer):
    """
    Nanopore RVQ Tokenizer å°è£…ç±»ã€‚

    åŠŸèƒ½ï¼š
        - åŠ è½½é¢„è®­ç»ƒ RVQ æ¨¡å‹
        - tokenize å•ä¸ª read / numpy ä¿¡å· / æ•´ä¸ª FAST5 ç›®å½•
    """

    def __init__(
        self,
        centroids_path: str,
    ):
        """
        åˆå§‹åŒ– tokenizerã€‚
        """
        data = np.load(centroids_path, allow_pickle=True).item()
        self.window_size = data["dimension"]
        self.stride = data["stride"]
        self.index = self._init_worker(data["centroids"])

    def _init_worker(self, centroids):
        d = centroids.shape[1]
        if hasattr(faiss, 'StandardGpuResources'):
        # === GPU æ¨¡å¼ ===
            print("ğŸš€ Initializing FAISS GPU index...")
            res = faiss.StandardGpuResources()  # GPU èµ„æºç®¡ç†å™¨
            cpu_index = faiss.IndexFlatL2(d)
            cpu_index.add(centroids) # type: ignore
            # å°† CPU ç´¢å¼•æ¬åˆ° GPUï¼ˆé»˜è®¤ device=0ï¼‰
            index = faiss.index_cpu_to_gpu(res, 0, cpu_index)
        else:
            # === CPU å›é€€æ¨¡å¼ ===
            print("ğŸ’» Using FAISS CPU index...")
            cpu_index = faiss.IndexFlatL2(d)
            cpu_index.add(centroids) # type: ignore
            index = cpu_index
        return index
    
    def _sliding_window_chunks(self, signal):
        """
        å¯¹ä¸€ç»´ä¿¡å·è¿›è¡Œæ»‘åŠ¨çª—å£åˆ‡ç‰‡ã€‚

        Args:
            signal (np.ndarray): ä¸€ç»´å½’ä¸€åŒ–ä¿¡å·
            window_size (int): çª—å£é•¿åº¦
            stride (int): æ­¥é•¿

        Returns:
            list of tuples: æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªä¸‰å…ƒç»„ (start, end, vector)ï¼Œå…¶ä¸­ï¼š
                            - start æ˜¯åˆ‡ç‰‡åœ¨åŸå§‹ä¿¡å·ä¸­çš„èµ·å§‹ç´¢å¼•
                            - end æ˜¯åˆ‡ç‰‡åœ¨åŸå§‹ä¿¡å·ä¸­çš„ç»“æŸç´¢å¼•ï¼ˆä¸åŒ…å«ï¼‰
                            - vector æ˜¯åˆ‡ç‰‡æœ¬èº«çš„å€¼
        """
        n_points = len(signal)
        if n_points < self.window_size:
            return []

        chunks_info = []
        start = 0
        while start + self.window_size <= n_points:
            end = start + self.window_size
            chunk = signal[start:end]
            chunks_info.append((start, end, chunk))
            start += self.stride
        return chunks_info

    def tokenize_data(self, signal: np.ndarray) -> list:
        if signal.size == 0:
            return []
        vec_list = []
        chunks_info = self._sliding_window_chunks(signal)
        for _, _, chunk in chunks_info:
            if chunk.size == 0:
                continue
            vec_list.append(chunk)
        if not vec_list:
            return []
        try:
            X = np.stack(vec_list, axis=0).astype(np.float32)
        except Exception:
            return []
        _, I = self.index.search(X, 1) # type: ignore
        cluster_ids = I[:, 0].tolist()

        parts = []
        for token_id in cluster_ids:
            parts.append(f"<|bwav:{int(token_id)}|>")
        return parts


    def tokenize_read(self, read, nanopore_signal_process_strategy="apple") -> list:
        try:
            channel_info = read.handle[read.global_key + 'channel_id'].attrs
            offset = int(channel_info['offset'])
            scaling = channel_info['range'] / channel_info['digitisation']
            raw = read.handle[read.raw_dataset_name][:]
            signal_raw = np.array(scaling * (raw + offset), dtype=np.float32)
            signal_processed = nanopore_process_signal(signal_raw,nanopore_signal_process_strategy)
            return self.tokenize_data(signal_processed)
        except Exception as e:
            fast5_path = getattr(read.handle, 'filename', 'unknown.fast5')
            print(f"âŒ Error on read {read.read_id} in {fast5_path}: {e}")
            return []

 
    def tokenize_fast5(self, fast5_path: str, output_path:str, nanopore_signal_process_strategy="apple"):
        print(f"âœ… Processing {fast5_path} with strategy{nanopore_signal_process_strategy}")
        results = []
        with get_fast5_file(fast5_path, mode="r") as f5:
            for read in tqdm(f5.get_reads(), desc=os.path.basename(fast5_path)):
                try:
                    token_list = self.tokenize_read(read,nanopore_signal_process_strategy)
                    token_str = "".join(token_list)
                    results.append({"id": read.read_id, "text": token_str})
                except Exception as e:
                    print(f"âŒ Failed on read {read.read_id}: {e}")
                    continue

        with gzip.open(output_path, 'wt', encoding='utf-8') as f:
            for item in results:
                f.write(json.dumps(item) + '\n')
        print(f"âœ… Wrote {len(results)} reads to {output_path}")