import os
import gzip
import json
import numpy as np
import pandas as pd  # æ·»åŠ  pandas å¯¼å…¥
from pathlib import Path
from tqdm import tqdm
import argparse
# import yaml  # ç§»é™¤ yaml å¯¼å…¥ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å†ä½¿ç”¨ YAML
from ont_fast5_api.fast5_interface import get_fast5_file # å‡è®¾ ont_fast5_api å·²å®‰è£…
from .kms_tokenizer import KMSTokenizer
import torch
from ...utils.signal import nanopore_process_signal
import time

def process_single_fast5(fast5_path, csv_path, model_path, device, nanopore_signal_process_strategy="apple"):
    """
    å¤„ç†å•ä¸ª FAST5 æ–‡ä»¶åŠå…¶å¯¹åº”çš„ CSV æ–‡ä»¶ã€‚

    Args:
        fast5_path (str): å•ä¸ª FAST5 æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ã€‚
        csv_path (str): å¯¹åº”çš„è¾“å…¥ CSV æ–‡ä»¶çš„è·¯å¾„ã€‚
        model_path (str): VQE æ¨¡å‹çš„è·¯å¾„ã€‚
        device (str): ç”¨äº VQE tokenizer çš„è®¾å¤‡ ('cpu', 'cuda', 'cuda:0', etc.)ã€‚
        nanopore_signal_process_strategy (str): ä¿¡å·å¤„ç†ç­–ç•¥ã€‚
    """
    print(f"ğŸ“– æ­£åœ¨è¯»å– FAST5: {fast5_path}")
    print(f"ğŸ“– æ­£åœ¨è¯»å– CSV: {csv_path}")

    # åˆå§‹åŒ– tokenizer å¹¶æŒ‡å®šè®¾å¤‡
    tokenizer = KMSTokenizer(
        model_ckpt=model_path,
        token_batch_size=8000
    )

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(fast5_path):
        print(f"âŒ æœªæ‰¾åˆ° FAST5 æ–‡ä»¶: {fast5_path}")
        return
    if not os.path.exists(csv_path):
        print(f"âŒ æœªæ‰¾åˆ° CSV æ–‡ä»¶: {csv_path}")
        return

    # è¯»å– CSV æ–‡ä»¶
    df = pd.read_csv(csv_path)
    print(f"ğŸ“Š ä» CSV åŠ è½½äº† {len(df)} è¡Œã€‚")

    # --- é‡è¦æ”¹åŠ¨ï¼šæ£€æŸ¥ CSV ä¸­çš„ fast5 æ–‡ä»¶åæ˜¯å¦ä¸ä¼ å…¥çš„ fast5_path åŒ¹é… ---
    unique_fast5_names_in_csv = df['fast5'].unique()
    expected_fast5_filename = os.path.basename(fast5_path)
    if len(unique_fast5_names_in_csv) != 1 or unique_fast5_names_in_csv[0] != expected_fast5_filename:
         print(f"âš ï¸  CSV æ–‡ä»¶ä¸­çš„ fast5 åç§° ({unique_fast5_names_in_csv}) ä¸ä¼ å…¥çš„ FAST5 æ–‡ä»¶å ({expected_fast5_filename}) ä¸åŒ¹é…æˆ–ä¸å”¯ä¸€ã€‚")
         print(f"     ç¡®ä¿ CSV æ–‡ä»¶åªåŒ…å«æ¥è‡ª {expected_fast5_filename} çš„æ•°æ®ã€‚")
         return # æˆ–è€…æ ¹æ®éœ€è¦æŠ›å‡ºå¼‚å¸¸


    # è·å–è¾“å‡ºè·¯å¾„ (åŸºäº FAST5 æ–‡ä»¶è·¯å¾„ï¼Œæ›¿æ¢æ‰©å±•åä¸º .jsonl.gz)
    output_jsonl_gz_path = os.path.splitext(fast5_path)[0] + '.jsonl.gz'
    print(f"ğŸ”„ æ­£åœ¨å¤„ç† FAST5: {os.path.basename(fast5_path)} -> {os.path.basename(output_jsonl_gz_path)}")

    results_for_this_fast5 = []

    # æ€»ä½“è¿›åº¦æ¡
    total_rows = len(df)
    overall_pbar = tqdm(total=total_rows, desc="Processing Chunks", unit="chunk")

    # æŒ‰ read_id åˆ†ç»„
    grouped_by_read = df.groupby('read_id')

    with get_fast5_file(fast5_path, mode="r") as f5:
        for read_id, group_df_by_read in grouped_by_read:
            # å¯¹äºå½“å‰ read_idï¼Œåªæå–å’Œå¤„ç†ä¸€æ¬¡ä¿¡å·
            try:
                # åœ¨ FAST5 æ–‡ä»¶ä¸­æŸ¥æ‰¾ç‰¹å®šçš„ read
                read = f5.get_read(read_id)
                if read is None:
                    print(f"    âš ï¸  åœ¨ {os.path.basename(fast5_path)} ä¸­æœªæ‰¾åˆ° Read ID {read_id}ã€‚æ­£åœ¨è·³è¿‡æ­¤ read çš„æ‰€æœ‰ chunksã€‚")
                    # æ›´æ–°è¿›åº¦æ¡ï¼Œè·³è¿‡è¿™ä¸ª read çš„æ‰€æœ‰ chunks
                    for _ in group_df_by_read.itertuples():
                        overall_pbar.update(1)
                    continue

                # --- æå–åŸå§‹ä¿¡å· (ä»…ä¸€æ¬¡) ---
                channel_info = read.handle[read.global_key + 'channel_id'].attrs
                offset = int(channel_info['offset'])
                scaling = channel_info['range'] / channel_info['digitisation']
                raw = read.handle[read.raw_dataset_name][:]
                signal_raw = np.array(scaling * (raw + offset), dtype=np.float32)

                # --- åº”ç”¨å¤„ç†ç­–ç•¥ (ä»…ä¸€æ¬¡) ---
                signal_processed = nanopore_process_signal(signal_raw, nanopore_signal_process_strategy)

                # --- å¤„ç†æ­¤ read_id çš„æ‰€æœ‰ chunks ---
                for _, row in group_df_by_read.iterrows():
                    chunk_start = int(row['chunk_start']) # ç¡®ä¿ä¸ºæ•´æ•°
                    chunk_size = int(row['chunk_size'])   # ç¡®ä¿ä¸ºæ•´æ•°
                    bases = row['bases']

                    # --- æå–ç‰‡æ®µ (Chunk) ---
                    chunk_end = chunk_start + chunk_size
                    # ç¡®ä¿ä¸è¶…å‡ºä¿¡å·é•¿åº¦èŒƒå›´
                    if chunk_end > len(signal_processed):
                         print(f"    âš ï¸  Read {read_id}: ç‰‡æ®µ ({chunk_start}:{chunk_end}) è¶…å‡ºä¿¡å·é•¿åº¦ ({len(signal_processed)})ã€‚æ­£åœ¨è·³è¿‡ã€‚")
                         overall_pbar.update(1)
                         continue

                    chunk_signal = signal_processed[chunk_start:chunk_end]

                    # --- æ ‡è®°åŒ–ç‰‡æ®µ ---
                    # time0 = time.time() # å¯é€‰ï¼šå–æ¶ˆæ³¨é‡Šä»¥æµ‹é‡æ—¶é—´
                    tokens = tokenizer.tokenize_chunk(chunk_signal)
                    text = "".join(tokens)
                    # time1 = time.time() # å¯é€‰ï¼šå–æ¶ˆæ³¨é‡Šä»¥æµ‹é‡æ—¶é—´
                    # time_cost = time1 - time0 # å¯é€‰ï¼šå–æ¶ˆæ³¨é‡Šä»¥æµ‹é‡æ—¶é—´
                    # print(f"      ğŸ”¤ Tokenizing chunk {chunk_start}-{chunk_end} (Len: {len(chunk_signal)}) took {time_cost:.4f}s") # å¯é€‰ï¼šå–æ¶ˆæ³¨é‡Šä»¥æ‰“å°æ—¶é—´

                    # --- å‡†å¤‡ç»“æœæ¡ç›® ---
                    result_entry = {
                        "fast5": os.path.basename(fast5_path), # å­˜å‚¨åŸå§‹æ–‡ä»¶åè€Œéå®Œæ•´è·¯å¾„
                        "read_id": read_id,
                        "chunk_start": chunk_start,
                        "chunk_size": chunk_size,
                        "bases": bases,
                        "text": text
                    }
                    results_for_this_fast5.append(result_entry)

                    # æ›´æ–°æ€»ä½“è¿›åº¦æ¡
                    overall_pbar.update(1)


            except Exception as e:
                print(f"    âŒ å¤„ç† {os.path.basename(fast5_path)} ä¸­çš„ read {read_id} æ—¶å‡ºé”™: {e}")
                # å³ä½¿å‘ç”Ÿé”™è¯¯ï¼Œä¹Ÿè¦æ›´æ–°è¿›åº¦æ¡ï¼Œè·³è¿‡è¿™ä¸ª read çš„æ‰€æœ‰ chunks
                for _ in group_df_by_read.itertuples():
                    overall_pbar.update(1)
                continue # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª read_id

    # å…³é—­æ€»ä½“è¿›åº¦æ¡
    overall_pbar.close()

    # --- å°†ç»“æœå†™å…¥ JSONL.GZ ---
    print(f"ğŸ’¾ æ­£åœ¨å°† {len(results_for_this_fast5)} æ¡ç»“æœå†™å…¥ {os.path.basename(output_jsonl_gz_path)}")
    with gzip.open(output_jsonl_gz_path, 'wt', encoding='utf-8') as gz_file:
        for item in results_for_this_fast5:
            gz_file.write(json.dumps(item) + '\n')

    print("ğŸ‰ å¤„ç†å®Œæˆï¼")

def main():
    """
    ä¸»å‡½æ•°ï¼šè§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œåˆå§‹åŒ–ç»„ä»¶ï¼Œå¹¶æ‰§è¡Œå¤„ç†æµç¨‹ã€‚
    """
    parser = argparse.ArgumentParser(description="Tokenize Nanopore signal chunks from a single FAST5 file using a VQE tokenizer.")
    parser.add_argument('--fast5_path', type=str, required=True, help='Path to the single FAST5 file.')
    parser.add_argument('--csv_path', type=str, required=True, help='Path to the corresponding input CSV file.')
    parser.add_argument('--model_path', type=str, required=True, help='Path to the VQE model checkpoint.')
    parser.add_argument('--device', type=str, default='cuda', help='Device to run the tokenizer on (e.g., cpu, cuda, cuda:0). Defaults to cuda.')
    parser.add_argument('--signal_strategy', type=str, default='apple', help='Nanopore signal processing strategy. Defaults to apple.')

    args = parser.parse_args()

    # ä»å‘½ä»¤è¡Œå‚æ•°è·å–å€¼
    fast5_path = args.fast5_path
    csv_path = args.csv_path
    model_path = args.model_path
    device = args.device
    signal_strategy = args.signal_strategy

    # æ‰“å°æ‰€æœ‰åŠ è½½çš„é…ç½®å‚æ•°
    print("--- Loaded Configuration from Command Line Args ---")
    print(f"  FAST5 Path (fast5_path): {fast5_path}")
    print(f"  CSV Path (csv_path): {csv_path}")
    print(f"  Model Path (model_path): {model_path}")
    print(f"  Device (device): {device}")
    print(f"  Signal Strategy (signal_strategy): {signal_strategy}")
    print("--------------------------------------------------")

    # æ‰§è¡Œå¤„ç†æµç¨‹
    print("ğŸš€ Starting single FAST5 processing...")
    process_single_fast5(
        fast5_path=fast5_path,
        csv_path=csv_path,
        model_path=model_path,
        device=device,
        nanopore_signal_process_strategy=signal_strategy
    )

if __name__ == "__main__":
    main()
