# nanopore_signal_tokenizer/fast5.py

import warnings
warnings.filterwarnings("ignore", message=".*pkg_resources is deprecated.*")

import os
import json  # ã€æ–°å¢ã€‘ç”¨äºä¿å­˜å…¨å±€æ±‡æ€»æŠ¥å‘Š
import numpy as np
import glob
from ont_fast5_api.fast5_interface import get_fast5_file
from .signal import (
    nanopore_remove_spikes,
    nanopore_normalize_novel,
    nanopore_normalize_huada,
    nanopore_repair_errors 
    )
from scipy.signal import medfilt
from pathos.multiprocessing import ProcessPool
from multiprocessing import cpu_count
import tqdm
from scipy.ndimage import median_filter
from typing import List, Dict, Any  # ã€æ–°å¢ã€‘ç±»å‹æç¤ºï¼Œæå‡å¯è¯»æ€§
import time

class Fast5Dir:
    """
    å°† Nanopore åŸå§‹ .fast5 æ–‡ä»¶æ‰¹é‡è½¬æ¢ä¸ºé¢„å¤„ç†åçš„ chunked .npy æ–‡ä»¶ã€‚

    ğŸ“Œ ä¿¡å·å¤„ç†æµæ°´çº¿ï¼ˆæ‰€æœ‰æ­¥éª¤åœ¨ to_chunks_parallel ä¸­æ§åˆ¶ï¼‰ï¼š
        1. ã€ç¼©æ”¾ã€‘raw â†’ pAï¼›
        2. ã€å½’ä¸€åŒ–ã€‘median-MADï¼ˆå¯é€‰ï¼‰ï¼›
        3. ã€ä¸­å€¼æ»¤æ³¢ã€‘kernel=5ï¼ˆå¯é€‰ï¼‰ï¼›
        4. ã€ä½é€šæ»¤æ³¢ã€‘Butterworthï¼ˆå¯é€‰ï¼‰ï¼›
        5. ã€åˆ†å—ã€‘æ»‘åŠ¨çª—å£ + æœ«å°¾å…œåº• + å¤šå¤´è£å‰ªã€‚

    ğŸ“¦ è¾“å‡ºï¼šæ¯ä¸ª .fast5 â†’ ä¸€ä¸ª .npyï¼Œå†…å®¹ä¸º list[dict]ï¼Œå« read_idã€ä½ç½®ã€chunk_dataã€head_cutã€‚
    
    ã€æ–°å¢è¯´æ˜ - å…¨å±€æ±‡æ€»ã€‘
    æœ¬ç±»åœ¨å®Œæˆå…¨éƒ¨æ–‡ä»¶å¤„ç†åï¼Œä¼šè‡ªåŠ¨ç”Ÿæˆä¸€ä»½å…¨å±€ç»Ÿè®¡æ‘˜è¦ï¼ˆglobal_summary.jsonï¼‰ï¼Œ
    åŒ…å«ï¼šæ€»æ–‡ä»¶æ•°ã€æˆåŠŸ/å¤±è´¥æ–‡ä»¶æ•°ã€æœ‰æ•ˆ reads æ•°ã€å„ç±»è·³è¿‡åŸå› è®¡æ•°ã€æ€» chunks æ•°ã€
    æ¯ä¸ªè¾“å‡ºæ–‡ä»¶çš„ chunk æ•°é‡åˆ†å¸ƒã€ä»¥åŠä¸¥é‡é”™è¯¯æ—¥å¿—ã€‚ä¾¿äºè´¨é‡æ§åˆ¶ä¸æµç¨‹ç›‘æ§ã€‚
    """

    def __init__(self, fast5_dir: str, default_fs: int = 5000):
        """
        åˆå§‹åŒ–ç›®å½•å¤„ç†å™¨ã€‚

        Args:
            fast5_dir (str): åŒ…å« .fast5 æ–‡ä»¶çš„ç›®å½•ã€‚
            default_fs (int): é»˜è®¤é‡‡æ ·ç‡ï¼ˆHzï¼‰ï¼Œç”¨äºç¼ºå¤± metadata çš„æƒ…å†µã€‚
        """
        if not os.path.isdir(fast5_dir):
            raise ValueError(f"FAST5 directory does not exist: {fast5_dir}")

        self.fast5_dir = fast5_dir
        self.fast5_files = sorted(glob.glob(os.path.join(fast5_dir, "*.fast5")))
        self.default_fs = default_fs

        if not self.fast5_files:
            raise FileNotFoundError(f"No .fast5 files found in {fast5_dir}")

        # ã€æ–°å¢ã€‘å…¨å±€ç»Ÿè®¡å­—å…¸ï¼Œç”¨äºæ”¶é›†æ•´ä¸ªå¤„ç†è¿‡ç¨‹çš„æ±‡æ€»ä¿¡æ¯
        self.global_stats = {
            "total_fast5_files": len(self.fast5_files),
            "processed_fast5_files": 0,
            "skipped_fast5_files": 0,
            "total_reads": 0,
            "valid_reads": 0,
            "skipped_reads": {
                "no_reads_in_file": 0,
                "signal_extraction_failed": 0,
                "signal_out_of_raw_range": 0,
                "signal_out_of_med_range": 0,
                "signal_out_of_norm_range": 0,
                "empty_or_nan_after_norm": 0,
                "filtering_failed": 0,
                "too_short": 0,
            },
            "total_chunks": 0,
            "chunks_per_file": {},  # key: basename, value: chunk count
            "errors": [],  # è®°å½•ä¸¥é‡é”™è¯¯ï¼ˆå¦‚æ–‡ä»¶æ— æ³•æ‰“å¼€ï¼‰
            "processing_start_time": None,
            "processing_end_time": None,
            "total_elapsed_seconds": 0,
        }

    @staticmethod
    def get_sampling_rate_from_read(read):
        """ä» FAST5 read ä¸­æå–é‡‡æ ·ç‡ï¼Œå¤±è´¥æ—¶è¿”å› Noneã€‚"""
        try:
            channel_info = read.handle[read.global_key + 'channel_id'].attrs
            return int(channel_info['sampling_rate'])
        except Exception:
            return None

    def _sliding_window_chunks_with_tail(
        self,
        signal: np.ndarray,
        window_size: int,
        stride: int,
        tail_threshold: int,
    ):
        """
        å¯¹ä¸€ç»´ä¿¡å·è¿›è¡Œæ»‘åŠ¨çª—å£åˆ‡åˆ†ï¼Œå¹¶åœ¨æœ«å°¾ä¸è¶³ä¸€ä¸ªçª—å£ä½†æ»¡è¶³æœ€å°é•¿åº¦æ—¶è¡¥å……ä¸€ä¸ª chunkã€‚

        åˆ‡åˆ†ç­–ç•¥ï¼š
          - ä¸»ä½“ä½¿ç”¨å›ºå®š stride æ»‘åŠ¨ï¼›
          - è‹¥æœ«å°¾å‰©ä½™ç‰‡æ®µé•¿åº¦ â‰¥ tail_thresholdï¼Œåˆ™ä»ä¿¡å·æœ«å°¾å€’æ•° window_size ç‚¹å†åˆ‡ä¸€ä¸ª chunkï¼›
          - é¿å…ä¸æœ€åä¸€ä¸ªæ»‘åŠ¨çª—å£é‡å¤ã€‚

        Args:
            signal (np.ndarray): è¾“å…¥ä¸€ç»´ä¿¡å·ã€‚
            window_size (int): æ¯ä¸ª chunk çš„é•¿åº¦ï¼ˆç‚¹æ•°ï¼‰ã€‚
            stride (int): æ»‘åŠ¨æ­¥é•¿ï¼ˆç‚¹æ•°ï¼‰ã€‚
            tail_threshold (int): è§¦å‘æœ«å°¾è¡¥ chunk çš„æœ€å°å‰©ä½™é•¿åº¦ã€‚

        Returns:
            List[Dict]: æ¯ä¸ªå…ƒç´ åŒ…å« 'chunk_start', 'chunk_end', 'chunk_data'ã€‚
        """
        n_points = len(signal)
        if n_points < window_size:
            return []

        chunks = []
        start = 0
        end = 0
        # ä¸»æ»‘åŠ¨çª—å£å¾ªç¯
        while start + window_size <= n_points:
            end = start + window_size
            chunks.append({
                'chunk_start': start,
                'chunk_end': end,
                'chunk_data': signal[start:end].copy()
            })
            start += stride

        # æœ«å°¾å…œåº•ï¼šè‹¥å‰©ä½™éƒ¨åˆ†è¶³å¤Ÿé•¿ä¸”æœªè¢«è¦†ç›–ï¼Œåˆ™ä»æœ«å°¾åˆ‡ä¸€ä¸ªå®Œæ•´çª—å£
        if n_points - end  >= tail_threshold:
            chunks.append({
                'chunk_start': start,
                'chunk_end': n_points,
                'chunk_data': signal[n_points-window_size:n_points].copy()
            })

        return chunks

    def _process_single_fast5(
        self,
        fast5_path: str,
        output_dir: str,
        window_size: int,
        stride: int,
        do_normalize: bool,
        do_medianfilter: bool,
        do_lowpassfilter: bool,
        cut_head_all: int,
        cut_head_step: int,
        tail_threshold: int,
        max_chunks_per_file: int = 100000,
        signal_min_value: int = -1000,
        signal_max_value: int = 1000,
        normal_min_value: float = -10.0,
        normal_max_value: float = 10.0
    ):
        """
        ã€å¢å¼ºç‰ˆã€‘å¤„ç†å•ä¸ª FAST5 æ–‡ä»¶ï¼Œå°† chunks æŒ‰æ•°é‡åˆ†ç‰‡ä¿å­˜ï¼Œå¹¶æ›´æ–°å…¨å±€ç»Ÿè®¡ã€‚

        æ–°å¢è¡Œä¸ºï¼š
          - åœ¨å‡½æ•°å†…éƒ¨ç»´æŠ¤ local_statsï¼Œè®°å½•æœ¬æ–‡ä»¶çš„å¤„ç†ç»“æœï¼›
          - å¤„ç†ç»“æŸåè¿”å› local_statsï¼Œä¾›ä¸»è¿›ç¨‹èšåˆåˆ° global_statsã€‚

        Args:
            ...ï¼ˆåŸæœ‰å‚æ•°ä¸å˜ï¼‰...
            max_chunks_per_file (int): æ¯ä¸ªè¾“å‡ºæ–‡ä»¶æœ€å¤§ chunk æ•°é‡ï¼Œé»˜è®¤ 100000ã€‚

        Returns:
            dict: æœ¬åœ°ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…å«æœ¬æ–‡ä»¶çš„ reads/chunks/errors è®¡æ•°ã€‚
        """
        NORM_SIG_MIN = normal_min_value
        NORM_SIG_MAX = normal_max_value

        # ã€æ–°å¢ã€‘åˆå§‹åŒ–æœ¬æ–‡ä»¶çš„å±€éƒ¨ç»Ÿè®¡
        local_stats = {
            "fast5_path": fast5_path,
            "reads_total": 0,
            "reads_valid": 0,
            "reads_skipped": {k: 0 for k in self.global_stats["skipped_reads"]},
            "chunks_saved": 0,
            "output_files": [],
            "error": None
        }

        os.makedirs(output_dir, exist_ok=True)
        basename = os.path.basename(fast5_path).rsplit('.', 1)[0]
        buffer = []
        part_idx = 0

        try:
            with get_fast5_file(fast5_path, mode="r") as f5:
                read_ids = f5.get_read_ids()
                if not read_ids:
                    print(f"âš ï¸ No reads found in {fast5_path}")
                    local_stats["reads_skipped"]["no_reads_in_file"] += 1
                    return local_stats

                reads = list(f5.get_reads())
                local_stats["reads_total"] = len(reads)

                for read in reads:
                    # --- ä¿¡å·é¢„å¤„ç†ï¼ˆåŒå‰ï¼‰---
                    #
                    try:
                        channel_info = read.handle[read.global_key + 'channel_id'].attrs
                        offset = int(channel_info['offset'])
                        scaling = channel_info['range'] / channel_info['digitisation']
                        raw = read.handle[read.raw_dataset_name][:]
                        signal_raw = np.array(scaling * (raw + offset), dtype=np.float32)
                    except Exception as e:
                        print(f"âš ï¸ Failed to extract signal for read {read.read_id}: {e}, skipped.")
                        local_stats["reads_skipped"]["signal_extraction_failed"] += 1
                        continue

                    # ä½¿ç”¨ nanopore_repair_error è¿‡æ»¤æç«¯å¼‚å¸¸å€¼
                    signal_clr = nanopore_repair_errors(signal_raw, signal_min_value, signal_max_value)
                    # å†æ¬¡æ£€æŸ¥æ˜¯å¦ä»æœ‰è¶Šç•Œï¼ˆç†è®ºä¸Šåº”å·²ä¿®å¤ï¼Œä½†ä¿é™©èµ·è§ï¼‰
                    if np.any(signal_clr < signal_min_value) or np.any(signal_clr > signal_max_value):
                        print(f"âš ï¸ Read {read.read_id} still out of raw range after filtering, skipped.")
                        local_stats["reads_skipped"]["signal_out_of_raw_range"] += 1
                        continue
                    # æŠŠnanopore_repair_erroræ²¡æœ‰ä¿®å¤çš„åŒ…å«åœ¨[signal_min_value,signal_max_value]èŒƒå›´å†…çš„æ•°æ®ç»™ä¿®å¤æ‰
                    signal_nos = nanopore_remove_spikes(signal_clr, window_size=window_size, spike_threshold=5.0)
                    
                    # å› ä¸ºrepairé‡Œæœ‰abs(raw-med)è¿™ä¸€æ­¥ï¼Œæ‰€ä»¥å¿…é¡»åœ¨è¿™æ­¥å‰ä¿®å¤æ•°æ®ï¼ŒæŠŠæç«¯å¤§çš„å€¼ç»™å¹²æ‰,ä¹Ÿå°±æ˜¯å¿…é¡»repair
                    signal_nom, global_mad = nanopore_normalize_novel(signal_nos)

                    #signal_nom = nanopore_repair_normal(signal_nom, NORM_SIG_MIN, NORM_SIG_MAX,window_size=33)
                    # åº”ç”¨ä¸­å€¼æ»¤æ³¢ï¼ˆæ³¨æ„ï¼šæ­¤å¤„åŸä»£ç å·²å¼ºåˆ¶å¼€å¯ï¼Œä½†å‚æ•°æ§åˆ¶ä»ä¿ç•™ï¼‰
                    signal_med = medfilt(signal_nom, kernel_size=5).astype(np.float32)
                    
                    signal = signal_med
                    signal_chk = signal_nom

                    # æ£€æŸ¥å½’ä¸€åŒ–åæ˜¯å¦åœ¨å…è®¸èŒƒå›´å†…
                    if np.any(signal_chk < NORM_SIG_MIN) or np.any(signal_chk > NORM_SIG_MAX):
                        actual_min = signal.min()
                        actual_max = signal.max()
                        print(f"âš ï¸ Ignored read {fast5_path} {read.read_id} due to out-of-range signal values after normalization. "
                              f"Actual range: [{actual_min:.3f}, {actual_max:.3f}], "
                              f"Allowed: [{NORM_SIG_MIN}, {NORM_SIG_MAX}]")

                        # ğŸ”ã€æ¢å¤ã€‘å¤šé˜¶æ®µä¸Šä¸‹æ–‡æ‰“å°ï¼ˆæ¥è‡ª fast5.bak.pyï¼‰
                        outlier_mask = (signal_chk < NORM_SIG_MIN) | (signal_chk > NORM_SIG_MAX)
                        outlier_indices = np.where(outlier_mask)[0]
                        # === æ–°å¢ï¼šè¿‡æ»¤æ‰ä¸å‰ä¸€ä¸ªå¼‚å¸¸ç‚¹è·ç¦»å°äº5çš„ç‚¹ ===
                        print_half_window_size = 5
                        if outlier_indices.size > 0:
                            keep = [True]  # ç¬¬ä¸€ä¸ªç‚¹æ€»æ˜¯ä¿ç•™
                            last_kept = outlier_indices[0]
                            for idx in outlier_indices[1:]:
                                if idx - last_kept >= print_half_window_size:
                                    keep.append(True)
                                    last_kept = idx
                                else:
                                    keep.append(False)
                            outlier_indices = outlier_indices[keep]
                        max_print = 5
                        for i, idx in enumerate(outlier_indices[:max_print]):
                            start = max(0, idx - print_half_window_size)
                            end = min(len(signal_chk), idx + print_half_window_size)
                            context = signal[start:end]
                            context_raw = signal_raw[start:end]
                            context_clr = signal_clr[start:end]
                            context_nos = signal_nos[start:end]
                            context_nom = signal_nom[start:end]
                            context_med = signal_med[start:end]
                            print(f"  â†’ Outlier #{i+1} at index {idx}: value = {signal[idx]:.3f}")
                            print(f"    Raw ({start}â€“{end-1}): {[f'{x:.3f}' for x in context_raw]}")
                            print(f"    Clr ({start}â€“{end-1}): {[f'{x:.3f}' for x in context_clr]}")
                            print(f"    Nom ({start}â€“{end-1}): {[f'{x:.3f}' for x in context_nom]}")
                            print(f"    Nos ({start}â€“{end-1}): {[f'{x:.3f}' for x in context_nos]}")
                            print(f"    Med ({start}â€“{end-1}): {[f'{x:.3f}' for x in context_med]}")
                        if len(outlier_indices) > max_print:
                            print(f"  â†’ ... and {len(outlier_indices) - max_print} more outliers.")

                    if np.any(signal_nom < NORM_SIG_MIN) or np.any(signal_nom > NORM_SIG_MAX):
                        local_stats["reads_skipped"]["signal_out_of_norm_range"] += 1
                        continue

                    if signal.size == 0 or np.isnan(signal).any():
                        print(f"âš ï¸ Invalid signal after normalization for read {read.read_id}, skipped.")
                        local_stats["reads_skipped"]["empty_or_nan_after_norm"] += 1
                        continue

                    if do_lowpassfilter:
                        fs_from_read = self.get_sampling_rate_from_read(read)
                        fs = fs_from_read if fs_from_read is not None else self.default_fs
                        try:
                            filtered_signal = nanopore_filter(signal, fs=fs)
                        except Exception as e:
                            print(f"âš ï¸ Filtering failed for read {read.read_id} (fs={fs}): {e}, skipped.")
                            local_stats["reads_skipped"]["filtering_failed"] += 1
                            continue
                        if filtered_signal.size == 0 or np.isnan(filtered_signal).any():
                            print(f"âš ï¸ Invalid signal after filtering for read {read.read_id}, skipped.")
                            local_stats["reads_skipped"]["filtering_failed"] += 1
                            continue
                        signal = filtered_signal

                    if len(signal) < window_size:
                        print(f"âš ï¸ Read {read.read_id} too short (<{window_size} points), skipped.")
                        local_stats["reads_skipped"]["too_short"] += 1
                        continue

                    # âœ… æ­¤ read é€šè¿‡æ‰€æœ‰æ£€æŸ¥
                    local_stats["reads_valid"] += 1

                    max_head = min(cut_head_all, len(signal) - 1)
                    head_cuts = list(range(0, max_head + 1, cut_head_step)) or [0]

                    read_chunks = []
                    for head_cut in head_cuts:
                        if head_cut >= len(signal):
                            continue
                        trimmed_signal = signal[head_cut:]
                        chunks = self._sliding_window_chunks_with_tail(
                            trimmed_signal, window_size, stride, tail_threshold
                        )
                        for ch in chunks:
                            read_chunks.append({
                                'read_id': read.read_id,
                                'head_cut': head_cut,
                                'chunk_start_pos': head_cut + ch['chunk_start'],
                                'chunk_end_pos': head_cut + ch['chunk_end'],
                                'chunk_data': ch['chunk_data']
                            })

                    if read_chunks:
                        buffer.extend(read_chunks)

                        if len(buffer) >= max_chunks_per_file:
                            save_path = os.path.join(output_dir, f"{basename}_part{part_idx:05d}.npy")
                            np.save(save_path, buffer[:max_chunks_per_file])
                            local_stats["chunks_saved"] += len(buffer[:max_chunks_per_file])
                            local_stats["output_files"].append(save_path)
                            print(f"âœ… Saved {len(buffer[:max_chunks_per_file])} chunks to {save_path}")
                            buffer = buffer[max_chunks_per_file:]
                            part_idx += 1

                # ä¿å­˜å‰©ä½™ buffer
                if buffer:
                    save_path = os.path.join(output_dir, f"{basename}_part{part_idx:05d}.npy")
                    np.save(save_path, buffer)
                    saved_count = len(buffer)
                    local_stats["chunks_saved"] += saved_count
                    local_stats["output_files"].append(save_path)
                    print(f"âœ… Saved final {saved_count} chunks to {save_path}")

        except Exception as e:
            error_msg = f"âŒ Critical error processing {fast5_path}: {e}"
            print(error_msg)
            local_stats["error"] = str(e)
            return local_stats

        return local_stats
    def _aggregate_and_save_global_summary(self, output_dir: str):
        """
        ã€æ–°å¢ã€‘èšåˆæ‰€æœ‰å±€éƒ¨ç»Ÿè®¡ï¼Œç”Ÿæˆå¹¶ä¿å­˜å…¨å±€æ±‡æ€»æŠ¥å‘Šåˆ° global_summary.jsonã€‚
        """
        summary_path = os.path.join(output_dir, "global_summary.json")
        try:
            # è¡¥å……æ—¶é—´ä¿¡æ¯
            self.global_stats["processing_end_time"] = time.time()
            self.global_stats["total_elapsed_seconds"] = (
                self.global_stats["processing_end_time"] - self.global_stats["processing_start_time"]
            )

            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(self.global_stats, f, indent=4, ensure_ascii=False)
            print(f"ğŸ“Š å…¨å±€æ±‡æ€»å·²ä¿å­˜è‡³: {summary_path}")
        except Exception as e:
            print(f"âš ï¸ æ— æ³•ä¿å­˜å…¨å±€æ±‡æ€»: {e}")

    def to_chunks(
        self,
        output_dir: str,
        window_size: int = 32,
        stride: int = 8,
        do_normalize: bool = True,
        do_medianfilter: bool = False,
        do_lowpassfilter: bool = False,
        cut_head_all: int = 5,
        cut_head_step: int = 2,
        tail_threshold: int = 16,
        n_jobs: int = -1,
        signal_min_value: int = -1000,
        signal_max_value: int = 1000,
        normal_min_value: float = -5.0,
        normal_max_value: float = 5.0
    ):
        """
        å¹¶è¡Œå¤„ç†æ•´ä¸ª FAST5 ç›®å½•ï¼Œç”Ÿæˆ chunked .npy æ–‡ä»¶ã€‚

        ğŸ¯ å¤šå¤´è£å‰ªè¯´æ˜ï¼š
            ä¸ºé€‚é…ä¸‹æ¸¸ CNN çš„ä¸‹é‡‡æ · strideï¼ˆå¦‚ 12ï¼‰ï¼Œéœ€è¦†ç›–æ‰€æœ‰å¯èƒ½çš„è¾“å…¥å¯¹é½ç›¸ä½ã€‚
            é€šè¿‡è®¾ç½® cut_head_all=11, cut_head_step=1ï¼Œå¯ç”Ÿæˆ 12 ç§èµ·å§‹åç§»ï¼ˆ0~11ï¼‰ï¼Œ
            ç¡®ä¿æ¨¡å‹å­¦ä¹ åˆ°å¹³ç§»é²æ£’çš„ token è¡¨ç¤ºã€‚

        ğŸ¯ æœ«å°¾å…œåº•è¯´æ˜ï¼š
            å½“æ»‘åŠ¨çª—å£ç»“æŸåï¼Œè‹¥å‰©ä½™ä¿¡å·é•¿åº¦ â‰¥ tail_thresholdï¼Œ
            åˆ™ä»ä¿¡å·æœ«å°¾å¼ºåˆ¶åˆ‡å‡ºä¸€ä¸ªå®Œæ•´ windowï¼Œé¿å…ä¿¡æ¯æµªè´¹ã€‚

        ã€æ–°å¢ã€‘å…¨å±€æ±‡æ€»ï¼š
            æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæ¯•åï¼Œè‡ªåŠ¨èšåˆå„è¿›ç¨‹è¿”å›çš„å±€éƒ¨ç»Ÿè®¡ï¼Œ
            ç”Ÿæˆ global_summary.jsonï¼ŒåŒ…å«å®Œæ•´ QC æŒ‡æ ‡ã€‚

        Args:
            output_dir (str): è¾“å‡ºç›®å½•ã€‚
            window_size (int): æ¯ä¸ª chunk çš„é•¿åº¦ï¼ˆé»˜è®¤ 32ï¼‰ã€‚
            stride (int): æ»‘åŠ¨æ­¥é•¿ï¼ˆé»˜è®¤ 8ï¼‰ã€‚
            do_normalize (bool): æ˜¯å¦æ‰§è¡Œ median-MAD å½’ä¸€åŒ–ã€‚
            do_medianfilter (bool): æ˜¯å¦åº”ç”¨ä¸­å€¼æ»¤æ³¢ã€‚
            do_lowpassfilter (bool): æ˜¯å¦åº”ç”¨ä½é€šæ»¤æ³¢ã€‚
            cut_head_all (int): æœ€å¤§å¼€å¤´è£å‰ªé•¿åº¦ï¼ˆinclusiveï¼‰ï¼Œå»ºè®®è®¾ä¸º stride-1ã€‚
            cut_head_step (int): è£å‰ªæ­¥é•¿ï¼Œæ§åˆ¶ç›¸ä½è¦†ç›–å¯†åº¦ã€‚
            tail_threshold (int): æœ«å°¾æœ€å°ä¿ç•™ç‚¹æ•°ï¼Œç”¨äºå†³å®šæ˜¯å¦è¡¥ chunkã€‚
            n_jobs (int): å¹¶è¡Œè¿›ç¨‹æ•°ã€‚-1 è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨ CPU æ ¸å¿ƒã€‚
        """
        os.makedirs(output_dir, exist_ok=True)

        if n_jobs == -1:
            n_jobs = cpu_count()

        # æ—¥å¿—è¾“å‡º
        head_cuts_preview = list(range(0, min(cut_head_all + 1, 20), cut_head_step))  # é˜²æ­¢æ‰“å°è¿‡é•¿
        if cut_head_all >= 20:
            head_cuts_preview.append("...")

        print(f"ğŸ“ Processing {len(self.fast5_files)} FAST5 files from: {self.fast5_dir}")
        print(f"ParallelGroup: using {n_jobs} processes")
        print(f"âš™ï¸  Signal pipeline:")
        print(f"    - Normalize: {'ON' if do_normalize else 'OFF'}")
        print(f"    - Median Filter: {'ON' if do_medianfilter else 'OFF'}")
        print(f"    - Low-pass Filter: {'ON' if do_lowpassfilter else 'OFF'}")
        print(f"    - Head cuts: all={cut_head_all}, step={cut_head_step} â†’ sample phases={head_cuts_preview}")
        print(f"    - Tail threshold: {tail_threshold} (fallback chunk if tail â‰¥ this)")
        print(f"ğŸ’¾ Saving chunks to: {output_dir}")

        # ã€æ–°å¢ã€‘è®°å½•å¼€å§‹æ—¶é—´
        import time  # å±€éƒ¨å¯¼å…¥é¿å…æ±¡æŸ“
        self.global_stats["processing_start_time"] = time.time()

        # æ„é€ å‚æ•°åˆ—è¡¨
        args_list = [
            (
                fp,
                output_dir,
                window_size,
                stride,
                do_normalize,
                do_medianfilter,
                do_lowpassfilter,
                cut_head_all,
                cut_head_step,
                tail_threshold,
                signal_min_value,
                signal_max_value,
                normal_min_value,
                normal_max_value
            )
            for fp in self.fast5_files
        ]

        # ä½¿ç”¨ pathos å¹¶è¡Œå¤„ç†ï¼ˆæ”¯æŒ pickle ä¸å‹å¥½çš„å¯¹è±¡ï¼‰
        with ProcessPool(nodes=n_jobs) as pool:
            results = pool.map(self._process_single_fast5_wrapper, args_list)

        # ã€æ–°å¢ã€‘èšåˆå…¨å±€ç»Ÿè®¡
        for res in results:
            if res is None:
                self.global_stats["skipped_fast5_files"] += 1
                continue

            self.global_stats["processed_fast5_files"] += 1
            self.global_stats["total_reads"] += res["reads_total"]
            self.global_stats["valid_reads"] += res["reads_valid"]
            self.global_stats["total_chunks"] += res["chunks_saved"]

            basename = os.path.basename(res["fast5_path"]).rsplit('.', 1)[0]
            self.global_stats["chunks_per_file"][basename] = res["chunks_saved"]

            # ç´¯åŠ å„ç±»è·³è¿‡åŸå› 
            for reason, count in res["reads_skipped"].items():
                self.global_stats["skipped_reads"][reason] += count

            # è®°å½•é”™è¯¯
            if res.get("error"):
                self.global_stats["errors"].append({
                    "file": res["fast5_path"],
                    "error": res["error"]
                })

        # ã€æ–°å¢ã€‘ä¿å­˜å…¨å±€æ±‡æ€»
        self._aggregate_and_save_global_summary(output_dir)

    def _process_single_fast5_wrapper(self, args):
        """
        ä¾› pathos.multiprocessing è°ƒç”¨çš„å‚æ•°è§£åŒ…åŒ…è£…å™¨ã€‚
        """
        (
            fast5_path,
            output_dir,
            window_size,
            stride,
            do_normalize,
            do_medianfilter,
            do_lowpassfilter,
            cut_head_all,
            cut_head_step,
            tail_threshold,
            signal_min_value,
            signal_max_value,
            normal_min_value,
            normal_max_value
        ) = args
        return self._process_single_fast5(
            fast5_path=fast5_path,
            output_dir=output_dir,
            window_size=window_size,
            stride=stride,
            do_normalize=do_normalize,
            do_medianfilter=do_medianfilter,
            do_lowpassfilter=do_lowpassfilter,
            cut_head_all=cut_head_all,
            cut_head_step=cut_head_step,
            tail_threshold=tail_threshold,
            signal_min_value=signal_min_value,
            signal_max_value=signal_max_value,
            normal_min_value=normal_min_value,
            normal_max_value=normal_max_value
        )
