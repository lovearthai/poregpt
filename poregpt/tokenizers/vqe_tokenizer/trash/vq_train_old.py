import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm

# ç›¸å¯¹å¯¼å…¥æ ¸å¿ƒç»„ä»¶
from .dataset import NanoporeSignalDataset
from .vq_model import NanoporeVQModel


def vq_train(
    npy_dir: str,
    output_model_path: str,
    batch_size: int = 16,
    lr: float = 3e-4,
    num_epochs: int = 10,
    codebook_size: int = 8192,
    chunk_size: int = 12000,
    num_workers: int = 8,
    val_size: int = 100,
    do_evaluate: bool = True,
    commitment_weight: float = 0.25
):
    """
    åˆ†å¸ƒå¼è®­ç»ƒ Nanopore VQ tokenizerã€‚
    ç°åœ¨ä¼šåˆ†åˆ«æ‰“å°ï¼šé‡å»ºæŸå¤±ã€commitment æŸå¤±ã€æ€»æŸå¤±ã€‚
    """
    import torch.distributed as dist
    from torch.nn.parallel import DistributedDataParallel as DDP
    from torch.utils.data.distributed import DistributedSampler

    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    dist.init_process_group(backend="nccl")
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    local_device_id = rank % torch.cuda.device_count()
    torch.cuda.set_device(local_device_id)
    device = f"cuda:{local_device_id}"

    if rank == 0:
        print(f"ğŸš€ Using {world_size} GPUs for training.")
        print(f"ğŸ“‚ Data directory: {npy_dir}")
        print(f"ğŸ’¾ Model will be saved to: {output_model_path}")
        print(f"âš™ï¸  Hyperparameters: "
              f"batch_size={batch_size}, lr={lr}, epochs={num_epochs}, "
              f"codebook_size={codebook_size}, chunk_size={chunk_size}, "
              f"do_evaluate={do_evaluate}")

    # ========== æ•°æ®åŠ è½½ ==========
    dataset = NanoporeSignalDataset(npy_dir=npy_dir, expected_chunk_len=chunk_size)
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        sampler=sampler,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True
    )

    # ========== å¯é€‰ï¼šéªŒè¯é›†ï¼ˆä»…ç”¨äºè¯„ä¼°ï¼‰==========
    val_loader = None
    if do_evaluate:
        val_dataset = NanoporeSignalDataset(npy_dir=npy_dir, expected_chunk_len=chunk_size)
        actual_val_size = min(val_size, len(val_dataset))
        indices = np.random.choice(len(val_dataset), size=actual_val_size, replace=False)
        val_subset = torch.utils.data.Subset(val_dataset, indices)
        val_loader = DataLoader(
            val_subset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=max(2, num_workers // 2),
            pin_memory=True
        )

    # ========== æ¨¡å‹ä¸ä¼˜åŒ–å™¨ ==========
    model = NanoporeVQModel(codebook_size=codebook_size,commitment_weight = commitment_weight).to(device)
    model = DDP(model, device_ids=[local_device_id])
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    # ========== è¯„ä¼°å‡½æ•°ï¼ˆä»…åœ¨ do_evaluate=True æ—¶è°ƒç”¨ï¼‰==========
    def evaluate_codebook_usage():
        model.eval()
        used_codes = set()
        total_tokens = 0
        with torch.no_grad():
            for batch in val_loader:
                x = batch.to(device)
                _, indices, _ = model(x)
                indices = indices.cpu().numpy().flatten()
                used_codes.update(indices.tolist())
                total_tokens += indices.size
        usage_ratio = len(used_codes) / codebook_size
        model.train()
        return usage_ratio, total_tokens

    # ========== è®­ç»ƒå¾ªç¯ ==========
    model.train()
    for epoch in range(num_epochs):
        sampler.set_epoch(epoch)

        # åˆ†åˆ«è®°å½•ä¸‰ç§æŸå¤±
        total_recon_loss = torch.tensor(0.0, device=device)
        total_commit_loss = torch.tensor(0.0, device=device)
        total_total_loss = torch.tensor(0.0, device=device)
        num_batches = torch.tensor(len(dataloader), device=device)

        pbar = tqdm(dataloader, desc=f"Rank {rank} | Epoch {epoch+1}/{num_epochs}", disable=(rank != 0))
        for batch in pbar:
            x = batch.to(device)
            # commit_loss æ˜¯å¦å·²åŒ…å« commitment_weightï¼Ÿ
            # åœ¨ vector_quantize_pytorch ä¸­ï¼Œè¿”å›çš„ commit_loss å·²ç»æ˜¯ä¹˜è¿‡ commitment_weight çš„ï¼ˆé»˜è®¤ 0.25ï¼‰
            # å› ä¸º VectorQuantize è¿”å›çš„ commit_loss æ˜¯ï¼š
            # commit_loss = (z_e - e_k.detach()).pow(2).mean() * self.commitment_weight
            # å®ƒæ˜¯ä¸€ä¸ª requires_grad=False çš„ scalar tensorï¼Œä½äºä¸è¾“å…¥ç›¸åŒçš„è®¾å¤‡ä¸Šï¼ˆGPUï¼‰ã€‚
            # æ‰€ä»¥ commit_loss æœ¬èº«å°±æ˜¯ GPU tensorï¼Œä¸éœ€è¦ .item()ã€‚
            recon, indices, commit_loss = model(x)
            # å¦‚æœä½ æƒ³å¼±åŒ–é‡å»ºã€å¼ºè°ƒç¦»æ•£è¡¨ç¤ºè´¨é‡ï¼Œå¯ä»¥åŠ ä¸€ä¸ªè¶…å‚æ•°ï¼š
            # recon_weight = 0.01  # << é™ä½é‡å»ºæƒé‡
            # loss = recon_weight * F.mse_loss(recon, x) + commit_loss
            # è¿™æ ·æ¨¡å‹ä¼šæ›´å…³æ³¨â€œç¼–ç å™¨è´´ç´§ç æœ¬â€å’Œâ€œç æœ¬åˆ†æ•£â€ï¼Œè€Œä¸æ˜¯åƒç´ çº§è¿˜åŸä¿¡å·â€”â€”éå¸¸é€‚åˆåš tokenizerã€‚
            recon_loss = F.mse_loss(recon, x)
            total_loss = recon_loss + commit_loss

            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()

            # ç´¯åŠ å„éƒ¨åˆ†æŸå¤±ï¼ˆæ³¨æ„ï¼šcommit_loss æ˜¯æ ‡é‡ tensorï¼‰
            total_recon_loss += recon_loss
            total_commit_loss += commit_loss
            total_total_loss += total_loss

        # èšåˆæ‰€æœ‰ GPU çš„æŸå¤±ï¼ˆæ±‚å’Œï¼‰
        dist.all_reduce(total_recon_loss, op=dist.ReduceOp.SUM)
        dist.all_reduce(total_commit_loss, op=dist.ReduceOp.SUM)
        dist.all_reduce(total_total_loss, op=dist.ReduceOp.SUM)
        dist.all_reduce(num_batches, op=dist.ReduceOp.SUM)

        # è®¡ç®—å¹³å‡æŸå¤±ï¼ˆåªåœ¨ rank 0 æ‰“å°ï¼‰
        avg_recon = total_recon_loss.item() / num_batches.item()
        avg_commit = total_commit_loss.item() / num_batches.item()
        avg_total = total_total_loss.item() / num_batches.item()

        if rank == 0:
            if do_evaluate and epoch < num_epochs - 1:
                usage_ratio, total_tokens = evaluate_codebook_usage()
                print(
                    f"Epoch {epoch+1} - "
                    f"Recon Loss: {avg_recon:.6f} | "
                    f"Commit Loss: {avg_commit:.6f} | "
                    f"Total Loss: {avg_total:.6f} | "
                    f"Codebook Usage: {usage_ratio:.1%} (tokens={total_tokens:,})"
                )
            else:
                print(
                    f"Epoch {epoch+1} - "
                    f"Recon Loss: {avg_recon:.6f} | "
                    f"Commit Loss: {avg_commit:.6f} | "
                    f"Total Loss: {avg_total:.6f}"
                )

    # ä¿å­˜æ¨¡å‹ï¼ˆä»… rank 0ï¼‰
    if rank == 0:
        torch.save(model.module.state_dict(), output_model_path)
        print(f"âœ… Model saved to {output_model_path}")

    dist.barrier()
    dist.destroy_process_group()
