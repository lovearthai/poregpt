# nanopore_signal_tokenizer/fast5.py

import warnings
warnings.filterwarnings("ignore", message=".*pkg_resources is deprecated.*")
from multiprocessing import Pool, cpu_count
import functools
import os
import numpy as np
import glob
from ont_fast5_api.fast5_interface import get_fast5_file
from .nanopore import nanopore_normalize, nanopore_filter
from scipy.signal import medfilt

class Fast5Dir:
    """
    å¤„ç†ä¸€ä¸ªåŒ…å« .fast5 æ–‡ä»¶çš„ç›®å½•ï¼Œå°†å…¶è½¬æ¢ä¸º chunked .npy æ–‡ä»¶ã€‚
    
    ä¿¡å·å¤„ç†æµç¨‹ï¼š
        raw â†’ scaled â†’ med-mad normalized â†’ Butterworth low-pass filtered â†’ chunked
    
    æ¯ä¸ª .fast5 â†’ ä¸€ä¸ª .npyï¼Œæ¯ä¸ª chunk æ˜¯ dictï¼š
        {
            'read_id': str,
            'chunk_start_pos': int,
            'chunk_end_pos': int,
            'chunk_data': np.ndarray (shape=(window_size,))
        }
    """

    def __init__(self, fast5_dir: str):
        """
        åˆå§‹åŒ–å¤„ç†å™¨ã€‚

        Args:
            fast5_dir (str): åŒ…å« .fast5 æ–‡ä»¶çš„ç›®å½•è·¯å¾„ã€‚
            default_fs (int): å…¨å±€é»˜è®¤é‡‡æ ·ç‡ï¼ˆHzï¼‰ï¼Œå½“ read ä¸­æ—  sampling_rate æ—¶ä½¿ç”¨ã€‚
        """
        if not os.path.isdir(fast5_dir):
            raise ValueError(f"FAST5 directory does not exist: {fast5_dir}")
        self.fast5_dir = fast5_dir
        self.fast5_files = sorted(glob.glob(os.path.join(fast5_dir, "*.fast5")))
        self.default_fs = 5000
        if not self.fast5_files:
            raise FileNotFoundError(f"No .fast5 files found in {fast5_dir}")

    @staticmethod
    def get_sampling_rate_from_read(read):
        """å°è¯•ä» read çš„ metadata ä¸­æå– sampling_rate"""
        try:
            channel_info = read.handle[read.global_key + 'channel_id'].attrs
            return int(channel_info['sampling_rate'])
        except Exception:
            return None  # è¡¨ç¤ºæœªæ‰¾åˆ°

    def _sliding_window_chunks_with_pos(self, signal, window_size=32, stride=8):
        n_points = len(signal)
        if n_points < window_size:
            return []

        chunks = []
        start = 0
        while start + window_size <= n_points:
            end = start + window_size
            chunk_data = signal[start:end].copy()
            chunks.append({
                'chunk_start': start,
                'chunk_end': end,
                'chunk_data': chunk_data
            })
            start += stride
        return chunks

    def _process_single_fast5(
        self,
        fast5_path: str,
        output_dir: str,
        window_size: int,
        stride: int,
    ):
        all_chunks = []
        try:
            with get_fast5_file(fast5_path, mode="r") as f5:
                for read in f5.get_reads():
                    # --- 1. ç¼©æ”¾åŸå§‹ä¿¡å· ---
                    channel_info = read.handle[read.global_key + 'channel_id'].attrs
                    offset = int(channel_info['offset'])
                    scaling = channel_info['range'] / channel_info['digitisation']
                    raw = read.handle[read.raw_dataset_name][:]
                    signal = np.array(scaling * (raw + offset), dtype=np.float32)

                    # --- 2. å½’ä¸€åŒ– ---
                    if do_normalize:
                        signal = nanopore_normalize(signal)
                    if signal.size == 0:
                        print(f"âš ï¸ Empty after normalization for read {read.read_id}, skipped.")
                        continue
                    
                    # åŸå§‹ä¿¡å·: raw_signal (é‡‡æ ·ç‡ 5000 Hz)
                    # å…¸å‹ k-mer æŒç»­æ—¶é—´ â‰ˆ 2â€“5 ms â†’ å¯¹åº” 10â€“25 ä¸ªé‡‡æ ·ç‚¹

                    # æ¨èçª—å£å¤§å°ï¼š3 ~ 7ï¼ˆå¥‡æ•°ï¼‰
                    if do_medfilter:
                        signal = medfilt(signal, kernel_size=5)


                    # --- 3. ç¡®å®šé‡‡æ ·ç‡ï¼šä¼˜å…ˆ read è‡ªå¸¦ï¼Œå¦åˆ™ç”¨å…¨å±€é»˜è®¤ ---
                    if do_lowpassfilter:
                        try:
                            fs_from_read = self.get_sampling_rate_from_read(read)
                            fs = fs_from_read if fs_from_read is not None else self.default_fs
                            filtered_signal = nanopore_filter(
                                signal, fs=fs
                            )
                        except Exception as e:
                            print(f"âš ï¸ Filtering failed for read {read.read_id} (fs={fs}): {e}, skipped.")
                            continue

                    if filtered_signal.size == 0 or np.isnan(filtered_signal).any():
                        print(f"âš ï¸ Invalid signal after filtering for read {read.read_id}, skipped.")
                        continue

                    # --- 5. åˆ‡ chunk ---
                    chunks = self._sliding_window_chunks_with_pos(
                        filtered_signal, window_size=window_size, stride=stride
                    )
                    if not chunks:
                        print(f"âš ï¸ Read {read.read_id} too short (<{window_size} points), skipped.")
                        continue

                    for ch in chunks:
                        all_chunks.append({
                            'read_id': read.read_id,
                            'chunk_start_pos': ch['chunk_start'],
                            'chunk_end_pos': ch['chunk_end'],
                            'chunk_data': ch['chunk_data']
                        })

            # --- ä¿å­˜ç»“æœ ---
            if all_chunks:
                basename = os.path.basename(fast5_path).rsplit('.', 1)[0]
                save_path = os.path.join(output_dir, f"{basename}.npy")
                np.save(save_path, all_chunks)
                print(f"âœ… Saved {len(all_chunks)} chunks from {basename} to {save_path}")
            else:
                print(f"âš ï¸ No valid chunks in {os.path.basename(fast5_path)}, skipping save.")

        except Exception as e:
            print(f"âŒ Critical error processing {fast5_path}: {e}")

    def to_chunks(
        self,
        output_dir: str,
        window_size: int = 32,
        stride: int = 8,
    ):
        """
        å°†æ•´ä¸ª FAST5 ç›®å½•è½¬æ¢ä¸º chunked .npy æ–‡ä»¶ã€‚

        Args:
            output_dir (str): è¾“å‡ºç›®å½•ã€‚
            window_size (int): æ¯ä¸ª chunk çš„é•¿åº¦ã€‚
            stride (int): æ»‘åŠ¨çª—å£æ­¥é•¿ã€‚
            cutoff (int): æ»¤æ³¢æˆªæ­¢é¢‘ç‡ï¼ˆHzï¼‰ã€‚
            order (int): Butterworth æ»¤æ³¢å™¨é˜¶æ•°ã€‚
        """
        os.makedirs(output_dir, exist_ok=True)
        print(f"ğŸ“ Processing {len(self.fast5_files)} FAST5 files from: {self.fast5_dir}")
        print(f"âš™ï¸  Signal pipeline: scale â†’ normalize â†’ filter (cutoff={cutoff}Hz, order={order}) â†’ chunk")
        print(f"   â±ï¸ Sampling rate: per-read if available, else global default fs={self.default_fs} Hz")
        print(f"ğŸ’¾ Saving chunks to: {output_dir}")

        for i, fp in enumerate(self.fast5_files):
            print(f"\n[{i+1}/{len(self.fast5_files)}] Processing: {os.path.basename(fp)}")
            self._process_single_fast5(
                fp,
                output_dir=output_dir,
                window_size=window_size,
                stride=stride,
            )

    # åœ¨ Fast5Dir ç±»ä¸­
    def to_chunks_parallel(
        self,
        output_dir: str,
        window_size: int = 32,
        stride: int = 8,
        n_jobs: int = None
    ):
        from pathos.multiprocessing import ProcessPool
        import os

        os.makedirs(output_dir, exist_ok=True)

        if n_jobs is None or n_jobs == -1:
            from multiprocessing import cpu_count
            n_jobs = cpu_count()

        print(f"ğŸ“ Processing {len(self.fast5_files)} FAST5 files from: {self.fast5_dir}")
        print(f"ParallelGroup: using {n_jobs} processes")

        # å‡†å¤‡å‚æ•°ï¼šæ¯ä¸ªä»»åŠ¡æ˜¯ä¸€ä¸ª fast5 æ–‡ä»¶è·¯å¾„
        # æˆ‘ä»¬å°†è°ƒç”¨ self._process_single_fast5(fp, ...)
        args_list = [
            (fp, output_dir, window_size, stride)
            for fp in self.fast5_files
        ]

        # ä½¿ç”¨ pathos çš„ ProcessPoolï¼Œå®ƒèƒ½ pickle æ–¹æ³•
        with ProcessPool(nodes=n_jobs) as pool:
            results = pool.map(self._process_single_fast5_wrapper_for_pathos, args_list)

        for res in results:
            print(res)


    def _process_single_fast5_wrapper_for_pathos(self, args):
        """ä¾› pathos è°ƒç”¨çš„åŒ…è£…å™¨ï¼ˆä»æ˜¯ç±»æ–¹æ³•ï¼‰"""
        fp, output_dir, window_size, stride = args
        return self._process_single_fast5(
            fp,
            output_dir=output_dir,
            window_size=window_size,
            stride=stride
        )
