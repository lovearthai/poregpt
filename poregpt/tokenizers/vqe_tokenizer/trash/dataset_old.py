# train_nanopore_rvq.py
# æœ¬è„šæœ¬ç›®æ ‡ï¼šè®­ç»ƒä¸€ä¸ªè‡ªç›‘ç£æ¨¡å‹ï¼Œå°† Nanopore åŸå§‹ç”µæµä¿¡å·ï¼ˆ5kHzï¼‰è½¬æ¢ä¸ºç¦»æ•£ token åºåˆ—ï¼Œ
# ç”¨äºåç»­è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPTï¼‰å»ºæ¨¡ DNA/RNA åºåˆ—ã€‚
# æ‰€æœ‰æ³¨é‡Šå‡ä¸ºå·¥ä¸šçº§è¯¦ç»†è¯´æ˜ï¼Œé€‚åˆ PyTorch æ–°æ‰‹ç†è§£ã€‚

import os
import torch                     # PyTorch ä¸»åº“ï¼Œç”¨äºå¼ é‡è®¡ç®—å’Œæ·±åº¦å­¦ä¹ 
import torch.nn as nn            # ç¥ç»ç½‘ç»œæ¨¡å—ï¼ˆå¦‚ Conv1d, BatchNorm, SiLUï¼‰
import torch.nn.functional as F  # å‡½æ•°å¼æ¥å£ï¼ˆå¦‚ loss, paddingï¼‰
from torch.utils.data import Dataset, DataLoader  # æ•°æ®åŠ è½½å·¥å…·
import numpy as np               # æ•°å€¼è®¡ç®—ï¼ˆç”Ÿæˆæ¨¡æ‹Ÿä¿¡å·ï¼‰
from tqdm import tqdm            # è¿›åº¦æ¡æ˜¾ç¤º

# æ›¿æ¢ encodec RVQ ä¸ºè½»é‡çº§å®ç°
from vector_quantize_pytorch import ResidualVQ


# ----------------------------
# 1. çœŸå® Nanopore æ•°æ®é›†ï¼ˆä» .npy chunks ç›®å½•åŠ è½½ï¼‰
# ----------------------------
class NanoporeSignalDataset(Dataset):
    """
    ä»é¢„å¤„ç†å¥½çš„ .npy chunk æ–‡ä»¶ç›®å½•åŠ è½½çœŸå® Nanopore ä¿¡å·ã€‚
    æ¯ä¸ª .npy æ–‡ä»¶ç”± process_fast5_to_chunks.py ç”Ÿæˆï¼Œæ ¼å¼ä¸º list of dicts:
        {
            'read_id': str,
            'chunk_start_pos': int,
            'chunk_end_pos': int,
            'chunk_data': np.ndarray (shape=(window_size,))
        }
    æœ¬ Dataset å°†æ‰€æœ‰ chunk_data åˆå¹¶ä¸ºä¸€ä¸ªæ‰å¹³åˆ—è¡¨ï¼Œæ¯ä¸ªæ ·æœ¬æ˜¯ä¸€æ®µå›ºå®šé•¿åº¦çš„ä¿¡å·ã€‚
    """
    def __init__(self, npy_dir, expected_chunk_len=32):
        """
        Args:
            npy_dir (str): åŒ…å« .npy chunk æ–‡ä»¶çš„ç›®å½•è·¯å¾„
            expected_chunk_len (int): æ¯ä¸ª chunk çš„é¢„æœŸé•¿åº¦ï¼ˆå¦‚ 32ï¼‰
        """
        self.npy_dir = npy_dir
        self.expected_chunk_len = expected_chunk_len
        self.chunks = []  # å­˜å‚¨æ‰€æœ‰ chunk_data (numpy arrays)

        # æ”¶é›†æ‰€æœ‰ .npy æ–‡ä»¶
        npy_files = [f for f in os.listdir(npy_dir) if f.endswith('.npy')]
        if not npy_files:
            raise ValueError(f"No .npy files found in {npy_dir}")

        print(f"ğŸ“‚ Loading chunks from {len(npy_files)} .npy files in {npy_dir}...")
        for fname in tqdm(npy_files, desc="Loading .npy files"):
            path = os.path.join(npy_dir, fname)
            try:
                data = np.load(path, allow_pickle=True)
                for item in data:
                    chunk = item['chunk_data']
                    if chunk.shape[0] != self.expected_chunk_len:
                        print(f"âš ï¸ Skipping chunk with unexpected length {chunk.shape[0]} in {fname}")
                        continue
                    self.chunks.append(chunk.astype(np.float32))
            except Exception as e:
                print(f"âŒ Error loading {path}: {e}")

        print(f"âœ… Loaded {len(self.chunks)} valid chunks (each length={expected_chunk_len})")

    def __len__(self):
        return len(self.chunks)

    def __getitem__(self, idx):
        """
        è¿”å›å•ä¸ª chunk ä½œä¸º [1, T] å¼ é‡ï¼ˆT = expected_chunk_lenï¼‰
        """
        signal = self.chunks[idx]
        # æ³¨æ„ï¼šæ­¤å¤„ä¸å†åšå½’ä¸€åŒ–ï¼å› ä¸º .npy å·²ç»æ˜¯ huada_normalisation å¤„ç†è¿‡çš„
        # å¦‚æœä½ å¸Œæœ›åœ¨ Dataset ä¸­å†åšä¸€æ¬¡ z-scoreï¼Œå¯å–æ¶ˆä¸‹é¢æ³¨é‡Šï¼š
        # signal = (signal - np.mean(signal)) / (np.std(signal) + 1e-6)
        return torch.from_numpy(signal).float().unsqueeze(0)  # [1, T]

