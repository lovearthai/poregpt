import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
import csv
import time
from typing import Optional
import yaml
import argparse


# ç›¸å¯¹å¯¼å…¥æ¨¡å—
from .dataset import NanoporeSignalDataset
from .cnn_model import NanoporeCNNModel


# ======================================================================================
# è¾…åŠ©å‡½æ•°ï¼šæ‰“å°è®­ç»ƒé…ç½®
# ======================================================================================
def print_training_args(**kwargs):
    """ä»¥ç¾è§‚æ ¼å¼æ‰“å°æ‰€æœ‰è®­ç»ƒè¶…å‚æ•°"""
    from pprint import pformat
    print("\n" + "=" * 60)
    print(" ğŸš€ Starting CNN Autoencoder Training with the following configuration:")
    print("=" * 60)
    print(pformat(kwargs, width=100, sort_dicts=False))
    print("=" * 60 + "\n")


# ======================================================================================
# è¾…åŠ©å‡½æ•°ï¼šä¿å­˜å®Œæ•´è®­ç»ƒçŠ¶æ€ï¼ˆæ¨¡å‹ + ä¼˜åŒ–å™¨ + éšæœºçŠ¶æ€ç­‰ï¼‰
# ======================================================================================
def save_full_checkpoint(
    path: str,
    model,
    optimizer,
    scheduler,
    epoch: int,
    global_step: int,
    rank: int
):
    """ä»…åœ¨ rank=0 æ—¶ä¿å­˜å®Œæ•´ checkpointï¼Œé¿å…å¤šè¿›ç¨‹å†™å†²çª"""
    if rank != 0:
        return

    checkpoint = {
        'epoch': epoch,
        'global_step': global_step,
        'model_state_dict': model.module.state_dict(),  # DDP åŒ…è£…åéœ€ .module
        'optimizer_state_dict': optimizer.state_dict(),
        'rng_state': torch.get_rng_state(),
        'cuda_rng_state': torch.cuda.get_rng_state() if torch.cuda.is_available() else None,
        'numpy_rng_state': np.random.get_state(),
    }
    if scheduler is not None:
        checkpoint['scheduler_state_dict'] = scheduler.state_dict()
    
    torch.save(checkpoint, path)
    print(f"âœ… Full checkpoint saved to {path}")


# ======================================================================================
# è¾…åŠ©å‡½æ•°ï¼šè®°å½•è®­ç»ƒæ—¥å¿—å¹¶è¿½åŠ åˆ° CSV
# ======================================================================================
def log_and_save(
    epoch: int,
    step: int,
    total_epochs: int,
    total_steps: int,
    epoch_start_time: float,
    epoch_total_steps: int,
    avg_recon_loss: float,
    lr: float,
    loss_csv_path: str,
):
    """æ‰“å°å½“å‰è®­ç»ƒè¿›åº¦ï¼Œå¹¶å°†æŸå¤±å’Œå­¦ä¹ ç‡è¿½åŠ åˆ° CSV æ–‡ä»¶"""
    current_time = time.time()
    elapsed_seconds = current_time - epoch_start_time
    steps_done = step % epoch_total_steps or 1
    avg_time_per_step = elapsed_seconds / steps_done
    remaining_seconds = avg_time_per_step * max(0, epoch_total_steps - steps_done)

    def format_hms(seconds: float) -> str:
        seconds = int(seconds)
        h = seconds // 3600
        m = (seconds % 3600) // 60
        s = seconds % 60
        return f"{h}:{m:02d}:{s:02d}" if h > 0 else f"{m:02d}:{s:02d}"

    elapsed_str = format_hms(elapsed_seconds)
    remaining_str = format_hms(remaining_seconds)

    epoch_width = len(str(total_epochs))
    step_width = len(str(total_steps))

    print(
        f"[Epoch {epoch+1:>{epoch_width}}/{total_epochs} | "
        f"Step {step:>{step_width}}/{total_steps} | "
        f"{elapsed_str}<{remaining_str}] "
        f"Recon Loss: {avg_recon_loss:>8.6f} | "
        f"LR: {lr:>7.2e} |"
    )

    # è¿½åŠ è®­ç»ƒæ—¥å¿—åˆ° CSV
    with open(loss_csv_path, 'a', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow([epoch + 1, step, avg_recon_loss, lr])


# ======================================================================================
# è¾…åŠ©å‡½æ•°ï¼šåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹
# ======================================================================================
def validate(model, val_loader, device):
    """
    åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œè¿”å›å¹³å‡é‡å»ºæŸå¤±ï¼ˆMSEï¼‰
    æ³¨æ„ï¼šæ­¤å‡½æ•°åº”åœ¨ model.eval() æ¨¡å¼ä¸‹è°ƒç”¨
    """
    model.eval()
    val_losses = []
    with torch.no_grad():
        for batch in val_loader:
            x = batch.to(device)  # [B, 1, T]
            recon = model(x)
            loss = F.mse_loss(recon, x)
            val_losses.append(loss.item())
    return np.mean(val_losses)


# ======================================================================================
# ä¸»è®­ç»ƒå‡½æ•°ï¼šæ”¯æŒå¤šå¡ DDP + éªŒè¯ + æ—¥å¿— + æ–­ç‚¹ç»­è®­
# ======================================================================================
def cnn_train(
    npy_dir: str,
    output_model_path: str,
    batch_size: int = 16,
    lr: float = 1e-4,
    num_epochs: int = 10,
    chunk_size: int = 12000,
    num_workers: int = 8,
    prefetch_factor: int = 128,
    val_ratio: float = 0.1,               # â† å…³é”®ï¼šéªŒè¯é›†é‡‡æ ·æ¯”ä¾‹ï¼ˆå³ä½¿æœ‰ç‹¬ç«‹ val è·¯å¾„ä¹Ÿç”Ÿæ•ˆï¼‰
    val_dataset_path: Optional[str] = None,  # â† å¯é€‰ï¼šç‹¬ç«‹éªŒè¯é›†ç›®å½•
    do_evaluate: bool = True,
    loss_log_interval: int = 10,
    loss_csv_path: str = "cnn_train_loss.csv",
    use_wandb: bool = True,
    wandb_project: str = "nanopore_cnn",
    wandb_name: str = "default_cnn_run",
    lr_scheduler_type: str = "cosine",
    warmup_steps: int = 1000,
    warmup_start_factor: float = 1e-6,
    warmup_end_factor: float = 1.0,
    main_scheduler_end_factor: float = 1e-5,
    save_checkpoint_every_epoch: int = 1,
    checkpoint_path: Optional[str] = None,
    cnn_type: int = 1,
):
    """
    ä½¿ç”¨ DDP å¤šå¡è®­ç»ƒ Nanopore ä¿¡å·çš„ CNN è‡ªç¼–ç å™¨ï¼Œå¹¶åœ¨æ¯ä¸ª epoch åè¿›è¡ŒéªŒè¯ã€‚
    
    æ ¸å¿ƒé€»è¾‘ï¼š
      - ä¼˜å…ˆä½¿ç”¨ val_dataset_path ä½œä¸ºéªŒè¯æ•°æ®æºï¼›
      - **ä½†æ— è®ºæ¥æºï¼Œéƒ½åªå–å…¶ä¸­ val_ratio æ¯”ä¾‹çš„æ•°æ®ç”¨äºéªŒè¯**ï¼›
      - éªŒè¯ä»…åœ¨ rank=0 æ‰§è¡Œï¼Œé¿å…é‡å¤è®¡ç®—å’Œ I/O å†²çªï¼›
      - è®­ç»ƒå’ŒéªŒè¯æŸå¤±å‡è®°å½•åˆ° CSV å’Œ WandBã€‚
    """
    # æ‰“å°æ‰€æœ‰è®­ç»ƒå‚æ•°ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    if torch.distributed.is_available():
        print_training_args(
            npy_dir=npy_dir,
            output_model_path=output_model_path,
            batch_size=batch_size,
            lr=lr,
            num_epochs=num_epochs,
            chunk_size=chunk_size,
            num_workers=num_workers,
            prefetch_factor=prefetch_factor,
            val_ratio=val_ratio,
            val_dataset_path=val_dataset_path,
            do_evaluate=do_evaluate,
            loss_csv_path=loss_csv_path,
            use_wandb=use_wandb,
            wandb_project=wandb_project,
            wandb_name=wandb_name,
            lr_scheduler_type=lr_scheduler_type,
            warmup_steps=warmup_steps,
            cnn_type=cnn_type,
            save_checkpoint_every_epoch=save_checkpoint_every_epoch,
        )

    # ==============================
    # åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ (DDP)
    # ==============================
    import torch.distributed as dist
    from torch.nn.parallel import DistributedDataParallel as DDP
    from torch.utils.data.distributed import DistributedSampler

    dist.init_process_group(backend="nccl")
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    local_device_id = rank % torch.cuda.device_count()
    torch.cuda.set_device(local_device_id)
    device = f"cuda:{local_device_id}"

    # ==============================
    # åˆå§‹åŒ– WandBï¼ˆä»… rank=0ï¼‰
    # ==============================
    if rank == 0 and use_wandb:
        import wandb
        wandb.init(
            project=wandb_project,
            name=wandb_name,
            config={
                "batch_size": batch_size,
                "lr": lr,
                "num_epochs": num_epochs,
                "chunk_size": chunk_size,
                "cnn_type": cnn_type,
                "world_size": world_size,
                "val_ratio": val_ratio,
                "val_dataset_path": val_dataset_path,
            }
        )
    else:
        wandb = None

    # ==============================
    # åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶ï¼ˆä»… rank=0ï¼‰
    # ==============================
    if rank == 0:
        print(f"ğŸš€ Using {world_size} GPUs.")
        print(f"ğŸ“‚ Train Data: {npy_dir}")
        if val_dataset_path:
            print(f"ğŸ” External val dataset path provided: {val_dataset_path}")
        else:
            print("ğŸ” No external val dataset; will sample from train data.")
        print(f"ğŸ’¾ Final model will be saved to: {output_model_path}")

        # åˆ›å»º CSV æ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´
        with open(loss_csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['epoch', 'step', 'recon_loss', 'lr'])

    # ==============================
    # æ„å»ºè®­ç»ƒæ•°æ®é›† + DataLoader
    # ==============================
    dataset = NanoporeSignalDataset(shards_dir=npy_dir)
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        sampler=sampler,
        num_workers=num_workers,
        prefetch_factor=prefetch_factor,
        pin_memory=True,
        drop_last=True
    )
    # ==============================
    # [DEBUG] æ‰“å°ä¸€ä¸ª batch çš„è¾“å…¥æ•°æ®ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»… rank=0ï¼‰
    # ==============================
    if rank == 0:
        print("\nğŸ” [DEBUG] Inspecting first batch of training data...")
        for batch in dataloader:
            x_sample = batch[0]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬: [1, T]
            print(f"  Shape: {x_sample.shape}")
            print(f"  Min: {x_sample.min().item():.4f}")
            print(f"  Max: {x_sample.max().item():.4f}")
            print(f"  Mean: {x_sample.mean().item():.4f}")
            print(f"  Std: {x_sample.std().item():.4f}")
            print(f"  First 20 values: {x_sample.flatten()[:20].cpu().numpy()}")
            print(f"  Last 20 values: {x_sample.flatten()[-20:].cpu().numpy()}")
            break  # åªçœ‹ç¬¬ä¸€ä¸ª batch çš„ç¬¬ä¸€ä¸ªæ ·æœ¬
    print("âœ… Debug inspection done.\n")
    # ==============================
    # æ„å»ºéªŒè¯æ•°æ®é›†ï¼ˆä»… rank=0ï¼‰
    # ==============================
    val_loader = None
    if do_evaluate and rank == 0:
        # Step 1: ç¡®å®šéªŒè¯æ•°æ®æ¥æº
        if val_dataset_path and os.path.isdir(val_dataset_path) and os.listdir(val_dataset_path):
            full_val_dataset = NanoporeSignalDataset(shards_dir=val_dataset_path)
            print(f"âœ… Loaded external validation dataset ({len(full_val_dataset)} chunks).")
        else:
            full_val_dataset = NanoporeSignalDataset(shards_dir=npy_dir)
            print(f"âš ï¸ No valid external val dataset. Using training data as fallback.")

        # Step 2: ã€å…³é”®é€»è¾‘ã€‘æ— è®ºæ¥æºï¼Œéƒ½æŒ‰ val_ratio é‡‡æ ·å­é›†
        actual_val_size = max(1, int(val_ratio * len(full_val_dataset)))
        np.random.seed(42)  # å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿å®éªŒå¯å¤ç°
        indices = np.random.choice(len(full_val_dataset), size=actual_val_size, replace=False)
        val_dataset = torch.utils.data.Subset(full_val_dataset, indices)

        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,          # éªŒè¯æ—¶ä¸æ‰“ä¹±
            num_workers=max(2, num_workers // 2),
            pin_memory=True
        )
        print(f"ğŸ“Š Validation set size after {val_ratio:.1%} sampling: {len(val_dataset)}")

    # ==============================
    # æ„å»ºæ¨¡å‹ã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨
    # ==============================
    model = NanoporeCNNModel(cnn_type=cnn_type).to(device)
    model = DDP(model, device_ids=[local_device_id])

    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    total_training_steps = len(dataloader) * num_epochs
    scheduler = None

    if rank == 0 and lr_scheduler_type != "constant":
        print(f"ğŸ“ˆ LR Scheduler: {lr_scheduler_type}, warmup={warmup_steps}")

    if lr_scheduler_type != "constant":
        from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR
        warmup_scheduler = LinearLR(optimizer, start_factor=warmup_start_factor, end_factor=warmup_end_factor, total_iters=warmup_steps)
        main_steps = max(1, total_training_steps - warmup_steps)

        if lr_scheduler_type == "cosine":
            main_scheduler = CosineAnnealingLR(optimizer, T_max=main_steps)
        elif lr_scheduler_type == "linear":
            rel_factor = max(1e-8, min(1.0, main_scheduler_end_factor / warmup_end_factor))
            main_scheduler = LinearLR(optimizer, start_factor=1.0, end_factor=rel_factor, total_iters=main_steps)
        else:
            raise ValueError(f"Unsupported scheduler: {lr_scheduler_type}")

        scheduler = SequentialLR(optimizer, [warmup_scheduler, main_scheduler], milestones=[warmup_steps])

    # ==============================
    # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆæ–­ç‚¹ç»­è®­ï¼‰
    # ==============================
    start_epoch = 0
    start_global_step = 0
    if checkpoint_path and rank == 0:
        if os.path.isfile(checkpoint_path):
            print(f"ğŸ“¥ Loading checkpoint: {checkpoint_path}")
        else:
            print(f"âš ï¸ Checkpoint not found. Training from scratch.")
            checkpoint_path = None

    # å¹¿æ’­åŠ è½½æ ‡å¿—åˆ°æ‰€æœ‰è¿›ç¨‹
    load_flag = torch.tensor([1 if checkpoint_path else 0], dtype=torch.int32, device=device)
    if rank == 0:
        load_flag[0] = int(os.path.isfile(checkpoint_path)) if checkpoint_path else 0
    dist.broadcast(load_flag, src=0)

    if load_flag.item() == 1:
        ckpt = torch.load(checkpoint_path, map_location=device, weights_only=False)
        model.load_state_dict(ckpt['model_state_dict'])
        optimizer.load_state_dict(ckpt['optimizer_state_dict'])
        if scheduler and 'scheduler_state_dict' in ckpt:
            scheduler.load_state_dict(ckpt['scheduler_state_dict'])
        if rank == 0:
            torch.set_rng_state(ckpt['rng_state'])
            if ckpt.get('cuda_rng_state') is not None:
                torch.cuda.set_rng_state(ckpt['cuda_rng_state'])
            np.random.set_state(ckpt['numpy_rng_state'])
            start_epoch = ckpt.get('epoch', -1) + 1
            start_global_step = ckpt.get('global_step', 0)
            print(f"âœ… Resuming from epoch {start_epoch}")

    # ==============================
    # ä¸»è®­ç»ƒå¾ªç¯
    # ==============================
    global_step = start_global_step
    total_steps = len(dataloader) * num_epochs

    for epoch in range(start_epoch, num_epochs):
        epoch_start_time = time.time()
        sampler.set_epoch(epoch)  # ç¡®ä¿æ¯ä¸ª epoch æ‰“ä¹±ä¸åŒ
        model.train()

        recon_losses = []

        for step, batch in enumerate(dataloader):
            global_step += 1
            x = batch.to(device)  # [B, 1, T]

            # å‰å‘ + æŸå¤±
            recon = model(x)
            loss = F.mse_loss(recon, x)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # æ›´æ–°å­¦ä¹ ç‡
            if scheduler is not None:
                scheduler.step()

            recon_losses.append(loss.item())

            # å®šæœŸè®°å½•è®­ç»ƒæ—¥å¿—
            if (step + 1) % loss_log_interval == 0 or step == len(dataloader) - 1:
                avg_recon = np.mean(recon_losses)
                recon_losses.clear()

                # å¤šå¡åŒæ­¥å¹³å‡æŸå¤±
                avg_tensor = torch.tensor(avg_recon, device=device)
                dist.all_reduce(avg_tensor, op=dist.ReduceOp.AVG)
                avg_recon = avg_tensor.item()

                if rank == 0:
                    current_lr = optimizer.param_groups[0]['lr']
                    log_and_save(
                        epoch=epoch,
                        step=global_step,
                        total_epochs=num_epochs,
                        total_steps=total_steps,
                        epoch_start_time=epoch_start_time,
                        epoch_total_steps=len(dataloader),
                        avg_recon_loss=avg_recon,
                        lr=current_lr,
                        loss_csv_path=loss_csv_path,
                    )

                    if use_wandb:
                        wandb.log({
                            "train/recon_loss": avg_recon,
                            "learning_rate": current_lr,
                            "epoch": epoch + 1,
                        }, step=global_step)

        # ==============================
        # âœ… æ¯ä¸ª epoch ç»“æŸåæ‰§è¡ŒéªŒè¯
        # ==============================
        if do_evaluate and rank == 0 and val_loader is not None:
            val_loss = validate(model.module, val_loader, device)  # æ³¨æ„ï¼šç”¨ .module è§£åŒ… DDP
            current_lr = optimizer.param_groups[0]['lr']

            # æ‰“å°éªŒè¯ç»“æœ
            print(
                f"[Epoch {epoch+1}/{num_epochs}] "
                f"âœ… Val Recon Loss: {val_loss:>8.6f} | "
                f"LR: {current_lr:>7.2e}"
            )

            # å°†éªŒè¯ç»“æœå†™å…¥ CSVï¼ˆstep åˆ—ç”¨å­—ç¬¦ä¸² 'validation' æ ‡è®°ï¼‰
            with open(loss_csv_path, 'a', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow([epoch + 1, 'validation', val_loss, current_lr])

            # è®°å½•åˆ° WandB
            if use_wandb:
                wandb.log({
                    "val/recon_loss": val_loss,
                    "epoch": epoch + 1,
                }, step=global_step)

        # æ‰€æœ‰è¿›ç¨‹ç­‰å¾… rank=0 å®ŒæˆéªŒè¯ï¼ˆé¿å… race conditionï¼‰
        dist.barrier()

        # å®šæœŸä¿å­˜ checkpoint
        if rank == 0 and (epoch + 1) % save_checkpoint_every_epoch == 0:
            ckpt_path = f"{output_model_path}.epoch{epoch+1}.pth"
            save_full_checkpoint(
                path=ckpt_path,
                model=model,
                optimizer=optimizer,
                scheduler=scheduler,
                epoch=epoch,
                global_step=global_step,
                rank=rank
            )

    # ==============================
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    # ==============================
    if rank == 0:
        save_full_checkpoint(
            path=output_model_path,
            model=model,
            optimizer=optimizer,
            scheduler=scheduler,
            epoch=num_epochs - 1,
            global_step=global_step,
            rank=rank
        )
        print(f"âœ… Final model saved to {output_model_path}")
        if use_wandb:
            wandb.finish()

    # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
    dist.barrier()
    dist.destroy_process_group()
def main():
    # å®šä¹‰ä¸€ä¸ªç®€å•çš„è§£æå™¨ï¼Œåªç”¨äºè·å– config æ–‡ä»¶è·¯å¾„
    parser = argparse.ArgumentParser(description="Train Nanopore Signal CNN using a YAML config file.")
    parser.add_argument("--config", type=str, required=True, help="Path to the YAML config file.")
    args, _ = parser.parse_known_args() # è§£æå·²çŸ¥å‚æ•°ï¼ˆä¸»è¦æ˜¯ --configï¼‰ï¼Œå¿½ç•¥å…¶ä»–å¯èƒ½ä¼ å…¥çš„å‚æ•°

    # è¯»å– YAML é…ç½®æ–‡ä»¶
    config_file_path = args.config # ä½¿ç”¨å‘½ä»¤è¡Œä¼ å…¥çš„è·¯å¾„
    with open(config_file_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # ä»é…ç½®å­—å…¸ä¸­æå–å‚æ•°ï¼Œå¹¶ä½¿ç”¨ get è®¾ç½®é»˜è®¤å€¼
    # ä» data éƒ¨åˆ†æå–
    npy_dir = config.get('data', {}).get('npy_dir', '')
    val_dataset_path = config.get('data', {}).get('val_dataset_path', None)

    # ä» training éƒ¨åˆ†æå–
    output_model_path = config.get('training', {}).get('output_model_path', "demo_nanopore_vq_tokenizer.pth")
    batch_size = config.get('training', {}).get('batch_size', 16)
    lr = config.get('training', {}).get('lr', 3e-4)
    num_epochs = config.get('training', {}).get('num_epochs', 10)
    chunk_size = config.get('training', {}).get('chunk_size', 12000)
    num_workers = config.get('training', {}).get('num_workers', 8)
    val_ratio = config.get('training', {}).get('val_ratio', 0.1)
    loss_csv_path = config.get('training', {}).get('loss_csv_path', "train_loss.csv")
    loss_log_interval = config.get('training', {}).get('loss_log_interval', 10)
    checkpoint_path = config.get('training', {}).get('checkpoint_path', "checkpoint_nanopore_vq_tokenizer.pth")
    cnn_type = config.get('training', {}).get('cnn_type', 0)
    prefetch_factor = config.get('training', {}).get('prefetch_factor', 128)

    # ä» logging éƒ¨åˆ†æå–
    do_evaluate = config.get('logging', {}).get('do_evaluate', False) # é»˜è®¤ä¸º Falseï¼Œä¸ argparse çš„ store_true è¡Œä¸ºä¸åŒ
    use_wandb = config.get('logging', {}).get('use_wandb', True)
    wandb_project = config.get('logging', {}).get('wandb_project', 'nanopore_cnn')
    wandb_name = config.get('logging', {}).get('wandb_name', 'default_cnn_run')

    # ä» scheduler éƒ¨åˆ†æå–
    lr_scheduler_type = config.get('scheduler', {}).get('lr_scheduler_type', 'cosine')
    warmup_steps = config.get('scheduler', {}).get('warmup_steps', 1000)
    warmup_start_factor = config.get('scheduler', {}).get('warmup_start_factor', 1e-6)
    warmup_end_factor = config.get('scheduler', {}).get('warmup_end_factor', 1.0)
    main_scheduler_end_factor = config.get('scheduler', {}).get('main_scheduler_end_factor', 1e-5)

    # ä» checkpointing éƒ¨åˆ†æå–
    save_checkpoint_every_epoch = config.get('checkpointing', {}).get('save_checkpoint_every_epoch', 1)

    # è°ƒç”¨ cnn_train å‡½æ•°
    cnn_train(
        npy_dir=npy_dir,
        output_model_path=output_model_path,
        batch_size=batch_size,
        lr=lr,
        num_epochs=num_epochs,
        chunk_size=chunk_size,
        num_workers=num_workers,
        prefetch_factor=prefetch_factor,
        val_ratio=val_ratio,
        val_dataset_path=val_dataset_path,
        do_evaluate=do_evaluate,
        loss_log_interval=loss_log_interval,
        loss_csv_path=loss_csv_path,
        use_wandb=use_wandb,
        wandb_project=wandb_project,
        wandb_name=wandb_name,
        lr_scheduler_type=lr_scheduler_type,
        warmup_steps=warmup_steps,
        warmup_start_factor=warmup_start_factor,
        warmup_end_factor=warmup_end_factor,
        main_scheduler_end_factor=main_scheduler_end_factor,
        save_checkpoint_every_epoch=save_checkpoint_every_epoch,
        checkpoint_path=checkpoint_path,
        cnn_type=cnn_type
    )

if __name__ == "__main__":
    main()

