import torch
import torch.nn as nn
import torch.nn.functional as F
from vector_quantize_pytorch import VectorQuantize
from typing import Tuple, Dict
# å¯¼å…¥æ–°çš„ CNN æ¨¡å‹
from .cnn_model import NanoporeCNNModel
class Conv1dWithMeanChannel(nn.Module):
    """
    Conv1då±‚ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªè¾“å‡ºé€šé“ï¼ˆç´¢å¼•0ï¼‰æ˜¯è¾“å…¥ä¿¡å·åœ¨å·ç§¯æ ¸çª—å£å†…çš„å‡å€¼ã€‚
    å…¶ä½™çš„è¾“å‡ºé€šé“ç”±æ ‡å‡†å·ç§¯æ“ä½œç”Ÿæˆã€‚
    æ³¨æ„ï¼šæ­¤ç‰ˆæœ¬çš„ in_channels å›ºå®šä¸º 1ï¼Œå¹¶ä½¿ç”¨ä¼˜åŒ–çš„å‡å€¼è®¡ç®—æ–¹æ³•ã€‚
    """
    def __init__(self, out_channels, kernel_size, stride=1, padding=0, bias=False):
        super(Conv1dWithMeanChannel, self).__init__()
        self.in_channels = 1  # å›ºå®šä¸º 1
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding

        if out_channels <= 0:
            raise ValueError(f"out_channels å¿…é¡»ä¸ºæ­£æ•°ï¼Œå¾—åˆ°çš„æ˜¯ {out_channels}")

        # åˆ›å»ºä¸€ä¸ªä¸“é—¨ç”¨äºè®¡ç®—å‡å€¼çš„å·ç§¯å±‚
        # æƒé‡åˆå§‹åŒ–ä¸º 1/kernel_sizeï¼Œä½¿å¾—å·ç§¯ç»“æœä¸ºå¹³å‡å€¼
        # åç½®è®¾ä¸º 0
        self.mean_conv = nn.Conv1d(
            in_channels=1,
            out_channels=1,  # åªéœ€è¦ä¸€ä¸ªè¾“å‡ºé€šé“æ¥å­˜æ”¾å‡å€¼
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            bias=False # ä¸éœ€è¦åç½®
        )
        # å°†æƒé‡è®¾ç½®ä¸º 1/kernel_size
        with torch.no_grad():
            self.mean_conv.weight.fill_(1.0 / kernel_size)

        # æˆ‘ä»¬éœ€è¦è‡³å°‘1ä¸ªé€šé“æ¥å­˜æ”¾å‡å€¼ã€‚å¦‚æœ out_channels > 1ï¼Œ
        # å¯¹å…¶ä½™çš„ (out_channels - 1) ä¸ªé€šé“æ‰§è¡Œæ ‡å‡†å·ç§¯ã€‚
        self.use_standard_conv = out_channels > 1
        if self.use_standard_conv:
            # ä¸ºå…¶ä½™ (out_channels - 1) ä¸ªé€šé“åˆ›å»ºæ ‡å‡†å·ç§¯å±‚
            self.std_conv = nn.Conv1d(1, out_channels - 1, kernel_size, stride=stride, padding=padding, bias=bias)

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­å‡½æ•°ã€‚

        Args:
            x (torch.Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º [Batch_Size, 1, Input_Length] (å› ä¸º in_channels å›ºå®šä¸º 1)

        Returns:
            torch.Tensor: è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸º [Batch_Size, out_channels, Output_Length]
                          å…¶ä¸­ç¬¬ä¸€ä¸ªé€šé“æ˜¯è¾“å…¥çš„å±€éƒ¨å‡å€¼ã€‚
        """
        # --- è®¡ç®—å±€éƒ¨å‡å€¼ (ä¼˜åŒ–ç‰ˆ) ---
        # ç›´æ¥ä½¿ç”¨é¢„è®¾æƒé‡çš„å·ç§¯å±‚æ¥è®¡ç®—å‡å€¼
        # è¯¥å·ç§¯å±‚çš„æƒé‡ä¸º [1/kernel_size, 1/kernel_size, ..., 1/kernel_size]
        # å·ç§¯è¿ç®—è‡ªåŠ¨å®Œæˆäº†æ±‚å’Œä¸é™¤æ³•ï¼Œå¾—åˆ°å‡å€¼
        mean_channel = self.mean_conv(x) # [B, 1, L_out]

        # --- æ„é€ æœ€ç»ˆè¾“å‡º ---
        if not self.use_standard_conv:
            # å¦‚æœåªéœ€è¦1ä¸ªè¾“å‡ºé€šé“ï¼Œåˆ™ç›´æ¥è¿”å›è®¡ç®—å‡ºçš„å‡å€¼é€šé“
            return mean_channel

        # --- å¦‚æœéœ€è¦æ›´å¤šé€šé“ ---
        # å¯¹è¾“å…¥xæ‰§è¡Œæ ‡å‡†å·ç§¯ï¼Œç”Ÿæˆå…¶ä½™çš„ (out_channels - 1) ä¸ªé€šé“
        std_conv_out = self.std_conv(x) # [B, out_ch - 1, L_out]

        # å°†è®¡ç®—å‡ºçš„å‡å€¼é€šé“ï¼ˆä½œä¸ºç¬¬ä¸€ä¸ªï¼‰ä¸æ ‡å‡†å·ç§¯çš„ç»“æœé€šé“æ‹¼æ¥èµ·æ¥
        output = torch.cat([mean_channel, std_conv_out], dim=1) # [B, out_ch, L_out]

        return output


class NanoporeVQModel(nn.Module):
    """
    Nanopore VQ Tokenizer for Direct RNA Sequencing (130 bps, 4 kHz)

    æ”¯æŒå¤šç§ CNN æ¶æ„é…ç½®ï¼Œé€šè¿‡ `cnn_type` åˆ‡æ¢ï¼š
        - cnn_type=0: å¤§å®¹é‡éä¸¥æ ¼å¯¹ç§°æ¨¡å‹ï¼ˆé»˜è®¤ï¼‰
        - cnn_type=1: å°å®¹é‡ä¸¥æ ¼å¯¹ç§°æ¨¡å‹ï¼ˆé€šé“æ•° 1â†’16â†’32â†’64ï¼‰

    è®¾è®¡ç›®æ ‡é€šç”¨ï¼š
        - æ„Ÿå—é‡ â‰ˆ 33 é‡‡æ ·ç‚¹ï¼ˆâ‰ˆ1 ä¸ª RNA ç¢±åŸºï¼‰
        - æ€»ä¸‹é‡‡æ ·ç‡ = 5Ã—ï¼ˆæ¯ç¢±åŸº â‰ˆ6 ä¸ª tokensï¼‰
        - è¾“å‡º codebook_dim ç»´ latentï¼Œç›´æ¥ç”¨äº VQ
        - Decoder åœ¨ cnn_type=1 æ—¶ä¸¥æ ¼å¯¹ç§°äº encoder

    é€‚ç”¨äºï¼šVQ tokenizer + LLM basecalling pipeline
    """

    def __init__(
        self,
        codebook_size: int = 8192,
        commitment_weight: float = 1.0,
        orthogonal_reg_weight: float = 1.0,
        codebook_diversity_loss_weight: float = 1.0,
        cnn_type: int = 0,
        learnable_codebook: bool= True,
        init_codebook_path: str = None,
        freeze_cnn: bool = False,
        cnn_checkpoint_path: str = None
    ):
        """
        åˆå§‹åŒ– NanoporeVQModelã€‚

        Args:
            codebook_size (int): VQ ç æœ¬å¤§å°ã€‚
            codebook_dim (int): VQ åµŒå…¥ç»´åº¦ï¼ˆå³ encoder æœ€ç»ˆè¾“å‡ºé€šé“æ•°ï¼‰ã€‚
            commitment_weight (float): VQ commitment loss æƒé‡ã€‚
            orthogonal_reg_weight (float): æ­£äº¤æ­£åˆ™åŒ–æƒé‡ã€‚
            codebook_diversity_loss_weight (float): ç æœ¬å¤šæ ·æ€§æŸå¤±æƒé‡ã€‚
            cnn_type (int): CNN æ¶æ„ç±»å‹ã€‚
                - 0: é»˜è®¤å¤§æ¨¡å‹ï¼ˆ1 â†’ 64 â†’ 128 â†’ codebook_dimï¼‰
                - 1: ä¸¥æ ¼å¯¹ç§°å°æ¨¡å‹ï¼ˆ1 â†’ 16 â†’ 32 â†’ 64ï¼‰ï¼Œæ­¤æ—¶ codebook_dim å¿…é¡»ä¸º 64
        """
        super().__init__()

        # è®¾ç½® codebook_dim æ ¹æ® cnn_type
        if cnn_type == 0:
            codebook_dim = 256
        elif cnn_type == 1:
            codebook_dim = 64
        elif cnn_type == 2:
            codebook_dim = 512  # å›ºå®šä¸º 512ï¼Œä¸ä½ æä¾›çš„ç»“æ„ä¸€è‡´
        elif cnn_type == 3:
            codebook_dim = 64  # å›ºå®šä¸º 512ï¼Œä¸ä½ æä¾›çš„ç»“æ„ä¸€è‡´
        else:
            raise ValueError(f"Unsupported cnn_type: {cnn_type}. Supported: 0, 1, or 2.")

        self.codebook_dim = codebook_dim
        self.cnn_type = cnn_type
        self.latent_dim = codebook_dim
        self.codebook_size = codebook_size
        print(f"codebook_dim:{codebook_dim}")
        # æ„å»º encoder å’Œ decoder
        if cnn_type == 0:
            self._build_encoder_type0()
            self._build_decoder_type0()
            self.cnn_stride = 5   # æ€»ä¸‹é‡‡æ ·ç‡ï¼ˆä»…æœ€åä¸€å±‚ stride=5ï¼‰
            self.RF = 33          # æ„Ÿå—é‡ï¼ˆé‡‡æ ·ç‚¹ï¼‰ï¼Œå¯¹åº” ~1 ä¸ª RNA ç¢±åŸº
        elif cnn_type == 1:
            self._build_encoder_type1()
            self._build_decoder_type1()
            self.cnn_stride = 5   # æ€»ä¸‹é‡‡æ ·ç‡ï¼ˆä»…æœ€åä¸€å±‚ stride=5ï¼‰
            self.RF = 33          # æ„Ÿå—é‡ï¼ˆé‡‡æ ·ç‚¹ï¼‰ï¼Œå¯¹åº” ~1 ä¸ª RNA ç¢±åŸº
        elif cnn_type == 2:
            self._build_encoder_type2()
            self._build_decoder_type2()
            self.cnn_stride = 12  # 1 * 1 * 3 * 2 * 2
            self.RF = 65  #
        elif cnn_type == 3:
            self._build_encoder_type3()
            self._build_decoder_type3()
            self.cnn_stride = 5   # æ€»ä¸‹é‡‡æ ·ç‡ï¼ˆä»…æœ€åä¸€å±‚ stride=5ï¼‰
            self.RF = 33          # æ„Ÿå—é‡ï¼ˆé‡‡æ ·ç‚¹ï¼‰ï¼Œå¯¹åº” ~1 ä¸ª RNA ç¢±åŸº
        else:
            raise ValueError(f"Unsupported cnn_type: {cnn_type}. Supported: 0 or 1.")


        # ======================================================================
        # VECTOR QUANTIZATION (VQ)
        # ======================================================================
        rank = torch.distributed.get_rank() if torch.distributed.is_initialized() else 0
        if learnable_codebook == True:
            ema_update = False
        else:
            ema_update = True

        self.vq = VectorQuantize(
            dim=self.latent_dim,
            codebook_size=codebook_size,
            kmeans_init=True,
            kmeans_iters=10,
            decay=0.99,
            threshold_ema_dead_code=2,
            commitment_weight=commitment_weight,
            codebook_diversity_loss_weight=codebook_diversity_loss_weight,
            orthogonal_reg_weight=orthogonal_reg_weight,
            orthogonal_reg_max_codes=256,
            orthogonal_reg_active_codes_only=True,
            learnable_codebook=learnable_codebook,
            ema_update = ema_update,
        )
        
        # å¦‚æœæœ‰åˆå§‹codebookè·¯å¾„ï¼ŒåŠ è½½å®ƒ
        if init_codebook_path:
            self._load_init_codebook(init_codebook_path)
        # å¦‚æœæœ‰CNNæ£€æŸ¥ç‚¹è·¯å¾„ï¼ŒåŠ è½½æƒé‡
        if cnn_checkpoint_path:
            self._load_cnn_weights(cnn_checkpoint_path, freeze_cnn)
 

        if rank == 0:
            self._print_vq_config()
   
    def _load_cnn_weights(self, cnn_checkpoint_path, freeze_cnn=False):
        """ä»æ£€æŸ¥ç‚¹åŠ è½½CNNæƒé‡"""
        try:
            import os
            import torch
            
            if not os.path.isfile(cnn_checkpoint_path):
                print(f"âš ï¸ CNN checkpointæ–‡ä»¶ä¸å­˜åœ¨: {cnn_checkpoint_path}")
                return
            
            print(f"ğŸ“¥ ä» {cnn_checkpoint_path} åŠ è½½CNNæƒé‡")
            
            # åŠ è½½æ£€æŸ¥ç‚¹
            cnn_ckpt = torch.load(cnn_checkpoint_path, map_location='cpu',weights_only=False)
            cnn_state_dict = cnn_ckpt.get('model_state_dict', cnn_ckpt)
            
            # å¦‚æœæƒé‡æœ‰'module.'å‰ç¼€ï¼Œå»æ‰å®ƒ
            if list(cnn_state_dict.keys())[0].startswith('module.'):
                cnn_state_dict = {k.replace('module.', ''): v for k, v in cnn_state_dict.items()}
            
            # åªåŠ è½½encoderå’Œdecoderçš„æƒé‡
            encoder_decoder_keys = [k for k in cnn_state_dict.keys() 
                                   if k.startswith(('encoder.', 'decoder.'))]
            
            if not encoder_decoder_keys:
                print(f"âš ï¸ åœ¨checkpointä¸­æœªæ‰¾åˆ°encoder/decoderæƒé‡")
                return
            
            # è·å–å½“å‰æ¨¡å‹çŠ¶æ€
            model_state = self.state_dict()
            loaded_keys = []
            
            for key in encoder_decoder_keys:
                if key in model_state and cnn_state_dict[key].shape == model_state[key].shape:
                    model_state[key] = cnn_state_dict[key]
                    loaded_keys.append(key)
            
            # åŠ è½½æƒé‡
            self.load_state_dict(model_state, strict=False)
            print(f"âœ… åŠ è½½äº† {len(loaded_keys)} ä¸ªencoder/decoderå‚æ•°")
            
            # å†»ç»“å‚æ•°ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if freeze_cnn:
                print("ğŸ”’ å†»ç»“encoderå’Œdecoderå‚æ•°")
                for name, param in self.named_parameters():
                    if name.startswith(('encoder.', 'decoder.')):
                        param.requires_grad = False
            
        except Exception as e:
            print(f"âŒ åŠ è½½CNNæƒé‡å¤±è´¥: {e}")


    # åœ¨ vq_model.py ä¸­ä¿®æ”¹ _load_init_codebook å‡½æ•°
    def _load_init_codebook2(self, init_codebook_path):
        """ä»numpyæ–‡ä»¶åŠ è½½åˆå§‹codebook - åªå†™æ­»ç¬¬ä¸€ç»´ä¸º1"""
        try:
            import numpy as np
            import os
            
            if not os.path.isfile(init_codebook_path):
                print(f"âš ï¸ Codebookæ–‡ä»¶ä¸å­˜åœ¨: {init_codebook_path}")
                return
            
            # ç›´æ¥åŠ è½½numpyæ–‡ä»¶
            init_codebook = np.load(init_codebook_path)
            
            # æ‰“å°åŸå§‹å½¢çŠ¶
            print(f"ğŸ“Š åŠ è½½çš„codebookåŸå§‹å½¢çŠ¶: {init_codebook.shape}")
            
            # è·å–æ¨¡å‹æœŸæœ›çš„å½¢çŠ¶
            expected_shape = self.vq._codebook.embed.shape
            print(f"ğŸ“Š æ¨¡å‹æœŸæœ›çš„å½¢çŠ¶: {expected_shape}")
            
            # æ ¸å¿ƒä¿®å¤ï¼šå¦‚æœnumpyæ˜¯2Då½¢çŠ¶ (N, D)ï¼Œå°±å˜æˆ3D (1, N, D)
            if len(init_codebook.shape) == 2:
                # ä»2D (N, D) å˜æˆ3D (1, N, D)
                init_codebook = init_codebook[np.newaxis, :, :]
                print(f"âœ… è‡ªåŠ¨è½¬æ¢: 2D -> 3D, æ–°å½¢çŠ¶: {init_codebook.shape}")
            elif len(init_codebook.shape) == 3:
                # å·²ç»æ˜¯3Dï¼Œç›´æ¥ä½¿ç”¨
                print(f"âœ… Codebookå·²ç»æ˜¯3Då½¢çŠ¶: {init_codebook.shape}")
            else:
                print(f"âŒ ä¸æ”¯æŒçš„codebookç»´åº¦: {len(init_codebook.shape)}D")
                return
            
            # ç°åœ¨æ£€æŸ¥ç»´åº¦æ˜¯å¦åŒ¹é…
            if init_codebook.shape != expected_shape:
                print(f"âš ï¸ Codebookå½¢çŠ¶ä¸åŒ¹é…:")
                print(f"   æ¨¡å‹æœŸæœ›: {expected_shape}")
                print(f"   å®é™…å¾—åˆ°: {init_codebook.shape}")
                
                # å°è¯•åªæ¯”è¾ƒåä¸¤ä¸ªç»´åº¦
                if init_codebook.shape[1:] == expected_shape[1:]:
                    print(f"âœ… åä¸¤ä¸ªç»´åº¦åŒ¹é…ï¼Œå¯ä»¥ç»§ç»­")
                    # å½¢çŠ¶ä¸åŒ¹é…å¯èƒ½æ˜¯å› ä¸ºç¬¬ä¸€ç»´ä¸åŒï¼Œæˆ‘ä»¬ç›´æ¥å¤åˆ¶æ•°æ®
                    if isinstance(self.vq._codebook.embed, nn.Parameter):
                        with torch.no_grad():
                            # ç›´æ¥å¤åˆ¶æ•°æ®ï¼Œå¿½ç•¥ç¬¬ä¸€ç»´
                            self.vq._codebook.embed.data.copy_(torch.from_numpy(init_codebook).float())
                        print(f"âœ… CodebookåŠ è½½æˆåŠŸï¼ˆå¿½ç•¥ç¬¬ä¸€ç»´å·®å¼‚ï¼‰")
                    else:
                        self.vq._codebook.embed = torch.from_numpy(init_codebook).float()
                        print(f"âœ… CodebookåŠ è½½æˆåŠŸï¼ˆå¿½ç•¥ç¬¬ä¸€ç»´å·®å¼‚ï¼‰")
                    return
                else:
                    print(f"âŒ ç»´åº¦å®Œå…¨ä¸åŒ¹é…ï¼Œæ— æ³•åŠ è½½")
                    return
            
            # ç›´æ¥èµ‹å€¼ï¼ˆå¦‚æœæ˜¯bufferï¼‰æˆ–å¤åˆ¶ï¼ˆå¦‚æœæ˜¯parameterï¼‰
            init_codebook_tensor = torch.from_numpy(init_codebook).float()
            
            if isinstance(self.vq._codebook.embed, nn.Parameter):
                with torch.no_grad():
                    self.vq._codebook.embed.data.copy_(init_codebook_tensor)
            else:
                # å¦‚æœæ˜¯bufferï¼Œç›´æ¥èµ‹å€¼
                self.vq._codebook.embed = init_codebook_tensor
            
            print(f"âœ… ä» {init_codebook_path} åŠ è½½åˆå§‹codebookæˆåŠŸ")
            print(f"   æœ€ç»ˆå½¢çŠ¶: {init_codebook_tensor.shape}")
            print(f"   æ˜¯å¦å¯å­¦ä¹ : {isinstance(self.vq._codebook.embed, nn.Parameter)}")
            
        except Exception as e:
            print(f"âŒ åŠ è½½åˆå§‹codebookå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    # åœ¨ vq_model.py ä¸­ä¿®æ”¹ _load_init_codebook æ–¹æ³•
    def _load_init_codebook(self, init_codebook_path):
        """ä»numpyæ–‡ä»¶åŠ è½½åˆå§‹codebook - ä¿®å¤å†…å­˜å¸ƒå±€é—®é¢˜"""
        try:
            import numpy as np
            import os
            
            if not os.path.isfile(init_codebook_path):
                print(f"âš ï¸ Codebookæ–‡ä»¶ä¸å­˜åœ¨: {init_codebook_path}")
                return
            
            # ç›´æ¥åŠ è½½numpyæ–‡ä»¶
            init_codebook = np.load(init_codebook_path)
            print(f"ğŸ“¥ åŠ è½½codebook: {init_codebook.shape}")
            
            # å¦‚æœå½¢çŠ¶æ˜¯2Dï¼Œæ·»åŠ ä¸€ä¸ªç»´åº¦å˜æˆ3D
            if len(init_codebook.shape) == 2:
                init_codebook = init_codebook[np.newaxis, :, :]
                print(f"  -> è°ƒæ•´ä¸º3D: {init_codebook.shape}")
            
            # è½¬æ¢ä¸ºtensor - ä½¿ç”¨ä¸æ¨¡å‹å‚æ•°ç›¸åŒçš„è®¾å¤‡
            device = self.vq._codebook.embed.device
            init_codebook_tensor = torch.from_numpy(init_codebook).float().to(device)
            
            # å…³é”®ä¿®å¤ï¼šç¡®ä¿å†…å­˜å¸ƒå±€ä¸€è‡´
            # ä½¿ç”¨ contiguous() ç¡®ä¿å†…å­˜è¿ç»­
            init_codebook_tensor = init_codebook_tensor.contiguous()
            
            # è·å–åŸå§‹å‚æ•°çš„å¼•ç”¨
            embed_param = self.vq._codebook.embed
            
            # å¦‚æœæ˜¯Parameterï¼Œç›´æ¥ä¿®æ”¹data
            if isinstance(embed_param, nn.Parameter):
                with torch.no_grad():
                    # ç¡®ä¿ç›®æ ‡ä¹Ÿæ˜¯è¿ç»­çš„
                    embed_param.data = embed_param.data.contiguous()
                    # å¤åˆ¶æ•°æ®
                    embed_param.data.copy_(init_codebook_tensor)
            else:
                # å¦‚æœæ˜¯bufferï¼Œç›´æ¥èµ‹å€¼ä½†ä¿æŒå†…å­˜å¸ƒå±€
                self.vq._codebook.embed = init_codebook_tensor.contiguous()
            
            print(f"âœ… Codebookåˆå§‹åŒ–æˆåŠŸ")
            print(f"   æœ€ç»ˆå½¢çŠ¶: {self.vq._codebook.embed.shape}")
            print(f"   å†…å­˜è¿ç»­: {self.vq._codebook.embed.is_contiguous()}")
            
        except Exception as e:
            print(f"âŒ åŠ è½½åˆå§‹codebookå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()


    def _print_vq_config(self) -> None:
        """æ‰“å° VQ é…ç½®ä¿¡æ¯ï¼ˆä»… rank 0ï¼‰"""
        print("Intialized VectorQuantize with the following hyperparameters:")
        print(f"  dim: {self.latent_dim}")
        print(f"  codebook_size: {self.codebook_size}")
        print(f"  kmeans_init: True")
        print(f"  kmeans_iters: 10")
        print(f"  decay: 0.99")
        print(f"  threshold_ema_dead_code: 2")
        print(f"  commitment_weight: {self.vq.commitment_weight}")
        print(f"  codebook_diversity_loss_weight: {self.vq.codebook_diversity_loss_weight}")
        print(f"  orthogonal_reg_weight: {self.vq.orthogonal_reg_weight}")
        print(f"  orthogonal_reg_max_codes: 256")
        print(f"  orthogonal_reg_active_codes_only: True")
        print(f"  cnn_type: {self.cnn_type}")
        print("-" * 60)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ENCODER BUILDERS
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _build_encoder_type0(self) -> None:
        """æ„å»º cnn_type=0 çš„ encoderï¼š1 â†’ 64 â†’ 128 â†’ latent_dimï¼ˆå¦‚ 256ï¼‰"""
        self.encoder = nn.Sequential(
            # Layer 1: è¶…å±€éƒ¨ç‰¹å¾æå–ï¼ˆæ— ä¸‹é‡‡æ ·ï¼‰
            nn.Conv1d(1, 64, kernel_size=5, stride=1, padding=2, bias=False),
            nn.BatchNorm1d(64),
            nn.SiLU(),

            # Layer 2: å±€éƒ¨ä¸Šä¸‹æ–‡èšåˆï¼ˆæ— ä¸‹é‡‡æ ·ï¼‰
            nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2, bias=False),
            nn.BatchNorm1d(128),
            nn.SiLU(),

            # Layer 3: ä¸‹é‡‡æ · + å‡ç»´è‡³ latent spaceï¼ˆRF=33, stride=5ï¼‰
            nn.Conv1d(128, self.latent_dim, kernel_size=25, stride=5, padding=12, bias=False),
            nn.BatchNorm1d(self.latent_dim),
        )

    def _build_encoder_type1(self) -> None:
        """æ„å»º cnn_type=1 çš„ encoderï¼š1 â†’ 16 â†’ 32 â†’ 64ï¼ˆä¸¥æ ¼å¯¹ç§°ï¼‰"""
        self.encoder = nn.Sequential(
            # Layer 1: 1 â†’ 16
            nn.Conv1d(1, 16, kernel_size=5, stride=1, padding=2, bias=False),
            nn.BatchNorm1d(16),
            nn.SiLU(),

            # Layer 2: 16 â†’ 32
            nn.Conv1d(16, 32, kernel_size=5, stride=1, padding=2, bias=False),
            nn.BatchNorm1d(32),
            nn.SiLU(),

            # Layer 3: 32 â†’ 64, stride=5, RF=33
            nn.Conv1d(32, 64, kernel_size=25, stride=5, padding=12, bias=False),
            nn.BatchNorm1d(64),
        )
    def _build_encoder_type2(self) -> None:
        """cnn_type=2: å¤šé˜¶æ®µä¸‹é‡‡æ ·ï¼Œæ€» stride=12ï¼Œè¾“å‡ºé€šé“=512"""
        self.encoder = nn.Sequential(
            # Layer 1: 1 â†’ 64, stride=1
            nn.Conv1d(1, 64, kernel_size=5, stride=1, padding=2, bias=False),
            nn.BatchNorm1d(64),
            nn.SiLU(),

            # Layer 2: 64 â†’ 64, stride=1
            nn.Conv1d(64, 64, kernel_size=5, stride=1, padding=2, bias=False),
            nn.BatchNorm1d(64),
            nn.SiLU(),

            # Layer 3: 64 â†’ 128, stride=3
            nn.Conv1d(64, 128, kernel_size=9, stride=3, padding=4, bias=False),
            nn.BatchNorm1d(128),
            nn.SiLU(),

            # Layer 4: 128 â†’ 128, stride=2
            nn.Conv1d(128, 128, kernel_size=9, stride=2, padding=4, bias=False),
            nn.BatchNorm1d(128),
            nn.SiLU(),

            # Layer 5: 128 â†’ 512, stride=2
            nn.Conv1d(128, self.latent_dim, kernel_size=5, stride=2, padding=2, bias=False),
            nn.BatchNorm1d(self.latent_dim),
        )
    def _build_encoder_type3(self) -> None:
        """æ„å»º cnn_type=1 çš„ encoderï¼š1 â†’ 16 â†’ 32 â†’ 64ï¼ˆä¸¥æ ¼å¯¹ç§°ï¼‰
        Modified: First layer has the first channel as local mean.
        """
        self.encoder = nn.Sequential(
            # Layer 1: 1 â†’ 16, ç¬¬ä¸€ä¸ªé€šé“(kernel_size=5åŒºåŸŸå†…çš„å‡å€¼)ï¼Œå…¶ä½™15ä¸ªé€šé“æ¥è‡ªæ ‡å‡†å·ç§¯
            # æ³¨æ„ï¼šè°ƒç”¨æ—¶ä¸å†éœ€è¦ä¼ å…¥ in_channelsï¼Œå› ä¸ºå®ƒå·²è¢«å›ºå®šä¸º 1
            Conv1dWithMeanChannel(out_channels=16, kernel_size=5, stride=1, padding=2, bias=False),
            nn.BatchNorm1d(16),
            nn.SiLU(),

            # Layer 2: 16 â†’ 32
            nn.Conv1d(16, 32, kernel_size=5, stride=1, padding=2, bias=False),
            nn.BatchNorm1d(32),
            nn.SiLU(),

            # Layer 3: 32 â†’ 64, stride=5, RF=33
            nn.Conv1d(32, 64, kernel_size=25, stride=5, padding=12, bias=False),
            nn.BatchNorm1d(64),
        )


    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # DECODER BUILDERS
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _build_decoder_type0(self) -> None:
        """æ„å»º cnn_type=0 çš„ decoderï¼ˆè¿‘ä¼¼å¯¹ç§°ï¼Œé«˜ç»´ refineï¼‰"""
        self.decoder = nn.Sequential(
            # Upsample Ã—5: é€†æ“ä½œ encoder æœ€åä¸€å±‚
            nn.ConvTranspose1d(
                in_channels=self.latent_dim,
                out_channels=128,
                kernel_size=25,
                stride=5,
                padding=12,
                output_padding=0,
                bias=False,
            ),
            nn.BatchNorm1d(128),
            nn.SiLU(),

            # Refine layer: æ¶ˆé™¤æ£‹ç›˜ä¼ªå½±
            nn.Conv1d(128, 64, kernel_size=5, padding=2,bias=False),
            nn.BatchNorm1d(64),
            nn.SiLU(),

            # Final projection to raw signal
            nn.Conv1d(64, 1, kernel_size=5,padding=2,bias=True),
        )

    def _build_decoder_type1(self) -> None:
        """æ„å»º cnn_type=1 çš„ decoderï¼ˆä¸¥æ ¼å¯¹ç§°ï¼š64 â†’ 32 â†’ 16 â†’ 1ï¼‰"""
        self.decoder = nn.Sequential(
            # Inverse of encoder Layer 3: 64 â†’ 32
            nn.ConvTranspose1d(
                in_channels=64,
                out_channels=32,
                kernel_size=25,
                stride=5,
                padding=12,
                output_padding=0,
                bias=False,
            ),
            nn.BatchNorm1d(32),
            nn.SiLU(),

            # Inverse of encoder Layer 2: 32 â†’ 16
            nn.Conv1d(32, 16, kernel_size=5, padding=2,bias=False),
            nn.BatchNorm1d(16),
            nn.SiLU(),

            # Inverse of encoder Layer 1: 16 â†’ 1
            nn.Conv1d(16, 1, kernel_size=5, padding=2,bias=True)
        )
    def _build_decoder_type2(self) -> None:
        """ä¸¥æ ¼å¯¹ç§° decoder: 512 â†’ 128 â†’ 128 â†’ 64 â†’ 64 â†’ 1ï¼Œä¸Šé‡‡æ ·é¡ºåºä¸ encoder ä¸‹é‡‡æ ·é€†åºå¯¹åº”"""
        self.decoder = nn.Sequential(
            # Inverse of encoder Layer 5: 512 â†’ 128, upsample Ã—2
            nn.ConvTranspose1d(512, 128, kernel_size=5, stride=2, padding=2, output_padding=0,bias=False),
            nn.BatchNorm1d(128),
            nn.SiLU(),

            # Inverse of encoder Layer 4: 128 â†’ 128, upsample Ã—2
            nn.ConvTranspose1d(128, 128, kernel_size=9, stride=2, padding=4, output_padding=0,bias=False),
            nn.BatchNorm1d(128),
            nn.SiLU(),

            # Inverse of encoder Layer 3: 128 â†’ 64, upsample Ã—3
            nn.ConvTranspose1d(128, 64, kernel_size=9, stride=3, padding=4, output_padding=0,bias=False),
            nn.BatchNorm1d(64),
            nn.SiLU(),

            # Inverse of encoder Layer 2: 64 â†’ 64
            nn.Conv1d(64, 64, kernel_size=5, padding=2,bias=False),
            nn.BatchNorm1d(64),
            nn.SiLU(),

            # Inverse of encoder Layer 1: 64 â†’ 1
            nn.Conv1d(64, 1, kernel_size=5,padding=2,bias=True)
        )
    def _build_decoder_type3(self) -> None:
        """æ„å»º cnn_type=1 çš„ decoderï¼ˆä¸¥æ ¼å¯¹ç§°ï¼š64 â†’ 32 â†’ 16 â†’ 1ï¼‰"""
        self.decoder = nn.Sequential(
            # Inverse of encoder Layer 3: 64 â†’ 32
            nn.ConvTranspose1d(
                in_channels=64,
                out_channels=32,
                kernel_size=25,
                stride=5,
                padding=12,
                output_padding=0,
                bias=False,
            ),
            nn.BatchNorm1d(32),
            nn.SiLU(),

            # Inverse of encoder Layer 2: 32 â†’ 16
            nn.Conv1d(32, 16, kernel_size=5, padding=2,bias=False),
            nn.BatchNorm1d(16),
            nn.SiLU(),
            # Inverse of encoder Layer 1: 16 â†’ 1
            nn.Conv1d(16, 1, kernel_size=5, padding=2,bias=True)
        )

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Dict]:
        """
        å‰å‘ä¼ æ’­ã€‚

        Args:
            x (torch.Tensor): è¾“å…¥ä¿¡å·ï¼Œå½¢çŠ¶ [B, 1, T]

        Returns:
            recon (torch.Tensor): é‡å»ºä¿¡å·ï¼Œ[B, 1, T]
            indices (torch.Tensor): VQ ç¦»æ•£ tokenï¼Œ[B, T//5]
            loss (torch.Tensor): VQ æ€»æŸå¤±ï¼ˆæ ‡é‡ï¼‰
            loss_breakdown (dict): æŸå¤±åˆ†é¡¹ï¼ˆcommitment, diversity, ortho...ï¼‰
        """
        # Encode: [B, 1, T] â†’ [B, C, T//5]
        z_continuous = self.encoder(x)

        # Permute for VQ: [B, C, N] â†’ [B, N, C]
        z_permuted = z_continuous.permute(0, 2, 1)

        # Quantize
        z_quantized_permuted, indices, loss, loss_breakdown = self.vq(
            z_permuted, return_loss_breakdown=True
        )

        # Back to [B, C, N] for decoder
        z_quantized = z_quantized_permuted.permute(0, 2, 1)

        # Decode
        recon = self.decoder(z_quantized)

        # Length alignment: ensure recon length == input length
        target_len = x.shape[-1]
        current_len = recon.shape[-1]
        if current_len > target_len:
            recon = recon[..., :target_len]
        elif current_len < target_len:
            recon = F.pad(recon, (0, target_len - current_len))

        return recon, indices, loss, loss_breakdown
