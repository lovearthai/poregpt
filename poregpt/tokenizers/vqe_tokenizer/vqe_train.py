import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
import csv  # âœ… æ–°å¢ï¼šç”¨äºå†™å…¥ CSV
import time  # ç¡®ä¿å·²å¯¼å…¥
# ç›¸å¯¹å¯¼å…¥æ ¸å¿ƒç»„ä»¶
from ..utils.dataset import NanoporeSignalDataset
from .vq_model import NanoporeVQModel
from typing import Dict, List
import collections
from ..utils.dwa import DynamicWeightAverager 

import argparse
# ========== è¯„ä¼°å‡½æ•°ï¼ˆä»…åœ¨ do_evaluate=True æ—¶è°ƒç”¨ï¼‰==========
import json
from pprint import pformat
from scipy.stats import entropy

# ====== æ‰“å°æ‰€æœ‰è®­ç»ƒå‚æ•° ======
def print_training_args(**kwargs):
    print("\n" + "="*60)
    print(" ğŸš€ Starting VQE Training with the following configuration:")
    print("="*60)
    # ä½¿ç”¨ pprint ç¾åŒ–è¾“å‡ºï¼ˆä¿ç•™ç±»å‹ä¿¡æ¯ï¼Œå¦‚ True/False/Noneï¼‰
    print(pformat(kwargs, width=100, sort_dicts=False))
    print("="*60 + "\n")


# ====== å®šä¹‰ä¸€ä¸ªä¿å­˜å‡½æ•°ï¼ˆæ”¾åœ¨ vq_train å†…éƒ¨ï¼Œä¾‹å¦‚åœ¨ model åˆå§‹åŒ–ä¹‹åï¼‰======
def save_full_checkpoint(
    path: str,
    model,
    optimizer,
    scheduler,
    epoch: int,
    spoch: int,
    global_step: int,
    cnn_type:int,
    rank: int
):
    if rank != 0:
        return

    checkpoint = {
        'epoch': epoch,
        'spoch': spoch,
        'global_step': global_step,
        'cnn_type':cnn_type,
        'model_state_dict': model.module.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'rng_state': torch.get_rng_state(),
        'cuda_rng_state': torch.cuda.get_rng_state() if torch.cuda.is_available() else None,
        'numpy_rng_state': np.random.get_state(),
    }

    if scheduler is not None:
        checkpoint['scheduler_state_dict'] = scheduler.state_dict()
    torch.save(checkpoint, path)
    print(f"âœ… Full checkpoint saved to {path}")


def log_and_save(
    epoch: int,
    step: int,
    total_epochs: int,
    total_steps: int,
    epoch_start_time: float,          # â† æ›¿æ¢ elapsed_time / remaining_time
    epoch_total_steps: int,           # â† å½“å‰ epoch çš„æ€»æ­¥æ•°ï¼ˆç”¨äºä¼°ç®—å‰©ä½™æ—¶é—´ï¼‰
    avg_recon_loss: float,
    avg_total_loss: float,
    avg_comit_loss: float,
    avg_diver_loss: float,
    avg_ortho_loss: float,
    codebook_usage: float,
    loss_csv_path: str,
    dynamic_recon_weight: float,
    dynamic_comit_weight: float,
    dynamic_ortho_weight: float,
    dynamic_diver_weight: float,
    lr: float,
):
    """
    æ‰“å°å½“å‰è®­ç»ƒçŠ¶æ€å¹¶ä¿å­˜åˆ°CSVæ–‡ä»¶ã€‚
    æ—¶é—´å­—ç¬¦ä¸²åœ¨å‡½æ•°å†…éƒ¨ç”Ÿæˆï¼Œæ ¼å¼ä¸º H:MM:SSï¼ˆè‹¥ >=1hï¼‰æˆ– MM:SSã€‚
    """
    import time

    # === ğŸ•’ åŠ¨æ€è®¡ç®—æ—¶é—´ ===
    current_time = time.time()
    elapsed_seconds = current_time - epoch_start_time
    steps_done = step % epoch_total_steps or epoch_total_steps  # é˜²æ­¢ step=0
    if steps_done == 0:
        steps_done = 1
    avg_time_per_step = elapsed_seconds / steps_done
    remaining_steps = epoch_total_steps - steps_done
    remaining_seconds = avg_time_per_step * max(0, remaining_steps)

    def format_hms(seconds: float) -> str:
        seconds = int(seconds)
        h = seconds // 3600
        m = (seconds % 3600) // 60
        s = seconds % 60
        if h > 0:
            return f"{h}:{m:02d}:{s:02d}"
        else:
            return f"{m:02d}:{s:02d}"

    elapsed_str = format_hms(elapsed_seconds)
    remaining_str = format_hms(remaining_seconds)

    # === ğŸ”¢ åŠ¨æ€å¯¹é½ ===
    epoch_width = len(str(total_epochs))
    step_width = len(str(total_steps))

    # === ğŸ–¨ï¸ æ‰“å°æ—¥å¿— ===
    print(
        f"[Epoch {epoch+1:>{epoch_width}}/{total_epochs} | "
        f"Step {step:>{step_width}}/{total_steps} | "
        f"{elapsed_str}<{remaining_str}] "
        f"Total: {avg_total_loss:>8.6f} | "
        f"Recon: {avg_recon_loss:>8.6f} | "
        f"Comit: {avg_comit_loss:>8.6f} | "
        f"Ortho: {avg_ortho_loss:>8.6f} | "
        f"Diver: {avg_diver_loss:>3.2f} | "
        f"Usage: {codebook_usage*100:>3.1f}% | "
        f"LR: {lr:>7.2e} |"
    )

    # === ğŸ’¾ å†™å…¥ CSV ===
    row_data = [
        epoch + 1,
        step,
        avg_recon_loss,
        avg_total_loss,
        avg_comit_loss,
        avg_diver_loss,
        avg_ortho_loss,
        codebook_usage * 100,  # ä¿å­˜ä¸ºç™¾åˆ†æ¯”æ›´ç›´è§‚ï¼ˆå¯é€‰ï¼‰
        dynamic_recon_weight,
        dynamic_comit_weight,
        dynamic_ortho_weight,
        dynamic_diver_weight,
        lr
    ]
    with open(loss_csv_path, 'a', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(row_data)

def vqe_train(
    npy_dir: str,
    output_model_path: str,
    batch_size: int = 16,
    lr: float = 1e-4,
    num_epochs: int = 10,
    codebook_size: int = 8192,
    chunk_size: int = 12000,
    num_workers: int = 8,
    update_loss_weight_every: int = 10,
    prefetch_factor: int = 128,
    val_ratio: int = 0.1,
    do_evaluate: bool = True,
    commitment_weight: float = 1.0,
    codebook_diversity_loss_weight: float = 1.0,
    orthogonal_reg_weight: float = 1.0,
    loss_log_interval: int = 10,
    loss_csv_path: str = "train_loss.csv",  # âœ… æ–°å¢å‚æ•°ï¼šloss æ—¥å¿— CSV è·¯å¾„
    use_wandb: bool = True,                 # æ˜¯å¦å¯ç”¨ wandb
    wandb_project: str = "nanopore_vq",     # wandb é¡¹ç›®å
    wandb_name: str = "default_wandb_runname",  # è¿è¡Œåç§°ï¼ˆå¯é€‰
    # ====== ğŸ“ˆ å­¦ä¹ ç‡è°ƒåº¦å™¨å‚æ•°ï¼ˆæ–°å¢ï¼‰======
    lr_scheduler_type: str = "cosine",          # 'cosine', 'linear', 'constant'
    warmup_steps: int = 500,                    # é¢„çƒ­æ­¥æ•°ï¼ˆå…¨å±€ stepï¼‰
    warmup_start_factor: float = 1e-6,          # warmup èµ·å§‹ lr = lr * start_factor
    warmup_end_factor: float = 1.0,             # warmup ç»“æŸ lr = lr * end_factor
    main_scheduler_end_factor: float = 1e-6,    # ä¸»è°ƒåº¦å™¨æœ€ç»ˆ lr = lr * end_factorï¼ˆä»… linear ç”¨ï¼‰
    save_checkpoint_every_spoch: int = 1000,    # æ¯å¤šå°‘ä¸ªupdate_loss_weight_everyè¿›è¡Œä¸€æ¬¡æ£€æŸ¥ç‚¹ä¿å­˜
    evaluate_every_spoch: int = 100,           # æ¯å¤šå°‘ä¸ªupdate_loss_weight_everyè¿›è¡Œä¸€æ¬¡evaluate
    checkpoint_path : str = None,
    cnn_type: int = 0,
    init_codebook_path: str = None             # ğŸ‘ˆ æ–°å¢ï¼šé¢„è®­ç»ƒç æœ¬è·¯å¾„
):
    # è°ƒç”¨ï¼šä¼ å…¥æ‰€æœ‰å‚æ•°
    print_training_args(
        npy_dir=npy_dir,
        output_model_path=output_model_path,
        batch_size=batch_size,
        lr=lr,
        num_epochs=num_epochs,
        codebook_size=codebook_size,
        chunk_size=chunk_size,
        num_workers=num_workers,
        update_loss_weight_every=update_loss_weight_every,
        prefetch_factor=prefetch_factor,
        val_ratio=val_ratio,
        do_evaluate=do_evaluate,
        commitment_weight=commitment_weight,
        codebook_diversity_loss_weight=codebook_diversity_loss_weight,
        orthogonal_reg_weight=orthogonal_reg_weight,
        loss_log_interval=loss_log_interval,
        loss_csv_path=loss_csv_path,
        use_wandb=use_wandb,
        wandb_project=wandb_project,
        wandb_name=wandb_name,
        lr_scheduler_type=lr_scheduler_type,
        warmup_steps=warmup_steps,
        warmup_start_factor=warmup_start_factor,
        warmup_end_factor=warmup_end_factor,
        main_scheduler_end_factor=main_scheduler_end_factor,
        save_checkpoint_every_spoch=save_checkpoint_every_spoch,
        evaluate_every_spoch=evaluate_every_spoch,
        checkpoint_path=checkpoint_path,
        init_codebook_path=init_codebook_path
    )


    """
    åˆ†å¸ƒå¼è®­ç»ƒ Nanopore VQ tokenizerã€‚
    ç°åœ¨ä¼šåˆ†åˆ«æ‰“å°ï¼šé‡å»ºæŸå¤±ã€commitment æŸå¤±ã€æ€»æŸå¤±ã€‚
    """
    import torch.distributed as dist
    from torch.nn.parallel import DistributedDataParallel as DDP
    from torch.utils.data.distributed import DistributedSampler

    if checkpoint_path and not os.path.isfile(checkpoint_path):
        print(f"Required checkpoint not found: {checkpoint_path}")
        checkpoint_path = None

    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    dist.init_process_group(backend="nccl")
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    local_device_id = rank % torch.cuda.device_count()
    torch.cuda.set_device(local_device_id)
    device = f"cuda:{local_device_id}"

    # ========== åˆå§‹åŒ– wandbï¼ˆä»… rank 0ï¼‰==========
    if rank == 0 and use_wandb:
       import wandb
       wandb.init(
           project=wandb_project,
           name=wandb_name,
           config={
               "batch_size": batch_size,
               "lr": lr,
               "num_epochs": num_epochs,
               "codebook_size": codebook_size,
               "chunk_size": chunk_size,
               "update_loss_weight_every": update_loss_weight_every,
               "commitment_weight": commitment_weight,
               "codebook_diversity_loss_weight": codebook_diversity_loss_weight,
               "orthogonal_reg_weight": orthogonal_reg_weight,
               "world_size": world_size,
           }
        )
    else:
        wandb = None  # é¿å…æœªå®šä¹‰


    if rank == 0:
        print(f"ğŸš€ Using {world_size} GPUs for training.")
        print(f"ğŸ“‚ Data directory: {npy_dir}")
        print(f"ğŸ’¾ Model will be saved to: {output_model_path}")
        print(f"âš™ï¸  Hyperparameters: "
              f"batch_size={batch_size}, lr={lr}, epochs={num_epochs}, "
              f"codebook_size={codebook_size}, chunk_size={chunk_size}, "
              f"do_evaluate={do_evaluate}, save_checkpoint_every_spoch={save_checkpoint_every_spoch}")

        # âœ… åˆå§‹åŒ– CSV æ–‡ä»¶ï¼ˆä»… rank 0ï¼‰
        with open(loss_csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            header = [
                'epoch', 'step',
                'recon_loss', 'total_loss', 'comit_loss', 'diver_loss', 'ortho_loss', 'codebook_usage',
                'wv_recon', 'wv_comit', 'wv_ortho', 'wv_diver',  # â† æ–°å¢
                'lr'
            ]
            writer.writerow(header)

    # ========== æ•°æ®åŠ è½½ ==========
    dataset = NanoporeSignalDataset(shards_dir=npy_dir)
    # ====== æ–°å¢ï¼šåªå–å‰ N ä¸ªæ ·æœ¬ï¼ˆæˆ–ä»»æ„å­é›†ï¼‰======
    #subset_size = int(1.0 * len(dataset))  # ä¾‹å¦‚ï¼šåªç”¨ 10% çš„æ•°æ®
    # æˆ–è€…æŒ‡å®šç»å¯¹æ•°é‡ï¼š
    # subset_size = 100_000
    # ç¡®ä¿ä¸è¶…é™
    #subset_size = min(subset_size, len(dataset))
    # å›ºå®šå­é›†é€‰æ‹©çš„éšæœºæ€§ï¼ˆä»…å½±å“ subset é€‰å–ï¼Œä¸å½±å“è®­ç»ƒä¸­çš„ shuffleï¼‰
    #torch.manual_seed(42)
    #indices = torch.randperm(len(dataset)).tolist()[:subset_size]
    #dataset = torch.utils.data.Subset(dataset, indices)
    # æ³¨æ„ï¼šè¿™ä¸ª seed åªæ§åˆ¶ subset é€‰å–ï¼Œä¸å½±å“ DataLoader å†…éƒ¨çš„ shuffle=True æˆ– DistributedSampler çš„æ‰“ä¹±è¡Œä¸ºã€‚


    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        sampler=sampler,
        num_workers=num_workers,
        prefetch_factor=prefetch_factor,
        pin_memory=True,
        drop_last=True
    )

    # ========== å¯é€‰ï¼šéªŒè¯é›†ï¼ˆä»…ç”¨äºè¯„ä¼°ï¼‰==========
    val_loader = None
    def evaluate_codebook_usage():
        if val_loader is None:  # â­ å®‰å…¨æ£€æŸ¥
            return 0.0, 0
        model.eval()
        used_codes = set()
        total_tokens = 0
        with torch.no_grad():
            for batch in val_loader:
                x = batch.to(device)
                _, indices, _, _ = model.module(x)
                indices = indices.cpu().numpy().flatten()
                used_codes.update(indices.tolist())
                total_tokens += indices.size
        usage_ratio = len(used_codes) / codebook_size
        model.train()
        return usage_ratio, total_tokens

    def evaluate_codebook_metrics():
        if val_loader is None:
            return 0.0, 0, 0.0, 0.0  # usage_ratio, total_tokens, top1_ratio, top10_ratio

        model.eval()
        used_codes = set()
        token_counts = np.zeros(codebook_size, dtype=np.int64)
        total_tokens = 0

        with torch.no_grad():
            for batch in val_loader:
                x = batch.to(device)
                _, indices, _, _ = model.module(x)
                indices = indices.cpu().numpy().flatten()
                used_codes.update(indices.tolist())
                total_tokens += indices.size
                # ç´¯åŠ é¢‘æ¬¡
                for idx in indices:
                    token_counts[idx] += 1

        usage_ratio = len(used_codes) / codebook_size

        if total_tokens == 0:
            top1_ratio, top10_ratio = 0.0, 0.0
        else:
            sorted_counts = np.sort(token_counts)[::-1]
            top1_ratio = float(sorted_counts[0] / total_tokens)
            top3_ratio = float(sorted_counts[3] / total_tokens)
            top5_ratio = float(sorted_counts[5] / total_tokens)
            top7_ratio = float(sorted_counts[7] / total_tokens)
            top9_ratio = float(sorted_counts[9] / total_tokens)
            top10_ratio = float(sorted_counts[:min(10, codebook_size)].sum() / total_tokens)

        model.train()
        return usage_ratio, total_tokens, top1_ratio, top3_ratio,top5_ratio,top7_ratio,top9_ratio,top10_ratio

    def evaluate_codebook_metrics():
        if val_loader is None:
            # è¿”å›åŸæœ‰ + entropy, max_entropyï¼ˆè®¾ä¸º0ï¼‰
            return 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0

        model.eval()
        used_codes = set()
        token_counts = np.zeros(codebook_size, dtype=np.int64)
        total_tokens = 0

        with torch.no_grad():
            for batch in val_loader:
                x = batch.to(device)
                _, indices, _, _ = model.module(x)
                indices = indices.cpu().numpy().flatten()
                used_codes.update(indices.tolist())
                total_tokens += indices.size
                for idx in indices:
                    token_counts[idx] += 1

        usage_ratio = len(used_codes) / codebook_size

        # åˆå§‹åŒ–æ¯”ç‡
        top1_ratio = top3_ratio = top5_ratio = top7_ratio = top9_ratio = top10_ratio = 0.0
        entropy = 0.0
        max_entropy = np.log2(codebook_size)  # ç†è®ºæœ€å¤§ç†µï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰

        if total_tokens > 0:
            sorted_counts = np.sort(token_counts)[::-1]
            
            # Top-k ratios
            top1_ratio = float(sorted_counts[0] / total_tokens)
            if len(sorted_counts) > 3:
                top3_ratio = float(sorted_counts[3] / total_tokens)
            if len(sorted_counts) > 5:
                top5_ratio = float(sorted_counts[5] / total_tokens)
            if len(sorted_counts) > 7:
                top7_ratio = float(sorted_counts[7] / total_tokens)
            if len(sorted_counts) > 9:
                top9_ratio = float(sorted_counts[9] / total_tokens)
            top10_ratio = float(sorted_counts[:min(10, codebook_size)].sum() / total_tokens)

            # === æ–°å¢ï¼šè®¡ç®—é¦™å†œç†µ ===
            # è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
            prob = token_counts / total_tokens  # shape: (codebook_size,)
            # åªä¿ç•™éé›¶æ¦‚ç‡ï¼ˆé¿å… log(0)ï¼‰
            nonzero_prob = prob[prob > 0]
            if nonzero_prob.size > 0:
                entropy = -np.sum(nonzero_prob * np.log2(nonzero_prob))
            else:
                entropy = 0.0
        else:
            entropy = 0.0

        model.train()
        
        # è¿”å›é¡ºåºï¼š
        # usage_ratio, total_tokens,
        # top1, top3, top5, top7, top9, top10,
        # entropy, max_entropy
        return (
            usage_ratio, total_tokens,
            top1_ratio, top3_ratio, top5_ratio, top7_ratio, top9_ratio, top10_ratio,
            entropy, max_entropy
        )


    if do_evaluate and rank == 0:  # â­ åªåœ¨ rank 0 åˆ›å»º val_loaderï¼ˆå…¶ä»– rank ä¸éœ€è¦ï¼‰
        actual_val_size = int(val_ratio *len(dataset))
        if actual_val_size < 1:
            actual_val_size = 1
        # ğŸ”’ å›ºå®šéªŒè¯é›†çš„éšæœºæ€§ï¼ˆå…³é”®ï¼ï¼‰
        np.random.seed(42)  # æˆ–ä»»ä½•ä½ å–œæ¬¢çš„æ•´æ•°
        indices = np.random.choice(len(dataset), size=actual_val_size, replace=False)
        val_subset = torch.utils.data.Subset(dataset, indices)  # â† å¤ç”¨ dataset
        val_loader = DataLoader(
            val_subset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=max(2, num_workers // 2),
            pin_memory=True
        )
    
    # ========== æ¨¡å‹ä¸ä¼˜åŒ–å™¨ ==========
    # 1. åŠ è½½é¢„è®­ç»ƒç æœ¬ï¼ˆå¦‚æœæä¾›ï¼‰
    init_codebook = None
    if init_codebook_path is not None:
        if rank == 0:
            # print(f"ğŸ“¥ Loading pretrained centroids from: {init_codebook_path}")
            print(f"ğŸ“¥ Loading initial codebook from: {init_codebook_path}")
            init_codebook = np.load(init_codebook_path)  # shape: [K, D]
            print(f"   Loaded codebook shape: {init_codebook.shape} ")

            # âœ… å®‰å…¨æ£€æŸ¥
            assert init_codebook.shape[0] == codebook_size, \
                f"Codebook size mismatch: expected {codebook_size}, got {init_codebook.shape[0]}"
            # å‡è®¾ä½ çš„ VQ å±‚è¾“å…¥ç»´åº¦æ˜¯å›ºå®šçš„ï¼ˆå¦‚ 64ï¼‰ï¼Œéœ€ç¡®è®¤
            # å¦‚æœä¸ç¡®å®šï¼Œå¯ä»æ¨¡å‹å†…éƒ¨è·å– expected_dim
        else:
            init_codebook = None

        # å¹¿æ’­åˆ°æ‰€æœ‰ rankï¼ˆç¡®ä¿ DDP ä¸€è‡´æ€§ï¼‰
        if rank == 0:
            codebook_tensor = torch.from_numpy(init_codebook).float().to(device)
            codebook_size_tensor = torch.tensor([codebook_tensor.shape[1]], device=device)
        else:
            codebook_tensor = torch.empty((codebook_size, 1), dtype=torch.float32, device=device)
            codebook_size_tensor = torch.empty(1, dtype=torch.long, device=device)

        dist.broadcast(codebook_size_tensor, src=0)
        expected_dim = codebook_size_tensor.item()

        if rank != 0:
            codebook_tensor = torch.empty((codebook_size, expected_dim), dtype=torch.float32, device=device)
        dist.broadcast(codebook_tensor, src=0)
        init_codebook = codebook_tensor  # now on all ranks
    else:
        init_codebook = None

    # 2. åˆ›å»ºæ¨¡å‹
    model = NanoporeVQModel(
        codebook_size=codebook_size,
        commitment_weight=commitment_weight,
        codebook_diversity_loss_weight=codebook_diversity_loss_weight,
        orthogonal_reg_weight=orthogonal_reg_weight,
        cnn_type=cnn_type
    ).to(device)

    # 3. å¦‚æœæä¾›äº† init_codebookï¼Œæ›¿æ¢æ¨¡å‹çš„ codebook
    if init_codebook is not None:
        # å‡è®¾ä½ çš„ VQ å±‚æ˜¯ model.vq æˆ– model.quantizer ç­‰
        # ä½ éœ€è¦çŸ¥é“ codebook åœ¨æ¨¡å‹ä¸­çš„ç¡®åˆ‡è·¯å¾„ï¼
        # å¸¸è§æƒ…å†µï¼š
        # - model.module.vq.codebook
        # - model.module.quantizer.codebook
        # - model.module._vq.codebook

        # ğŸ” å…ˆç¡®è®¤ä½ çš„ VQ å±‚å«ä»€ä¹ˆï¼
        # ä¸´æ—¶æ‰“å°æ¨¡å‹ç»“æ„ï¼ˆä»… rank 0ï¼‰ï¼š
        if rank == 0:
            print("ğŸ” Model VQ attribute names (look for 'codebook'):")
            for name, param in model.named_parameters():
                if 'codebook' in name:
                    print(f"  â†’ Found: {name} with shape {param.shape}")

        # âœ… å…³é”®ï¼šæ›¿æ¢ codebookï¼ˆå‡è®¾ä½ çš„ VQ å±‚å« `vq`ï¼‰
        # è¯·æ ¹æ®ä½ çš„å®é™…æ¨¡å‹ç»“æ„è°ƒæ•´ä¸‹é¢çš„å±æ€§åï¼
        try:
            #model.vq.codebook.data.copy_(init_codebook)
            # âœ… æ¨èå†™æ³•
            model.vq.codebook = init_codebook
            # === æ ¡éªŒ ===
            loaded_codebook = model.vq.codebook
            if rank == 0:
                assert torch.allclose(loaded_codebook, init_codebook.to(loaded_codebook.device)), "Codebook initialization failed!"
                print("âœ… Codebook successfully initialized from Faiss centroids.")
        except AttributeError as e:
            if rank == 0:
                print(f"âŒ Failed to set codebook: {e}")
                print("ğŸ’¡ Hint: Check the actual attribute name of your VQ layer (e.g., 'quantizer', '_vq', etc.)")
            raise


    #model = DDP(model, device_ids=[local_device_id],find_unused_parameters=True )
    # 2. å…ˆ wrap æˆ DDPï¼ˆå…³é”®ï¼ï¼‰ä¸€å®šè¦åœ¨åŠ è½½æ£€æŸ¥ç‚¹ä¹‹å‰åšDDP
    model = DDP(model, device_ids=[local_device_id])

    # 3. å†åˆ›å»º optimizerï¼ˆåŸºäº DDP æ¨¡å‹çš„å‚æ•°ï¼‰
    
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    



    # åªå¯¹å‰ä¸‰ä¸ªåšåŠ¨æ€åŠ æƒ
    if rank == 0:
        # è‡ªå®šä¹‰åˆå§‹æƒé‡ï¼ˆä¾‹å¦‚æ›´é‡è§† recon_lossï¼‰
        init_w = {
            "recon_loss": 0.25,
            "comit_loss": 0.25,
            "ortho_loss": 0.25,
            "diver_loss": 0.25
        }
        # å®šä¹‰æƒé‡è¾¹ç•Œ
        bounds = {
            "recon_loss": (0.01, 0.99),
            "comit_loss": (0.01, 0.99),
            "ortho_loss": (0.01, 0.99),
            "diver_loss": (0.01, 0.99),
        }

        dwa = DynamicWeightAverager(
            loss_names=["recon_loss", "comit_loss", "ortho_loss", "diver_loss","total_loss"],
            weighted_loss_names=["recon_loss", "comit_loss", "ortho_loss","diver_loss"],
            initial_weights=init_w,
            weight_bounds=bounds,
            warmup_steps=10,          # å‰ 200 æ­¥å›ºå®šç”¨ init_w
            temperature=1.0,
            window_size=50,
            slow_window=45,
            fast_window=5,
            device=device
        )

    # ========== å­¦ä¹ ç‡è°ƒåº¦å™¨ ==========
    if rank == 0:
        total_training_steps = len(dataloader) * num_epochs
        print(f"ğŸ”¢ Total training steps: {total_training_steps}, Warmup steps: {warmup_steps}")


    # ========== å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå®Œå…¨å‚æ•°åŒ–ï¼‰==========
    scheduler = None
    total_training_steps = len(dataloader) * num_epochs

    if rank == 0:
        print(f"ğŸ”¢ Total training steps: {total_training_steps}")
        if lr_scheduler_type != "constant":
            print(f"ğŸ“ˆ Using LR scheduler: {lr_scheduler_type}, "
                  f"warmup_steps={warmup_steps}, "
                  f"warmup: {warmup_start_factor}â†’{warmup_end_factor}, "
                  f"main_end_factor={main_scheduler_end_factor}")

    if lr_scheduler_type != "constant":
        from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR

        # Warmup é˜¶æ®µï¼šä» warmup_start_factor * lr åˆ° warmup_end_factor * lr
        warmup_scheduler = LinearLR(
            optimizer,
            start_factor=warmup_start_factor,
            end_factor=warmup_end_factor,
            total_iters=warmup_steps
        )

        main_steps = max(1, total_training_steps - warmup_steps)

        if lr_scheduler_type == "cosine":
            # Cosine é€€ç«ï¼šä»å½“å‰ lrï¼ˆå³ warmup_end_factor * lrï¼‰é€€ç«åˆ° 0
            main_scheduler = CosineAnnealingLR(optimizer, T_max=main_steps)
        elif lr_scheduler_type == "linear":
            # Linear è¡°å‡ï¼šä»å½“å‰ lr è¡°å‡åˆ° main_scheduler_end_factor * åŸå§‹ lr
            # æ³¨æ„ï¼šLinearLR çš„ end_factor æ˜¯ç›¸å¯¹äº warmup ç»“æŸæ—¶çš„ lr
            # æ‰€ä»¥ç›®æ ‡ lr = (main_scheduler_end_factor * lr) / (warmup_end_factor * lr) = main_scheduler_end_factor / warmup_end_factor
            relative_end_factor = main_scheduler_end_factor / warmup_end_factor if warmup_end_factor > 0 else 0.0
            relative_end_factor = max(1e-8, min(1.0, relative_end_factor))  # å®‰å…¨ clamp
            main_scheduler = LinearLR(
                optimizer,
                start_factor=1.0,
                end_factor=relative_end_factor,
                total_iters=main_steps
            )
        else:
            raise ValueError(f"Unsupported lr_scheduler_type: {lr_scheduler_type}")

        scheduler = SequentialLR(
            optimizer,
            schedulers=[warmup_scheduler, main_scheduler],
            milestones=[warmup_steps]
        )
    # else: scheduler remains None â†’ constant LR
     # ğŸ‘‡ ğŸ‘‡ ğŸ‘‡ å°±åœ¨è¿™é‡Œæ’å…¥åŠ è½½ checkpoint çš„é€»è¾‘ ğŸ‘‡ ğŸ‘‡ ğŸ‘‡
    start_epoch = 0
    start_spoch = 0
    start_global_step = 0
    loaded_dwa_state = None

    start_epoch = 0
    start_spoch = 0
    start_global_step = 0
    loaded_dwa_state = None

    # ===== æ£€æŸ¥å¹¶åŠ è½½ checkpoint =====
    if checkpoint_path is not None and isinstance(checkpoint_path, str) and checkpoint_path.strip():
        # ä»… rank 0 æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆå¯é€‰ï¼šä¹Ÿå¯è®©æ‰€æœ‰ rank æ£€æŸ¥ï¼‰
        if rank == 0:
            if not os.path.isfile(checkpoint_path):
                print(f"âš ï¸ Warning: checkpoint_path '{checkpoint_path}' does not exist. Training from scratch.")
                checkpoint_path = None  # é‡ç½®ä¸º Noneï¼Œé¿å…åç»­åŠ è½½
            else:
                print(f"ğŸ“¥ Loading checkpoint from: {checkpoint_path}")
        
        # åŒæ­¥ï¼šç¡®ä¿æ‰€æœ‰ rank çŸ¥é“æ˜¯å¦è¦åŠ è½½ï¼ˆé˜²æ­¢ rank != 0 å¡ä½ï¼‰
        # æ–¹æ³•ï¼šé€šè¿‡ä¸€ä¸ªå…±äº«çš„ flag å¼ é‡
        load_flag = torch.tensor([1 if checkpoint_path is not None else 0], dtype=torch.int32, device=device)
        if rank == 0:
            load_flag[0] = int(os.path.isfile(checkpoint_path)) if checkpoint_path else 0
        dist.broadcast(load_flag, src=0)
        
        if load_flag.item() == 1:
            # æ‰€æœ‰ rank åŠ è½½ï¼ˆmap_location è‡ªåŠ¨å¤„ç†è®¾å¤‡ï¼‰
            ckpt = torch.load(checkpoint_path, map_location=device,weights_only=False)

            model.load_state_dict(ckpt['model_state_dict'])
            optimizer.load_state_dict(ckpt['optimizer_state_dict'])
            if scheduler is not None and 'scheduler_state_dict' in ckpt:
                scheduler.load_state_dict(ckpt['scheduler_state_dict'])

            # æ¢å¤éšæœºçŠ¶æ€ï¼ˆä»… rank 0ï¼‰
            if rank == 0:
                # Step 2: Safely restore PyTorch RNG state
                raw_rng = ckpt['rng_state']
                
                # Convert to bytes if needed
                if isinstance(raw_rng, torch.Tensor):
                    # Tensor case: ensure uint8 and contiguous
                    rng_bytes = raw_rng.cpu().numpy().tobytes()
                elif isinstance(raw_rng, np.ndarray):
                    rng_bytes = raw_rng.tobytes()
                elif isinstance(raw_rng, bytes):
                    rng_bytes = raw_rng
                else:
                    raise TypeError(f"Unexpected type for rng_state: {type(raw_rng)}")
                
                # Reconstruct as proper ByteTensor
                rng_state = torch.frombuffer(rng_bytes, dtype=torch.uint8).contiguous()
                torch.set_rng_state(rng_state)
                
                # Optional: Restore CUDA RNG if available
                if 'cuda_rng_state' in ckpt and ckpt['cuda_rng_state'] is not None:
                    raw_cuda_rng = ckpt['cuda_rng_state']
                    if isinstance(raw_cuda_rng, torch.Tensor):
                        cuda_bytes = raw_cuda_rng.cpu().numpy().tobytes()
                    elif isinstance(raw_cuda_rng, np.ndarray):
                        cuda_bytes = raw_cuda_rng.tobytes()
                    elif isinstance(raw_cuda_rng, bytes):
                        cuda_bytes = raw_cuda_rng
                    else:
                        raise TypeError(f"Unexpected type for cuda_rng_state: {type(raw_cuda_rng)}")
                    cuda_rng_state = torch.frombuffer(cuda_bytes, dtype=torch.uint8).contiguous()
                    torch.cuda.set_rng_state(cuda_rng_state)
                
                # Optional: Restore NumPy RNG
                if 'numpy_rng_state' in ckpt:
                    np.random.set_state(ckpt['numpy_rng_state'])
                    start_epoch = ckpt.get('epoch', -1) + 1
                    start_spoch = ckpt.get('spoch', -1) + 1
                    start_global_step = ckpt.get('global_step', 0)

            if rank == 0:
                print(f"âœ… Resuming from epoch {start_epoch}, spoch {start_spoch}")
        else:
            # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä»å¤´è®­ç»ƒ
            if rank == 0:
                print("â­ï¸  No valid checkpoint found. Starting training from scratch.")
    else:
        if rank == 0 and checkpoint_path is not None:
            print("âš ï¸  Invalid checkpoint_path (empty or not a string). Ignoring.")

   
    # ========== è®­ç»ƒå¾ªç¯ ==========
    model.train()
    codebook_usage = 0.0
    codebook_top1_ratio = 0.0
    codebook_top3_ratio = 0.0
    codebook_top5_ratio = 0.0
    codebook_top7_ratio = 0.0
    codebook_top9_ratio = 0.0
    codebook_top10_ratio = 0.0
    codebook_entropy = 0.0
    codebook_max_entropy = 0.0
    total_steps = len(dataloader)*num_epochs
    epoch_total_steps = len(dataloader)  # å½“å‰ epoch çš„æœ¬åœ° step æ•°ï¼ˆæ¯ä¸ª rank ç›¸åŒï¼‰
    # ğŸ‘‡ æ–°å¢ï¼šç¼“å­˜æƒé‡ï¼ˆåˆå§‹å€¼å¯è®¾ä¸º 1.0ï¼‰
    cached_wvalue = torch.tensor([0.25, 0.25, 0.25,0.25], device=device)  # [recon, comit, ortho]
    # åœ¨ for epoch in range(num_epochs): ä¹‹å‰
    loss_buffer = {
        "recon": [],
        "comit": [],
        "ortho": [],
        "diver": []
    }
    # æ¯10ä¸ªstepå°±æ˜¯ä¸€ä¸ªspoch
    # åœ¨ resume é€»è¾‘ä¹‹åï¼Œåˆå§‹åŒ– global_step
    global_step = start_global_step
    spoch = start_spoch
    total_spochs = int(total_steps/update_loss_weight_every)
    for epoch in range(start_epoch, num_epochs):
        epoch_start_time = time.time()  # â† æ–°å¢ï¼šè®°å½• epoch å¼€å§‹æ—¶é—´
        sampler.set_epoch(epoch)
        num_batches = torch.tensor(len(dataloader), device=device)
        for step, batch in enumerate(dataloader):
            global_step += 1  # ğŸ‘ˆ å…³é”®ï¼šæ¯æ­¥ +1
            x = batch.to(device)
            # break_loss æ˜¯å¦å·²åŒ…å« commitment_weightï¼Ÿ
            # åœ¨ vector_quantize_pytorch ä¸­ï¼Œè¿”å›çš„ break_loss å·²ç»æ˜¯ä¹˜è¿‡ commitment_weight çš„ï¼ˆé»˜è®¤ 0.25ï¼‰
            # å› ä¸º VectorQuantize è¿”å›çš„ break_loss æ˜¯ï¼š
            # break_loss = (z_e - e_k.detach()).pow(2).mean() * self.commitment_weight
            # å®ƒæ˜¯ä¸€ä¸ª requires_grad=False çš„ scalar tensorï¼Œä½äºä¸è¾“å…¥ç›¸åŒçš„è®¾å¤‡ä¸Šï¼ˆGPUï¼‰ã€‚
            # æ‰€ä»¥ break_loss æœ¬èº«å°±æ˜¯ GPU tensorï¼Œä¸éœ€è¦ .item()ã€‚
            recon, indices,break_loss, loss_breakdown = model(x)
            # å¦‚æœä½ æƒ³å¼±åŒ–é‡å»ºã€å¼ºè°ƒç¦»æ•£è¡¨ç¤ºè´¨é‡ï¼Œå¯ä»¥åŠ ä¸€ä¸ªè¶…å‚æ•°ï¼š
            # recon_weight = 0.01  # << é™ä½é‡å»ºæƒé‡
            # loss = recon_weight * F.mse_loss(recon, x) + break_loss
            # è¿™æ ·æ¨¡å‹ä¼šæ›´å…³æ³¨â€œç¼–ç å™¨è´´ç´§ç æœ¬â€å’Œâ€œç æœ¬åˆ†æ•£â€ï¼Œè€Œä¸æ˜¯åƒç´ çº§è¿˜åŸä¿¡å·â€”â€”éå¸¸é€‚åˆåš tokenizerã€‚
            recon_loss = F.mse_loss(recon, x)
            comit_loss = loss_breakdown.commitment
            diver_loss = loss_breakdown.codebook_diversity
            ortho_loss = loss_breakdown.orthogonal_reg
            #print("comit_loss grad:", comit_loss.requires_grad) # True
            #total_loss = (recon_loss + 
            #    comit_loss * (commitment_weight+epoch) + 
            #    ortho_loss * orthogonal_reg_weight + 
            #    diver_loss * codebook_diversity_loss_weight)


            total_loss = (recon_loss + 
                comit_loss * (commitment_weight) )
            
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()
            # ğŸ‘‡ æ›´æ–°å­¦ä¹ ç‡ï¼ˆæ¯ä¸ª stepï¼‰
            if scheduler is not None:
                scheduler.step()
            # ğŸ‘‡ åªç¼“å­˜æ ‡é‡å€¼ï¼ˆæ— æ¢¯åº¦ï¼‰
            loss_buffer["recon"].append(recon_loss.item())
            loss_buffer["comit"].append(comit_loss.item())
            loss_buffer["ortho"].append(ortho_loss.item())
            loss_buffer["diver"].append(diver_loss.item())
            # ====== ğŸ” åŠ¨æ€æƒé‡æ›´æ–°é€»è¾‘ï¼ˆæ¯éš” update_every æ­¥ï¼‰ ======
            wv_recon, wv_comit, wv_ortho,wv_diver = cached_wvalue.tolist()
            should_update_weights = (step + 1) % update_loss_weight_every == 0 or  (step == len(dataloader) - 1)
            if should_update_weights:
                spoch += 1
                # è®¡ç®—å½“å‰çª—å£å¹³å‡ï¼ˆé˜²æ­¢ç©ºï¼‰
                def safe_mean(lst):
                    return sum(lst) / len(lst) if lst else 0.0
                local_avg_losses = torch.tensor([
                    safe_mean(loss_buffer["recon"]),
                    safe_mean(loss_buffer["comit"]),
                    safe_mean(loss_buffer["ortho"]),
                    safe_mean(loss_buffer["diver"])
                ], device=device)
                # ğŸ‘‡ å…¨å±€åŒæ­¥ï¼šæ±‚æ‰€æœ‰ rank çš„å¹³å‡
                # â† æ‰€æœ‰ rank åœ¨è¿™é‡ŒåŒæ­¥ï¼Œloss å·²å¹³å‡ æœ¬èº«å°±èµ·åˆ°äº† éšå¼çš„ barrier ä½œç”¨ï¼Œæ— éœ€å†æ‰‹åŠ¨åŠ  dist.barri
                dist.all_reduce(local_avg_losses, op=dist.ReduceOp.AVG)
                global_avg_recon, global_avg_comit, global_avg_ortho, global_avg_diver = local_avg_losses.tolist()
                global_avg_total = (
                            global_avg_recon +
                            global_avg_comit * commitment_weight +
                            global_avg_ortho * orthogonal_reg_weight +
                            global_avg_diver * codebook_diversity_loss_weight )

                if rank == 0:
                    current_losses = {
                        "recon_loss": global_avg_recon,
                        "comit_loss": global_avg_comit,
                        "ortho_loss": global_avg_ortho,
                        "diver_loss": global_avg_diver,
                        "total_loss": global_avg_total
                    }
                    wvalue = dwa.update_and_get_weights(current_losses)
                    wvalue_tensor = torch.tensor([
                        wvalue["recon_loss"],
                        wvalue["comit_loss"],
                        wvalue["ortho_loss"],
                        wvalue["diver_loss"],
                    ], device=device)
                else:
                    wvalue_tensor = torch.empty(4, device=device)
                # å¹¿æ’­æ–°æƒé‡
                dist.broadcast(wvalue_tensor, src=0) # â† æ‰€æœ‰ rank åœ¨è¿™é‡ŒåŒæ­¥ï¼Œæ”¶åˆ°å¹¿æ’­çš„æƒé‡  æœ¬èº«å°±èµ·åˆ°äº† éšå¼çš„ barrier ä½œç”¨ï¼Œæ— éœ€å†æ‰‹åŠ¨åŠ  dist.barrier()ã€‚
                cached_wvalue = wvalue_tensor  # æ›´æ–°ç¼“å­˜
                # ğŸ” æ¸…ç©º bufferï¼Œä¸ºä¸‹ä¸€ä¸ªçª—å£å‡†å¤‡
                loss_buffer = {k: [] for k in loss_buffer}
                    

                if rank == 0:
                    current_lr = optimizer.param_groups[0]['lr']
                    # è·å–æœ€æ–° fast lossï¼ˆå¯ç”¨äºæ—¥å¿—ã€è°ƒè¯•ã€ç›‘æ§ï¼‰
                    log_and_save(
                        epoch=epoch,
                        step=global_step,
                        total_epochs=num_epochs,
                        total_steps=total_steps,
                        epoch_start_time=epoch_start_time,      # âœ… ä¼ å…¥æ—¶é—´æˆ³
                        epoch_total_steps=len(dataloader),      # âœ… ç”¨äºä¼°ç®—å‰©ä½™æ—¶é—´
                        avg_recon_loss=global_avg_recon,
                        avg_total_loss=global_avg_total,
                        avg_comit_loss=global_avg_comit,
                        avg_diver_loss=global_avg_diver,
                        avg_ortho_loss=global_avg_ortho,
                        codebook_usage=codebook_usage,
                        loss_csv_path=loss_csv_path,
                        dynamic_recon_weight=wv_recon,
                        dynamic_comit_weight=wv_comit,
                        dynamic_ortho_weight=wv_ortho,
                        dynamic_diver_weight=wv_diver,
                        lr=current_lr
                    )
                    # === ğŸ“Š wandb æ—¥å¿— ===
                    log_dict = {
                        "train/recon_loss": global_avg_recon,
                        "train/comit_loss": global_avg_comit,
                        "train/ortho_loss": global_avg_ortho,
                        "train/diver_loss": global_avg_diver,
                        "train/total_loss": global_avg_total,
                        "codebook/usage": codebook_usage,
                        "codebook/top1_ratio": codebook_top1_ratio,
                        "codebook/top3_ratio": codebook_top3_ratio,
                        "codebook/top5_ratio": codebook_top5_ratio,
                        "codebook/top7_ratio": codebook_top7_ratio,
                        "codebook/top9_ratio": codebook_top9_ratio,
                        "codebook/top10_ratio": codebook_top10_ratio,
                        "codebook/entropy": codebook_entropy,
                        "codebook/max_entropy": codebook_max_entropy,
                        "weights/recon": wv_recon,
                        "weights/comit": wv_comit,
                        "weights/ortho": wv_ortho,
                        "weights/diver": wv_diver,
                        "weights/commitment_weight": commitment_weight,
                        "epoch": epoch + 1,
                        "learning_rate": current_lr,  # å¦‚æœä½¿ç”¨ schedulerï¼Œå¯åŠ¨æ€è·å–
                    }
                    if use_wandb:
                        wandb.log(log_dict, step=global_step)

                if rank == 0 and (spoch + 1)% evaluate_every_spoch == 0 and spoch < total_spochs:
                    codebook_usage, total_tokens,codebook_top1_ratio,codebook_top3_ratio, codebook_top5_ratio, codebook_top7_ratio, codebook_top9_ratio,codebook_top10_ratio,codebook_entropy, codebook_max_entropy = evaluate_codebook_metrics()
                    print(
                        f"Spoch {spoch+1} - "
                        f"Codebook Usage: {codebook_usage:.2%} "
                        )
                if rank == 0 and (spoch + 1)% save_checkpoint_every_spoch == 0:
                    # âœ… æ£€æŸ¥ç‚¹ä¿å­˜é€»è¾‘ï¼ˆä»… rank 0ï¼‰
                    checkpoint_path = f"{output_model_path}.spoch{spoch+1}.pth"
                    save_full_checkpoint(
                        path=checkpoint_path,
                        model=model,
                        optimizer=optimizer,
                        scheduler=scheduler,
                        epoch=epoch,
                        spoch=spoch,
                        global_step=global_step,
                        cnn_type=cnn_type,
                        rank=rank
                    )
                    print(f"âœ… Checkpoint saved to {checkpoint_path}")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆä»… rank 0ï¼‰
    if rank == 0:
        save_full_checkpoint(
            path=output_model_path,
            model=model,
            optimizer=optimizer,
            scheduler=scheduler,
            epoch=num_epochs - 1,
            spoch=spoch,
            global_step=global_step,
            rank=rank
        )
        print(f"âœ… Final model saved to {output_model_path}")
        if use_wandb:
            wandb.finish()  # âœ… æ­£ç¡®å…³é—­
    dist.barrier()
    dist.destroy_process_group()

# pyproject.toml çš„ project.scripts è¦æ±‚ä½ æä¾›ä¸€ä¸ªå¯è¢« setuptools ç›´æ¥è°ƒç”¨çš„å‡½æ•°ï¼ˆæ— å‚ï¼‰ã€‚å› æ­¤ï¼Œä½ éœ€è¦ç¨ä½œé‡æ„ã€‚
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--npy_dir", type=str, required=True)
    parser.add_argument("--output_model_path", type=str, default="demo_nanopore_vq_tokenizer.pth")
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--lr", type=float, default=3e-4)
    parser.add_argument("--num_epochs", type=int, default=10)
    parser.add_argument("--codebook_size", type=int, default=8192)
    parser.add_argument("--chunk_size", type=int, default=12000)
    parser.add_argument("--num_workers", type=int, default=8)
    parser.add_argument("--val_ratio", type=float, default=0.1)
    parser.add_argument("--commitment_weight", type=float, default=1.0)
    parser.add_argument("--codebook_diversity_loss_weight", type=float, default=1.0)
    parser.add_argument("--orthogonal_reg_weight", type=float, default=1.0)
    parser.add_argument("--loss_csv_path", type=str, default="train_loss.csv")
    parser.add_argument("--save_checkpoint_every_spoch", type=int, default=10)
    parser.add_argument("--loss_log_interval", type=int, default=10)
    parser.add_argument("--do_evaluate", action="store_true", help="Enable codebook usage evaluation")
    parser.add_argument("--checkpoint_path", type=str, default="checkpiint_nanopore_vq_tokenizer.pth")
    parser.add_argument("--cnn_type", type=int, default=0)
    parser.add_argument("--init_codebook_path", type=str, default="")
    args = parser.parse_args()

    vqe_train(
        npy_dir=args.npy_dir,
        output_model_path=args.output_model_path,
        batch_size=args.batch_size,
        lr=args.lr,
        num_epochs=args.num_epochs,
        codebook_size=args.codebook_size,
        chunk_size=args.chunk_size,
        num_workers=args.num_workers,
        val_ratio=args.val_ratio,
        do_evaluate=args.do_evaluate,
        commitment_weight=args.commitment_weight,
        codebook_diversity_loss_weight=args.codebook_diversity_loss_weight,
        orthogonal_reg_weight=args.orthogonal_reg_weight,
        loss_csv_path=args.loss_csv_path,
        save_checkpoint_every_spoch=args.save_checkpoint_every_spoch,
        loss_log_interval=args.loss_log_interval,
        checkpoint_path=args.checkpoint_path,
        cnn_type=args.cnn_type,
        init_codebook_path=args.init_codebook_path
    )

# ä¿ç•™è¿™ä¸ªç”¨äºç›´æ¥è¿è¡Œè„šæœ¬ï¼ˆå…¼å®¹æ€§ï¼‰
if __name__ == "__main__":
    main()

