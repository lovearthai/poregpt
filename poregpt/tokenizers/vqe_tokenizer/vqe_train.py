# vqe_train_accelerate.py
# Nanopore Signal Tokenizer Training Script with VQ-VAE
# Industrial-grade training pipeline for nanopore raw signal tokenization using Vector Quantization.
# Supports distributed training (DDP), dynamic logging, checkpointing, and independent evaluation dataset.
# This version uses Hugging Face Accelerate for simplified multi-GPU/mixed precision training.

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
import csv
import time
import json
from pprint import pformat
from scipy.stats import entropy
import argparse
from typing import Dict, List, Optional
import yaml
# Import Accelerate components
from accelerate import Accelerator, DistributedType
# Relative imports from the same package
# Note: The relative imports might need adjustment depending on your package structure.
# If this script is run directly, you might need to add the parent directory to sys.path.
try:
    from .dataset import NanoporeSignalDataset
    from .vq_model import NanoporeVQModel
except ImportError:
    # Fallback for direct execution
    from dataset import NanoporeSignalDataset
    from vq_model import NanoporeVQModel


# =============================================================================
# Utility Functions
# =============================================================================
def log_and_save(
    epoch: int,
    global_step: int,
    total_epochs: int,
    total_global_steps: int,
    train_start_time: float,
    epoch_total_steps: int,
    avg_recon_loss: float,
    avg_total_loss: float,
    avg_comit_loss: float,
    avg_diver_loss: float,
    avg_ortho_loss: float,
    codebook_usage: float,
    loss_csv_path: str,
    lr: float,
    accelerator: Accelerator
):
    """
    Log training metrics to console and append to CSV for offline analysis.
    Time estimation is based on current epoch progress.
    Only the main process writes to console and CSV.
    """
    import time

    current_time = time.time()
    elapsed_seconds = current_time - train_start_time
    steps_done = global_step % total_global_steps or 1
    avg_time_per_step = elapsed_seconds / steps_done
    remaining_seconds = avg_time_per_step * max(0, total_global_steps - steps_done)

    def format_hms(seconds: float) -> str:
        seconds = int(seconds)
        h = seconds // 3600
        m = (seconds % 3600) // 60
        s = seconds % 60
        return f"{h}:{m:02d}:{s:02d}" if h > 0 else f"{m:02d}:{s:02d}"

    elapsed_str = format_hms(elapsed_seconds)
    remaining_str = format_hms(remaining_seconds)

    epoch_width = len(str(total_epochs))
    step_width = len(str(total_global_steps))

    # Only main process prints and writes to CSV
    if accelerator.is_main_process:
        print(
            f"[Epoch {epoch+1:>{epoch_width}}/{total_epochs} | "
            f"Step {global_step:>{step_width}}/{total_global_steps} | "
            f"{elapsed_str}<{remaining_str}] "
            f"Total: {avg_total_loss:>8.6f} | "
            f"Recon: {avg_recon_loss:>8.6f} | "
            f"Comit: {avg_comit_loss:>8.6f} | "
            f"Ortho: {avg_ortho_loss:>8.6f} | "
            f"Diver: {avg_diver_loss:>3.2f} | "
            f"Usage: {codebook_usage*100:>3.1f}% | "
            f"LR: {lr:>7.2e} |"
        )


def print_training_args(**kwargs):
    """
    Pretty-print all training hyperparameters at startup for reproducibility and debugging.
    """
    print("\n" + "="*60)
    print(" üöÄ Starting VQE Training with Accelerate. Configuration:")
    print("="*60)
    print(pformat(kwargs, width=100, sort_dicts=False))
    print("="*60 + "\n")


def save_full_checkpoint(
    path: str,
    model,
    optimizer,
    scheduler,
    epoch: int,
    spoch: int,
    global_step: int,
    cnn_type: int,
    accelerator: Accelerator
):
    """
    Save a full training checkpoint (model, optimizer, RNG states) for resuming.
    Uses Accelerate's save_state method which handles DDP/DDP sharding automatically.
    Only the main process saves the checkpoint.
    """
    # Accelerate's save_state handles distributed saving internally
    accelerator.save_state(path)
    if accelerator.is_main_process:
        print(f"‚úÖ Full checkpoint saved to {path}")
        # Optionally save additional metadata separately
        metadata = {
            'epoch': epoch,
            'spoch': spoch,
            'global_step': global_step,
            'cnn_type': cnn_type,
        }
        meta_path = path.replace('.pth', '_metadata.json')
        with open(meta_path, 'w') as f:
            json.dump(metadata, f)


def load_checkpoint_metadata(path: str):
    """
    Load metadata associated with a checkpoint.
    """
    meta_path = path.replace('.pth', '_metadata.json')
    if os.path.exists(meta_path):
        with open(meta_path, 'r') as f:
            return json.load(f)
    return None




# =============================================================================
# Main Training Function
# =============================================================================

def vqe_train(
    train_npy_dir: str,
    evaluation_npy_dir: Optional[str] = None,
    output_model_path: str = "nanopore_vq_tokenizer.pth",
    batch_size: int = 16, # Note: This now refers to the device_micro_batch_size
    lr: float = 1e-4,
    num_epochs: int = 10,
    codebook_size: int = 8192,
    chunk_size: int = 12000,
    num_workers: int = 8,
    update_loss_weight_every: int = 10,
    prefetch_factor: int = 128,
    val_ratio: float = 0.001,
    do_evaluate: bool = True,
    commitment_weight: float = 1.0,
    codebook_diversity_loss_weight: float = 1.0,
    orthogonal_reg_weight: float = 1.0,
    loss_log_interval: int = 10,
    loss_csv_path: str = "train_loss.csv",
    use_wandb: bool = True,
    wandb_project: str = "nanopore_vq",
    wandb_name: str = "default_run",
    lr_scheduler_type: str = "cosine",
    warmup_steps: int = 500,
    warmup_start_factor: float = 1e-6,
    warmup_end_factor: float = 1.0,
    main_scheduler_end_factor: float = 1e-6,
    save_checkpoint_every_spoch: int = 500,
    evaluate_every_spoch: int = 10,
    checkpoint_path: Optional[str] = None,
    cnn_type: int = 0,
    init_codebook_path: Optional[str] = None,
    cnn_checkpoint_path: Optional[str] = None,
    freeze_cnn: int = 0,
    learnable_codebook: bool = False,
    global_batch_size: int = 256,
    device_micro_batch_size: int = 16, # Renamed for clarity, was 'batch_size' before
    mixed_precision: str = "bf16", # Options: "no", "fp16", "bf16", "fp8"
    gradient_clipping: float = 1.0, # Set to None to disable
    cpu: bool = False, # Set to True to force CPU training
    dataset_logic_chunk_size: int = 6000,
):
    """
    Training of Nanopore VQ tokenizer using Hugging Face Accelerate.

    Key features:
      - Distributed training handled by Accelerate (multi-GPU, multi-node).
      - Mixed precision training support (FP16, BF16).
      - Automatic gradient scaling for FP16.
      - Independent evaluation dataset via `evaluation_npy_dir`.
      - Checkpoint resume support (using Accelerate's state management).
      - Pre-trained CNN weight loading & freezing.
      - Initial codebook initialization (e.g., from Faiss).
      - WandB & CSV logging (main process only).
      - Learning rate scheduling with warmup.
      - Gradient accumulation integrated with Accelerate.

    ‚ö†Ô∏è NOTE ON DWA (Dynamic Weight Averager):
        The DWA module is used SOLELY for monitoring and logging purposes.
        It does NOT influence the actual loss computation or gradient updates.
        The training loss remains:
            total_loss = recon_loss + comit_loss * commitment_weight
        DWA weights are only recorded in logs/CSV/W&B for analysis.
    """
    print_training_args(
        train_npy_dir=train_npy_dir,
        evaluation_npy_dir=evaluation_npy_dir,
        output_model_path=output_model_path,
        lr=lr,
        num_epochs=num_epochs,
        codebook_size=codebook_size,
        chunk_size=chunk_size,
        num_workers=num_workers,
        update_loss_weight_every=update_loss_weight_every,
        prefetch_factor=prefetch_factor,
        val_ratio=val_ratio,
        do_evaluate=do_evaluate,
        commitment_weight=commitment_weight,
        codebook_diversity_loss_weight=codebook_diversity_loss_weight,
        orthogonal_reg_weight=orthogonal_reg_weight,
        loss_csv_path=loss_csv_path,
        use_wandb=use_wandb,
        wandb_project=wandb_project,
        wandb_name=wandb_name,
        lr_scheduler_type=lr_scheduler_type,
        warmup_steps=warmup_steps,
        warmup_start_factor=warmup_start_factor,
        warmup_end_factor=warmup_end_factor,
        main_scheduler_end_factor=main_scheduler_end_factor,
        save_checkpoint_every_spoch=save_checkpoint_every_spoch,
        evaluate_every_spoch=evaluate_every_spoch,
        checkpoint_path=checkpoint_path,
        init_codebook_path=init_codebook_path,
        cnn_type=cnn_type,
        freeze_cnn= freeze_cnn,
        learnable_codebook = learnable_codebook,
        global_batch_size=global_batch_size,
        device_micro_batch_size=device_micro_batch_size,
        mixed_precision=mixed_precision,
        gradient_clipping=gradient_clipping,
        cpu=cpu,
        dataset_logic_chunk_size=dataset_logic_chunk_size,
    )

    # Calculate accumulation steps based on global and micro batch sizes
    effective_micro_batch = device_micro_batch_size * (1 if cpu else torch.cuda.device_count() if torch.cuda.is_available() else 1) # Estimate num_processes before accelerator init
    accumulation_steps = global_batch_size // effective_micro_batch

    if accumulation_steps == 0:
        raise ValueError(
            f"global_batch_size ({global_batch_size}) is too small for the current "
            f"device_micro_batch_size ({device_micro_batch_size}). "
            f"Minimum global_batch_size required is {effective_micro_batch}."
        )

    # Initialize Accelerator
    # This handles device placement, distributed setup, mixed precision, and gradient accumulation automatically.
    accelerator = Accelerator(
        mixed_precision=None,
        cpu=cpu,
        gradient_accumulation_steps=accumulation_steps, # Pass the calculated steps to Acceleratoir
        log_with="wandb", # <-- Enable wandb integration
        project_dir="log", # <--- Ê∑ªÂä†Ëøô‰∏ÄË°åÔºåÊèê‰æõ‰∏Ä‰∏™ÁõÆÂΩïÁªô accelerator ÁÆ°ÁêÜÊó•Âøó
    )
    # Initialise your wandb run, passing wandb parameters and any config information
    accelerator.init_trackers(
        project_name=wandb_project, 
        config={
                "device_micro_batch_size": device_micro_batch_size, # Changed from 'batch_size'
                "lr": lr,
                "num_epochs": num_epochs,
                "codebook_size": codebook_size,
                "chunk_size": chunk_size,
                "update_loss_weight_every": update_loss_weight_every,
                "commitment_weight": commitment_weight,
                "codebook_diversity_loss_weight": codebook_diversity_loss_weight,
                "orthogonal_reg_weight": orthogonal_reg_weight,
                "world_size": accelerator.num_processes, # Changed from 'world_size'
                "mixed_precision": mixed_precision,
                "global_batch_size": global_batch_size,
        },
        init_kwargs={"wandb": {"entity": "jiaoshuaihit-hit","name":wandb_name}}
    )

    # Log accelerator info
    if accelerator.is_main_process:
        print(f"üöÄ Accelerator initialized. Device: {accelerator.device}, Type: {accelerator.distributed_type}")
        print(f"   Number of processes: {accelerator.num_processes}")
        print(f"   Mixed Precision: {accelerator.mixed_precision}")
        print(f"   Global Batch Size: {global_batch_size}, Device Micro-Batch Size: {device_micro_batch_size}")
        print(f"   Gradient Accumulation Steps: {accumulation_steps}")


    # ========================
    # Data Loading
    # ========================
    # DataLoader's batch_size is now the micro-batch size per device/process.
    train_dataset = NanoporeSignalDataset(shards_dir=train_npy_dir,logic_chunk_size=dataset_logic_chunk_size)
    # Accelerate provides a convenient way to create distributed samplers
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=device_micro_batch_size, # Micro-batch size per process
        shuffle=True, # Shuffling is handled by the sampler
        num_workers=num_workers,
        prefetch_factor=prefetch_factor,
        pin_memory=True,
        drop_last=True
    )
    # Prepare dataloader with Accelerate (handles DistributedSampler internally)
    train_dataloader = accelerator.prepare(train_dataloader)

    # ========================
    # Evaluation Setup
    # ========================
    val_loader = None
    def evaluate_codebook_metrics():
        """Evaluate codebook usage, top-k concentration, and entropy on validation set."""
        if val_loader is None:
            return 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0
        # ÂÖ≥ÈîÆÔºöËé∑ÂèñËß£ÂåÖÂêéÁöÑÊ®°Âûã
        #unwrapped_model = accelerator.unwrap_model(model)
        #unwrapped_model.eval()
        model.eval()
        used_codes = set()
        token_counts = np.zeros(codebook_size, dtype=np.int64)
        total_tokens = 0

        with torch.no_grad():
            for batch in val_loader:
                x = batch # No need to call .to(device), Accelerate handles it
                # Ensure model input is correctly shaped if needed (e.g., adding batch dim)
                # Example: if x.shape is [seq_len, feat_dim] and model expects [batch, seq_len, feat_dim]
                # x = x.unsqueeze(0) # This adds the batch dimension
                # However, since we are using batches from DataLoader, x should already have a batch dimension
                # Assuming x shape is [batch_size, seq_len, feat_dim]
                
                # The model expects the correct shape; ensure your dataset/dataloader provides it.
                # If your raw data from dataset has shape [seq_len, feat_dim], DataLoader makes it [device_micro_batch_size, seq_len, feat_dim]
                # which should be fine for a model expecting [batch, seq_len, feat_dim].
                # ÂÖ≥ÈîÆÔºöÁ°Æ‰øùËæìÂÖ•Êï∞ÊçÆÁ±ªÂûã‰∏éÊ®°ÂûãÂèÇÊï∞Á±ªÂûãÂåπÈÖç
                #expected_dtype = next(unwrapped_model.parameters()).dtype
                #if x.dtype != expected_dtype:
                #    x = x.to(expected_dtype)
                print("---")  
                _, indices, _, _ = model(x) # Model input is handled by Accelerate
                print("+++")
                indices = indices.cpu().numpy().flatten()
                used_codes.update(indices.tolist())
                total_tokens += indices.size
                for idx in indices:
                    token_counts[idx] += 1

        usage_ratio = len(used_codes) / codebook_size
        top1_ratio = top3_ratio = top5_ratio = top7_ratio = top9_ratio = top10_ratio = 0.0
        entropy_val = 0.0
        max_entropy = np.log2(codebook_size)

        if total_tokens > 0:
            sorted_counts = np.sort(token_counts)[::-1]

            base_ratio = 1/codebook_size
            top1_ratio = sorted_counts[0] / total_tokens if len(sorted_counts) > 0 else 0.0
            top3_ratio = sorted_counts[2] / total_tokens if len(sorted_counts) > 2 else 0.0
            top5_ratio = sorted_counts[4] / total_tokens if len(sorted_counts) > 4 else 0.0
            top7_ratio = sorted_counts[6] / total_tokens if len(sorted_counts) > 6 else 0.0
            top9_ratio = sorted_counts[8] / total_tokens if len(sorted_counts) > 8 else 0.0

            top1_ratio = top1_ratio/base_ratio
            top3_ratio = top3_ratio/base_ratio
            top5_ratio = top5_ratio/base_ratio
            top7_ratio = top7_ratio/base_ratio
            top9_ratio = top9_ratio/base_ratio

            top10_ratio = float(sorted_counts[:min(9, codebook_size)].sum()) / total_tokens if len(sorted_counts) > 9 else 0.0
            top10_ratio = top10_ratio/base_ratio

            prob = token_counts / total_tokens
            nonzero_prob = prob[prob > 0]
            if nonzero_prob.size > 0:
                entropy_val = -np.sum(nonzero_prob * np.log2(nonzero_prob))
        #unwrapped_model.train()
        model.train()
        return (
            usage_ratio, total_tokens,
            top1_ratio, top3_ratio, top5_ratio, top7_ratio, top9_ratio, top10_ratio,
            entropy_val, max_entropy
        )

    def evaluate_codebook_metrics_v2():
        """Evaluate codebook usage, top-k concentration, and entropy on validation set."""
        if val_loader is None:
            return 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0
        # ÂÖ≥ÈîÆÔºöËé∑ÂèñËß£ÂåÖÂêéÁöÑÊ®°Âûã
        #unwrapped_model = accelerator.unwrap_model(model)
        #unwrapped_model.eval()
        #model.eval()

        # --- ÂÖ≥ÈîÆ‰øÆÊîπ 1: Ëé∑ÂèñÊú™ÂåÖË£ÖÁöÑÊ®°Âûã ---
        # ËøôÊ†∑Ê®°ÂûãÂ∞±ÂèòÊàê‰∫ÜÊôÆÈÄöÊ®°ÂûãÔºå‰∏ç‰ºöÂ∞ùËØïËøõË°å GPU ÈÄö‰ø°
        model_for_eval = accelerator.unwrap_model(model)
        model_for_eval.eval()

        used_codes = set()
        token_counts = np.zeros(codebook_size, dtype=np.int64)
        total_tokens = 0

        with torch.no_grad():
            pbar = tqdm(val_loader, desc=f"Eval")
            for batch in pbar:
                x = batch # No need to call .to(device), Accelerate handles it
                # Ensure model input is correctly shaped if needed (e.g., adding batch dim)
                # Example: if x.shape is [seq_len, feat_dim] and model expects [batch, seq_len, feat_dim]
                # x = x.unsqueeze(0) # This adds the batch dimension
                # However, since we are using batches from DataLoader, x should already have a batch dimension
                # Assuming x shape is [batch_size, seq_len, feat_dim]
                
                # The model expects the correct shape; ensure your dataset/dataloader provides it.
                # If your raw data from dataset has shape [seq_len, feat_dim], DataLoader makes it [device_micro_batch_size, seq_len, feat_dim]
                # which should be fine for a model expecting [batch, seq_len, feat_dim].
                # ÂÖ≥ÈîÆÔºöÁ°Æ‰øùËæìÂÖ•Êï∞ÊçÆÁ±ªÂûã‰∏éÊ®°ÂûãÂèÇÊï∞Á±ªÂûãÂåπÈÖç
                #expected_dtype = next(unwrapped_model.parameters()).dtype
                #if x.dtype != expected_dtype:
                #    x = x.to(expected_dtype)
                #print("---")  
                #_, indices, _, _ = model(x) # Model input is handled by Accelerate
                            # --- ÂÖ≥ÈîÆ‰øÆÊîπ 2: Á°Æ‰øùÊï∞ÊçÆÂú®‰∏ªËøõÁ®ãÁöÑËÆæÂ§á‰∏ä ---
                # ËôΩÁÑ∂ Accelerate ÈÄöÂ∏∏‰ºöÂ§ÑÁêÜÔºå‰ΩÜÊòæÂºèÊåáÂÆöÊõ¥ÂÆâÂÖ®
                x = x.to(accelerator.device)
                # --- ÂÖ≥ÈîÆ‰øÆÊîπ 3: ‰ΩøÁî®Êú™ÂåÖË£ÖÁöÑÊ®°Âûã ---
                # Ëøô‰∏ÄË°å‰∏ç‰ºöÂÜçÂç°‰ΩèÔºåÂõ†‰∏∫ model_for_eval ÊòØÊôÆÈÄöÊ®°Âûã
                _, indices, _, _ = model_for_eval(x)
                #print("+++")
                indices = indices.cpu().numpy().flatten()
                used_codes.update(indices.tolist())
                total_tokens += indices.size
                for idx in indices:
                    token_counts[idx] += 1

        usage_ratio = len(used_codes) / codebook_size
        top1_ratio = top3_ratio = top5_ratio = top7_ratio = top9_ratio = top10_ratio = 0.0
        entropy_val = 0.0
        max_entropy = np.log2(codebook_size)

        if total_tokens > 0:
            sorted_counts = np.sort(token_counts)[::-1]

            base_ratio = 1/codebook_size
            top1_ratio = sorted_counts[0] / total_tokens if len(sorted_counts) > 0 else 0.0
            top3_ratio = sorted_counts[2] / total_tokens if len(sorted_counts) > 2 else 0.0
            top5_ratio = sorted_counts[4] / total_tokens if len(sorted_counts) > 4 else 0.0
            top7_ratio = sorted_counts[6] / total_tokens if len(sorted_counts) > 6 else 0.0
            top9_ratio = sorted_counts[8] / total_tokens if len(sorted_counts) > 8 else 0.0

            top1_ratio = top1_ratio/base_ratio
            top3_ratio = top3_ratio/base_ratio
            top5_ratio = top5_ratio/base_ratio
            top7_ratio = top7_ratio/base_ratio
            top9_ratio = top9_ratio/base_ratio

            top10_ratio = float(sorted_counts[:min(9, codebook_size)].sum()) / total_tokens if len(sorted_counts) > 9 else 0.0
            top10_ratio = top10_ratio/base_ratio

            prob = token_counts / total_tokens
            nonzero_prob = prob[prob > 0]
            if nonzero_prob.size > 0:
                entropy_val = -np.sum(nonzero_prob * np.log2(nonzero_prob))
        model_for_eval.train()
        #model.train()
        return (
            usage_ratio, total_tokens,
            top1_ratio, top3_ratio, top5_ratio, top7_ratio, top9_ratio, top10_ratio,
            entropy_val, max_entropy
        )

    if do_evaluate and accelerator.is_main_process: # Only setup eval on main process initially
        if evaluation_npy_dir and os.path.isdir(evaluation_npy_dir):
            print(f"‚úÖ Using independent evaluation dataset: {evaluation_npy_dir}")
            val_dataset = NanoporeSignalDataset(shards_dir=evaluation_npy_dir,logic_chunk_size=dataset_logic_chunk_size)
        else:
            print(f"‚ö†Ô∏è No evaluation_npy_dir. Using {val_ratio:.1%} of training data for eval.")
            val_dataset = train_dataset
        if val_ratio > 0:
            actual_val_size = max(1, int(val_ratio * len(val_dataset)))
            np.random.seed(42)
            indices = np.random.choice(len(val_dataset), size=actual_val_size, replace=False)
            val_subset = torch.utils.data.Subset(val_dataset, indices)
            val_loader = DataLoader(
                val_subset,
                batch_size=device_micro_batch_size,
                shuffle=False,
                num_workers=max(2, num_workers // 2),
                pin_memory=True,
                drop_last=True
            )
            # Prepare val_loader with Accelerate if needed for consistency
            #val_loader = accelerator.prepare(val_loader) # Often not necessary for eval loader, but can be done
        else:
            val_loader = DataLoader(
                val_dataset,
                batch_size=device_micro_batch_size, # Use same micro-batch size
                shuffle=False,
                num_workers=max(2, num_workers // 2),
                pin_memory=True,
                drop_last=True
            )
            #val_loader = accelerator.prepare(val_loader) # Often not necessary for eval loader
        print(f"val_loader:{len(val_loader)}")


    # ========================
    # Model & Optimizer & Scheduler Preparation
    # ========================
    model = NanoporeVQModel(
        codebook_size=codebook_size,
        commitment_weight=commitment_weight,
        codebook_diversity_loss_weight=codebook_diversity_loss_weight,
        orthogonal_reg_weight=orthogonal_reg_weight,
        cnn_type=cnn_type,
        init_codebook_path=init_codebook_path,
        cnn_checkpoint_path = cnn_checkpoint_path,
        freeze_cnn = freeze_cnn,
        learnable_codebook=learnable_codebook
    )
    # No need to manually call .to(device) or wrap with DDP, Accelerate handles it
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=lr,
    )

    # Prepare model, optimizer, and scheduler with Accelerate
    model, optimizer = accelerator.prepare(model, optimizer)


    # Calculate total training steps correctly
    # len(train_dataloader) after prepare gives micro-batch count per epoch
    total_training_global_steps = num_epochs * len(train_dataloader) // accumulation_steps 

    # Scheduler setup (Accelerate requires it to be prepared after prepare(optimizer))
    scheduler = None
    # total_training_global_steps = int(len(train_dataloader) * num_epochs / global_batch_size) # <--- Remove or comment out old calculation

    if accelerator.is_main_process and lr_scheduler_type != "constant":
        print(f"üìà Using LR scheduler: {lr_scheduler_type}, warmup_steps={warmup_steps}")
        print(f"üìà Calculated total_training_global_steps: {total_training_global_steps}") # Optional: Add print to confirm
        print(f"üìà Calculated num_epochs: {num_epochs}") # Optional: Add print to confirm
        print(f"üìà Calculated accumulation_steps: {accumulation_steps}") # Optional: Add print to confirm
        print(f"Ë∞ÉËØï‰ø°ÊÅØ:")
        print(f"  len(train_dataloader) = {len(train_dataloader)}")
        print(f"  accumulation_steps = {accumulation_steps}")
        print(f"  num_epochs = {num_epochs}")
        print(f"  ËÆ°ÁÆóÁöÑÊÄªÊ≠•Êï∞ = {total_training_global_steps}")

    # Êàë‰ª¨ÊåáÂÆöÁöÑwarmup_stepsÊòØschedulerÂèÇÊï∞Êõ¥Êñ∞ÁöÑÊ¨°Êï∞ÔºåËÄå‰∏çÊòØÁúüÊ≠£ÁöÑstepsÔºåÊâÄ‰ª•

    if lr_scheduler_type != "constant":

        # Âõ†‰∏∫accelerateÁöÑË∞ÉÂ∫¶ÂíåÂπ∂Ë°åÂ∫¶ÊúâÂÖ≥ÔºåÊØèÊ¨°Ë∞ÉÂ∫¶ÂÖ∂ÂÆûÂêåÊó∂ËøêË°å‰∫ÜÂ§öÊ¨°
        # ÊâÄ‰ª•Ë¶Å‰πò‰ª•rankÊï∞


        from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR
        
        # Ensure warmup_steps does not exceed total steps to prevent errors
        actual_warmup_steps = min(warmup_steps, total_training_global_steps)
        main_steps = max(1, total_training_global_steps - actual_warmup_steps)

        actual_warmup_steps = actual_warmup_steps * accelerator.num_processes
        total_training_global_steps = total_training_global_steps * accelerator.num_processes

        warmup_scheduler = LinearLR(
            optimizer, 
            start_factor=warmup_start_factor, 
            end_factor=warmup_end_factor, 
            total_iters=actual_warmup_steps
        )
        
        if lr_scheduler_type == "cosine":
            eta_min = lr * main_scheduler_end_factor # e.g., 5e-5 * 1e-6 = 5e-11
            main_scheduler = CosineAnnealingLR(
                optimizer, 
                T_max=main_steps, 
                eta_min=eta_min
            )
        elif lr_scheduler_type == "linear":
            # Adjust end factor relative to warmup end factor for LinearLR continuity
            relative_end_factor = max(1e-8, min(1.0, main_scheduler_end_factor / warmup_end_factor)) if warmup_end_factor != 0 else main_scheduler_end_factor
            main_scheduler = LinearLR(
                optimizer, 
                start_factor=1.0, 
                end_factor=relative_end_factor, 
                total_iters=main_steps
            )
        else:
            raise ValueError(f"Unsupported lr_scheduler_type: {lr_scheduler_type}")

        # Combine warmup and main scheduler
        scheduler = SequentialLR(
            optimizer, 
            schedulers=[warmup_scheduler, main_scheduler], 
            milestones=[actual_warmup_steps]
        )
        
        # Prepare scheduler with Accelerate
        scheduler = accelerator.prepare(scheduler)
    # ========================
    # Resume from Checkpoint
    # ========================
    start_epoch = start_spoch = start_global_step = 0
    if checkpoint_path and os.path.isfile(checkpoint_path):
        if accelerator.is_main_process:
            print(f"üì• Loading checkpoint from: {checkpoint_path}")
        # Load state using Accelerate. This loads model, optimizer, scheduler, and RNG states.
        accelerator.load_state(checkpoint_path)
        # Load metadata if available
        metadata = load_checkpoint_metadata(checkpoint_path)
        if metadata:
            start_epoch = metadata.get('epoch', -1) + 1
            start_spoch = metadata.get('spoch', -1) + 1
            start_global_step = metadata.get('global_step', 0)
            if accelerator.is_main_process:
                print(f"‚úÖ Resuming from epoch {start_epoch}, spoch {start_spoch}, global_step {start_global_step}")
        else:
            if accelerator.is_main_process:
                print("‚ö†Ô∏è Checkpoint loaded, but no metadata found. Starting from scratch.")
    if  accelerator.is_main_process: 
        print(f"Gradient Accumulation Steps: {accumulation_steps}")
        print(f"Num Processes (GPUs): {accelerator.num_processes}")
        print(f"actual_warmup_steps: {actual_warmup_steps}")

    # ========================
    # Gradient Accumulation Setup (Handled by Accelerator)
    # ========================
    # Calculation moved to before Accelerator init for logging
    # The accumulation_steps value is passed to Accelerator during initialization.
    # No further manual setup needed here.
    # Print configuration AFTER accelerator.prepare
    # ========================
    # Training Loop
    # ========================
    model.train()
    global_step = start_global_step
    spoch = start_spoch
    total_steps = len(train_dataloader) * num_epochs
    total_global_steps = total_training_global_steps
    total_spochs = total_steps // update_loss_weight_every
    # Âú®ËÆ≠ÁªÉÂæ™ÁéØÂºÄÂßãÂâç
    if accelerator.is_main_process:
        print(f"\n=== Ë∞ÉÂ∫¶Âô®ËØ¶ÁªÜË∞ÉËØï ===")
        print(f"ÈÖçÁΩÆÂÄº:")
        print(f"  lr: {lr}")
        print(f"  warmup_start_factor: {warmup_start_factor}")
        print(f"  warmup_end_factor: {warmup_end_factor}")
        print(f"  warmup_steps (ÈÖçÁΩÆ): {warmup_steps}")
        print(f"  actual_warmup_steps (ËÆ°ÁÆó): {actual_warmup_steps}")
        
        # Ê£ÄÊü•Ë∞ÉÂ∫¶Âô®ÁöÑ warmup ÈÉ®ÂàÜ
        if hasattr(scheduler, '_schedulers'):
            warmup_sched = scheduler._schedulers[0]
            print(f"\nWarmupË∞ÉÂ∫¶Âô®:")
            print(f"  Á±ªÂûã: {type(warmup_sched)}")
            print(f"  start_factor: {warmup_sched.start_factor}")
            print(f"  end_factor: {warmup_sched.end_factor}")
            print(f"  total_iters: {warmup_sched.total_iters}")
        
        print(f"\nÂàùÂßãÂ≠¶‰π†Áéá: {scheduler.get_last_lr()[0]:.6e}")
        print("=== Ë∞ÉËØïÁªìÊùü ===\n")
    log_dict = {}

    wandb_codebook_usage = 0
    wandb_codebook_top1_ratio = 0 
    wandb_codebook_top3_ratio = 0
    wandb_codebook_top5_ratio = 0
    wandb_codebook_top7_ratio = 0
    wandb_codebook_top9_ratio = 0
    wandb_codebook_top10_ratio = 0 
    wandb_codebook_entropy = 0
    wandb_codebook_max_entropy = 0

    train_start_time = time.time()
    for epoch in range(start_epoch, num_epochs):
        # Set epoch for the sampler if applicable (handled by Accelerate)
        train_dataloader.sampler.set_epoch(epoch) if hasattr(train_dataloader.sampler, 'set_epoch') else None

        # Zero grad at the beginning of an epoch (or more precisely, before the first accumulation cycle)
        # Accelerate's optimizer handles the zero_grad call internally when needed,
        # but for explicit control and clarity, especially with manual accumulation, it's good practice.
        # However, Accelerate often calls zero_grad automatically *before* the first backward() in a new step cycle.
        # The safest place is right before the first backward() call in a new accumulation cycle.
        # We'll handle this inside the loop based on the accumulation logic.
        # With accelerator.accumulate, zero_grad is handled automatically at the start of each accum cycle.
        for step, batch in enumerate(train_dataloader):
            # Use Accelerate's accumulate context manager
            with accelerator.accumulate(model):
                # ËøôÈáåÈù¢ÁöÑ‰ª£Á†ÅÂú®ÊØè‰∏™ micro-batch ÈÉΩ‰ºöÊâßË°å
                # ÂåÖÊã¨ÂâçÂêë‰º†Êí≠„ÄÅÂèçÂêë‰º†Êí≠
                # No need to manually move batch to device
                x = batch
                # Forward pass
                recon, indices, break_loss, loss_breakdown = model(x)
                recon_loss = F.mse_loss(recon, x)
                comit_loss = loss_breakdown.commitment
                diver_loss = loss_breakdown.codebook_diversity
                ortho_loss = loss_breakdown.orthogonal_reg
                # üí° ACTUAL LOSS: Fixed weights. DWA is NOT applied here.
                total_loss = recon_loss + comit_loss * commitment_weight

                # Scale the loss by the number of accumulation steps for averaging
                # Accelerate's backward function handles gradient scaling for mixed precision automatically
                # The loss is automatically scaled by 1/accumulation_steps inside the context if needed.
                accelerator.backward(total_loss) # Pass the unscaled loss
                # Âè™Âú®Ê¢ØÂ∫¶Á¥ØÁßØÂÆåÊàêÂêéÊâßË°å‰ºòÂåñÂô®Ê≠•È™§
                #if accelerator.sync_gradients:
                # ÊâßË°åÊ¢ØÂ∫¶Ë£ÅÂâ™ÔºàÂ¶ÇÊûúÂêØÁî®Ôºâ
                #if gradient_clipping is not None:
                #    accelerator.clip_grad_norm_(model.parameters(), gradient_clipping)
                # ‰ΩøÁî® Accelerate ÁöÑÊñπÊ≥ïÊâßË°å‰ºòÂåñÂô®Ê≠•È™§
                optimizer.step() 
                scheduler.step()
                optimizer.zero_grad()
                
                     # ÈáçË¶ÅÔºöËøôÈáå‰∏çÈúÄË¶ÅË∞ÉÁî® optimizer.zero_grad()ÔºåAccelerate ‰ºöËá™Âä®Â§ÑÁêÜ
            # Determine if it's time to update weights (based on effective steps)
            # We use accelerator.sync_gradients to know when the optimizer step was just performed
            
            # --- Post-update operations (logging, evaluation, checkpointing) ---
            if accelerator.sync_gradients:
                # These happen after a full parameter update
                global_step += 1 # Increment global step for every micro-step processed


                g_recon = recon_loss.item() # Get value from last micro-step
                g_comit = comit_loss.item()
                g_ortho = ortho_loss.item()
                g_diver = diver_loss.item()
                g_total = total_loss.item()




                # Log metrics (main process only)
                if accelerator.is_main_process:
                    current_lr = optimizer.param_groups[0]['lr']
                    log_and_save(
                        epoch=epoch,
                        global_step=global_step,
                        total_epochs=num_epochs,
                        total_global_steps=total_global_steps,
                        train_start_time=train_start_time,
                        epoch_total_steps=len(train_dataloader),
                        avg_recon_loss=g_recon,
                        avg_total_loss=g_total,
                        avg_comit_loss=g_comit,
                        avg_diver_loss=g_diver,
                        avg_ortho_loss=g_ortho,
                        codebook_usage=wandb_codebook_usage, # Placeholder, updated later if eval runs
                        loss_csv_path=loss_csv_path,
                        lr=current_lr,
                        accelerator=accelerator # Pass accelerator instance
                    )

                    # Prepare log dict for WandB
                    log_dict.update({
                        "train/recon_loss": g_recon,
                        "train/comit_loss": g_comit,
                        "train/ortho_loss": g_ortho,
                        "train/diver_loss": g_diver,
                        "train/total_loss": g_total,
                        "codebook/usage": wandb_codebook_usage, # Placeholder
                        "codebook/entropy": wandb_codebook_entropy, # Placeholder
                        "codebook/max_entropy": wandb_codebook_max_entropy, # Placeholder
                        "topcode/top1_ratio": wandb_codebook_top1_ratio, # Placeholder
                        "topcode/top3_ratio": wandb_codebook_top3_ratio, # Placeholder
                        "topcode/top5_ratio": wandb_codebook_top5_ratio, # Placeholder
                        "topcode/top7_ratio": wandb_codebook_top7_ratio, # Placeholder
                        "topcode/top9_ratio": wandb_codebook_top9_ratio, # Placeholder
                        "topcode/topx_ratio": wandb_codebook_top10_ratio, # Placeholder
                        "learning_rate": current_lr,
                        "epoch": epoch + 1,
                        "global_step": global_step, # Useful for plotting against global steps
                    })
                    accelerator.log(log_dict, step=global_step) # <--- Ê∑ªÂä†Ëøô‰∏ÄË°å
                # --- Evaluation ---
                # Run evaluation based on spoch
                if global_step % evaluate_every_spoch == 0:
                    accelerator.wait_for_everyone() # Sync all processes before eval
                    if accelerator.is_main_process :
                        try:
                            result = evaluate_codebook_metrics_v2()
                            if accelerator.is_main_process:
                                (wandb_codebook_usage, 
                                    total_tokens, 
                                   wandb_codebook_top1_ratio, 
                                   wandb_codebook_top3_ratio,
                                   wandb_codebook_top5_ratio, 
                                   wandb_codebook_top7_ratio, 
                                   wandb_codebook_top9_ratio,
                                   wandb_codebook_top10_ratio, 
                                   wandb_codebook_entropy, 
                                   wandb_codebook_max_entropy) = result
                                print(f"Effective Step {global_step} - Codebook Usage: {wandb_codebook_usage:.2%}")
                        except Exception as e:
                            print(f"Error during evaluation: {e}")
                    else:
                        wandb_codebook_usage = 0.0 # Default value if eval doesn't run
                    accelerator.wait_for_everyone() # Sync all processes after eval

                # Periodic checkpointing (main process only)
                if accelerator.is_main_process and global_step  % save_checkpoint_every_spoch == 0:
                    ckpt_path = f"{output_model_path}.step{global_step}.pth"
                    save_full_checkpoint(
                        path=ckpt_path,
                        model=model,
                        optimizer=optimizer,
                        scheduler=scheduler, # Pass scheduler state
                        epoch=epoch,
                        spoch=spoch,
                        global_step=global_step,
                        cnn_type=cnn_type,
                        accelerator=accelerator # Pass accelerator instance
                    )
                # --- End of if accelerator.sync_gradients block ---
                # --- End of with accelerator.accumulate(model) block ---
                # optimizer.step() and optimizer.zero_grad() are called automatically by Accelerate inside the context
                # when accelerator.sync_gradients becomes True (i.e., at the end of an accumulation cycle).


    # Final save (main process only)
    if accelerator.is_main_process:
        save_full_checkpoint(
            path=output_model_path,
            model=model,
            optimizer=optimizer,
            scheduler=scheduler,
            epoch=num_epochs - 1,
            spoch=spoch,
            global_step=global_step,
            cnn_type=cnn_type,
            accelerator=accelerator
        )
    # Clean up Accelerator resources
    accelerator.end_training()


# =============================================================================
# CLI Entry Point
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description="Train Nanopore VQ Tokenizer with Accelerate")
    parser.add_argument("--config", type=str, required=True, help="Path to the YAML configuration file")
    args = parser.parse_args()

    # Load configuration from YAML file
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # Pass the configuration dictionary values to vqe_train
    # Added new Accelerate-specific arguments
    vqe_train(
        train_npy_dir=config.get("train_npy_dir"),
        evaluation_npy_dir=config.get("evaluation_npy_dir"),
        output_model_path=config.get("output_model_path", "nanopore_vq_tokenizer.pth"),
        lr=config.get("lr", 3e-4),
        num_epochs=config.get("num_epochs", 10),
        codebook_size=config.get("codebook_size", 8192),
        chunk_size=config.get("chunk_size", 12000),
        num_workers=config.get("num_workers", 8),
        val_ratio=config.get("val_ratio", 0.1),
        do_evaluate=config.get("do_evaluate", False),
        commitment_weight=config.get("commitment_weight", 0.25),
        codebook_diversity_loss_weight=config.get("codebook_diversity_loss_weight", 0.0),
        orthogonal_reg_weight=config.get("orthogonal_reg_weight", 0.0),
        loss_csv_path=config.get("loss_csv_path", "train_loss.csv"),
        save_checkpoint_every_spoch=config.get("save_checkpoint_every_spoch", 10),
        loss_log_interval=config.get("loss_log_interval", 10), # Note: This logic was replaced by update_loss_weight_every
        checkpoint_path=config.get("checkpoint_path"),
        cnn_type=config.get("cnn_type", 1),
        init_codebook_path=config.get("init_codebook_path", ""),
        cnn_checkpoint_path=config.get("cnn_checkpoint_path", ""),
        freeze_cnn=config.get("freeze_cnn", 0),
        learnable_codebook=config.get("learnable_codebook", True),
        global_batch_size=config.get("global_batch_size", 256),
        device_micro_batch_size=config.get("device_micro_batch_size", 16), # Passed again for clarity
        # Accelerate specific arguments
        mixed_precision=config.get("mixed_precision", "bf16"), # Options: "no", "fp16", "bf16"
        gradient_clipping=config.get("gradient_clipping", 1.0), # Set to None to disable
        cpu=config.get("cpu", False), # Force CPU training
        # Add other parameters that might be in the YAML but not explicitly listed above
        update_loss_weight_every=config.get("update_loss_weight_every", 10),
        prefetch_factor=config.get("prefetch_factor", 128),
        use_wandb=config.get("use_wandb", True),
        wandb_project=config.get("wandb_project", "nanopore_vq"),
        wandb_name=config.get("wandb_name", "default_run"),
        lr_scheduler_type=config.get("lr_scheduler_type", "cosine"),
        warmup_steps=config.get("warmup_steps", 100),
        warmup_start_factor=config.get("warmup_start_factor", 1e-5),
        warmup_end_factor=config.get("warmup_end_factor", 1.0),
        main_scheduler_end_factor=config.get("main_scheduler_end_factor", 1e-5),
        evaluate_every_spoch=config.get("evaluate_every_spoch", 10),
        dataset_logic_chunk_size=config.get("dataset_logic_chunk_size",6000)
    )

if __name__ == "__main__":
    main()
