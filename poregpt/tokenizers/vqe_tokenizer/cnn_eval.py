# -*- coding: utf-8 -*-
"""
Feature Extraction Pipeline for Nanopore Signal Tokenization

Purpose:
    ä»åŸå§‹çº³ç±³å­”ç”µä¿¡å·ä¸­æå–åŸºäº CNN çš„ token-level ç‰¹å¾ï¼ˆembeddingsï¼‰ï¼Œ
    æ¯ä¸ªè¾“å…¥æ ·æœ¬å°†è¢«è½¬æ¢ä¸º T ä¸ª 64 ç»´ç‰¹å¾å‘é‡ï¼ˆT ç”±æ¨¡å‹ç»“æ„å†³å®šï¼Œå¦‚ 2400ï¼‰ï¼Œ
    æ‰€æœ‰ç‰¹å¾æŒ‰ token ç²’åº¦åˆ†ç‰‡å­˜å‚¨ä¸º .npy æ–‡ä»¶ï¼Œå¹¶é™„å¸¦ç»“æ„åŒ–å…ƒæ•°æ®ï¼ˆshards.jsonï¼‰ã€‚

Design Philosophy:
    - æ”¯æŒå¤§è§„æ¨¡æ•°æ®ï¼ˆTB çº§ï¼‰ï¼šé€šè¿‡å†…å­˜æ˜ å°„ï¼ˆmemmapï¼‰é¿å… OOMï¼›
    - å¯æ¢å¤æ€§ï¼šæ¯ä¸ª shard ç‹¬ç«‹ï¼Œæ”¯æŒä¸­æ–­åç»­è·‘ï¼›
    - å…ƒæ•°æ®å®Œå¤‡ï¼šè®°å½•æ ·æœ¬æ•°ã€token æ•°ã€ç»´åº¦ã€åˆ†ç‰‡è¾¹ç•Œç­‰ï¼Œä¾¿äºä¸‹æ¸¸åŠ è½½ï¼›
    - ä¸è®­ç»ƒ/å»ºæ¨¡æµç¨‹è§£è€¦ï¼šä»…ä¾èµ– checkpoint å’Œè¾“å…¥ shard ç›®å½•ã€‚

Input:
    - input_shards_dir: åŸå§‹ä¿¡å·åˆ†ç‰‡ç›®å½•ï¼ˆç”± dataset.NanoporeSignalDataset è¯»å–ï¼‰
    - checkpoint_path: è®­ç»ƒå¥½çš„ CNN æ¨¡å‹æƒé‡è·¯å¾„

Output:
    - output_shard_dir/
        â”œâ”€â”€ shard_00000.npy   # shape: [N, 64], float32
        â”œâ”€â”€ shard_00001.npy
        â””â”€â”€ shards.json       # å…ƒæ•°æ®ç´¢å¼•æ–‡ä»¶

Note:
    æ¨¡å‹è¾“å‡ºæ ¼å¼ä¸º [B, C, T]ï¼ˆPyTorch Conv1d æ ‡å‡† NCL æ ¼å¼ï¼‰ï¼Œ
    æœ¬è„šæœ¬å°†å…¶è½¬ç½®ä¸º [B, T, C] åå±•å¹³ä¸º [B*T, C]ï¼Œä»¥ç¬¦åˆ token åºåˆ—æƒ¯ä¾‹ã€‚
"""

import os
import json
import torch
import numpy as np
from torch.utils.data import DataLoader
from tqdm import tqdm
import argparse

# å¯¼å…¥æœ¬åœ°æ¨¡å—ï¼šCNN æ¨¡å‹å®šä¹‰ + ä¿¡å·æ•°æ®é›†
from .cnn_model import NanoporeCNNModel
from .dataset import NanoporeSignalDataset


def load_trained_cnn(checkpoint_path: str, cnn_type: int, device: str):
    """
    åŠ è½½é¢„è®­ç»ƒçš„ Nanopore CNN æ¨¡å‹ï¼ˆæ”¯æŒå•å¡ / DDP æƒé‡å…¼å®¹ï¼‰

    Args:
        checkpoint_path (str): æ¨¡å‹ checkpoint è·¯å¾„ï¼Œéœ€åŒ…å« 'model_state_dict'
        cnn_type (int): æ¨¡å‹é…ç½®ç±»å‹ï¼ˆç”¨äºåˆå§‹åŒ– NanoporeCNNModelï¼‰
        device (str): æ¨ç†è®¾å¤‡ï¼ˆ'cpu' æˆ– 'cuda'ï¼‰

    Returns:
        torch.nn.Module: å·²åŠ è½½æƒé‡å¹¶åˆ‡æ¢è‡³ eval æ¨¡å¼çš„æ¨¡å‹

    Notes:
        - è‡ªåŠ¨å¤„ç† DDP è®­ç»ƒä¿å­˜çš„æƒé‡ï¼ˆkey å‰ç¼€ 'module.'ï¼‰
        - ä¸åŠ è½½ä¼˜åŒ–å™¨ç­‰æ— å…³çŠ¶æ€ï¼Œä»…åŠ è½½æ¨¡å‹å‚æ•°
    """
    # åˆå§‹åŒ–æ¨¡å‹ç»“æ„ï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    model = NanoporeCNNModel(cnn_type=cnn_type)

    # å®‰å…¨åŠ è½½ checkpointï¼ˆweights_only=False å…¼å®¹æ—§ç‰ˆ PyTorchï¼‰
    ckpt = torch.load(checkpoint_path, map_location=device, weights_only=False)
    state_dict = ckpt['model_state_dict']

    # å…¼å®¹ DDPï¼ˆDistributedDataParallelï¼‰è®­ç»ƒä¿å­˜çš„æƒé‡
    if state_dict and list(state_dict.keys())[0].startswith('module.'):
        # ç§»é™¤ 'module.' å‰ç¼€ä»¥é€‚é…å•å¡æ¨ç†
        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}

    # åŠ è½½æƒé‡ï¼ˆä¸¥æ ¼åŒ¹é…ï¼‰
    model.load_state_dict(state_dict)

    # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼ˆå…³é—­ dropout / BN æ›´æ–°ï¼‰
    model.to(device).eval()

    print(f"âœ… Loaded model from {checkpoint_path}")
    return model


def cnn_eval(
    input_shards_dir: str,
    output_shard_dir: str,
    checkpoint_path: str,
    shard_size: int = 1_000_000,
    feature_dim: int = 64,
    batch_size: int = 128,
    num_workers: int = 8,
    cnn_type: int = 1,
    device: str = 'cuda',
):
    """
    ä¸»ç‰¹å¾æå–å‡½æ•°ï¼šé€ batch æå– token-level embeddings å¹¶åˆ†ç‰‡æŒä¹…åŒ–

    Workflow:
        1. åŠ è½½æ¨¡å‹ä¸æ•°æ®é›†
        2. æ¨æ–­æ—¶é—´æ­¥é•¿ Tï¼ˆä»æ¨¡å‹è¾“å‡ºåŠ¨æ€è·å–ï¼‰
        3. éå†æ•°æ®é›†ï¼Œå¯¹æ¯ä¸ª batchï¼š
            a. å‰å‘ä¼ æ’­å¾—åˆ° [B, C, T] ç‰¹å¾
            b. è½¬ç½®ä¸º [B, T, C] â†’ å±•å¹³ä¸º [B*T, C]
            c. é€ token å†™å…¥ memmap åˆ†ç‰‡æ–‡ä»¶
        4. ç”Ÿæˆ shards.json å…ƒæ•°æ®

    Args:
        input_shards_dir (str): è¾“å…¥ä¿¡å·åˆ†ç‰‡ç›®å½•ï¼ˆéœ€ç¬¦åˆ NanoporeSignalDataset æ ¼å¼ï¼‰
        output_shard_dir (str): è¾“å‡ºç‰¹å¾åˆ†ç‰‡ç›®å½•ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰
        checkpoint_path (str): æ¨¡å‹æƒé‡è·¯å¾„
        shard_size (int): æ¯ä¸ªåˆ†ç‰‡æœ€å¤šå®¹çº³çš„ token æ•°é‡ï¼ˆéæ ·æœ¬æ•°ï¼ï¼‰
        feature_dim (int): æœŸæœ›çš„ç‰¹å¾ç»´åº¦ï¼ˆåº”ä¸æ¨¡å‹è¾“å‡ºé€šé“æ•°ä¸€è‡´ï¼‰
        batch_size (int): æ¨ç†æ‰¹å¤§å°ï¼ˆå½±å“ GPU æ˜¾å­˜å’Œååï¼‰
        num_workers (int): DataLoader å­è¿›ç¨‹æ•°ï¼ˆåŠ é€Ÿ I/Oï¼‰
        cnn_type (int): æ¨¡å‹ç±»å‹æ ‡è¯†ï¼ˆç”¨äºåˆå§‹åŒ–ï¼‰
        device (str): æ¨ç†è®¾å¤‡

    Side Effects:
        - åœ¨ output_shard_dir ä¸‹åˆ›å»º .npy åˆ†ç‰‡æ–‡ä»¶å’Œ shards.json
        - æ‰“å°è¿›åº¦æ—¥å¿—åˆ° stdout
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_shard_dir, exist_ok=True)

    # === Step 1: åŠ è½½æ¨¡å‹ä¸æ•°æ®é›† ===
    model = load_trained_cnn(checkpoint_path, cnn_type, device)
    dataset = NanoporeSignalDataset(shards_dir=input_shards_dir)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=False,          # å¿…é¡»ä¸º False ä»¥ä¿è¯æ ·æœ¬é¡ºåºå¯å¤ç°
        prefetch_factor=64,
        num_workers=num_workers,
        pin_memory=True,        # åŠ é€Ÿ GPU æ•°æ®ä¼ è¾“
        drop_last=False         # ä¿ç•™æœ€åä¸å®Œæ•´ batch
    )

    total_samples = len(dataset)
    print(f"ğŸ“Š Total samples: {total_samples:,}")

    # === Step 2: åŠ¨æ€æ¢æµ‹æ¨¡å‹è¾“å‡ºçš„æ—¶é—´æ­¥é•¿ T ===
    # æ³¨æ„ï¼šæ­¤å¤„ä»…ç”¨ç¬¬ä¸€ä¸ª batch çš„ç¬¬ä¸€ä¸ªæ ·æœ¬æ¢æµ‹ï¼Œé¿å…é‡å¤è®¡ç®—
    with torch.no_grad():
        for batch in dataloader:
            x = batch.to(device)
            feat = model.encode(x[:1])  # shape: [1, C, T]
            # éªŒè¯è¾“å‡ºä¸º 3D ä¸”é€šé“æ•°åŒ¹é…
            assert feat.dim() == 3, f"Expected 3D output from model.encode(), got {feat.shape}"
            assert feat.shape[1] == feature_dim, (
                f"Channel dimension mismatch: expected {feature_dim}, got {feat.shape[1]}"
            )
            T = feat.shape[2]  # æ—¶é—´æ­¥é•¿ï¼ˆå¦‚ 2400ï¼‰
            break  # ä»…éœ€ä¸€æ¬¡æ¢æµ‹

    total_tokens = total_samples * T
    print(f"ğŸ” Feature dim: {feature_dim}, Time steps per sample: {T}")
    print(f"ğŸ”¢ Total tokens to extract: {total_tokens:,}")

    # === Step 3: åˆå§‹åŒ–ç¼“å†²ä¸åˆ†ç‰‡çŠ¶æ€ ===
    buffer = []                 # List of [C,] arrays, will be stacked when flushed
    shard_index = 0
    global_token_idx = 0
    shards_info = []

    pbar = tqdm(total=total_tokens, desc="Extracting features (by token)")

    def _flush_buffer():
        """å°†å½“å‰ buffer ä¸€æ¬¡æ€§å†™å…¥æ–° shard æ–‡ä»¶"""
        nonlocal buffer, shard_index, global_token_idx, shards_info

        if not buffer:
            return

        current_shard_size = len(buffer)
        shard_file = f"shard_{shard_index:05d}.npy"
        shard_path = os.path.join(output_shard_dir, shard_file)

        # Stack into [N, C]
        shard_data = np.stack(buffer, axis=0).astype(np.float32)

        # Write via memmap (efficient and compatible)
        memmap = np.memmap(
            shard_path,
            dtype='float32',
            mode='w+',
            shape=shard_data.shape
        )
        memmap[:] = shard_data
        del memmap  # Ensure flush

        # Record metadata
        shards_info.append({
            "shard_file": shard_file,
            "start_token_index": global_token_idx,
            "num_tokens": current_shard_size,
            "shape": [current_shard_size, feature_dim]
        })

        print(f"ğŸ†• Created: {shard_path} ({current_shard_size} tokens)")
        global_token_idx += current_shard_size
        buffer.clear()
        shard_index += 1

    # === Step 4: ä¸»æ¨ç†ä¸å†™å…¥å¾ªç¯ ===
    with torch.no_grad():
        for batch in dataloader:
            x = batch.to(device)
            feats = model.encode(x)                     # [B, C, T]
            feats = feats.permute(0, 2, 1)              # [B, T, C]
            feats = feats.reshape(-1, feature_dim)      # [B*T, C]
            feats_np = feats.cpu().numpy().astype(np.float32)
            num_tokens = feats_np.shape[0]

            # Add all tokens to buffer
            for i in range(num_tokens):
                buffer.append(feats_np[i])
                pbar.update(1)

                # Flush if buffer reaches shard_size
                if len(buffer) >= shard_size:
                    _flush_buffer()

        # Flush remaining tokens
        if buffer:
            _flush_buffer()

    # === Step 5: ä¿å­˜å…¨å±€å…ƒæ•°æ® ===
    shards_json_path = os.path.join(output_shard_dir, "shards.json")
    with open(shards_json_path, 'w') as f:
        json.dump({
            "total_samples": total_samples,
            "tokens_per_sample": T,
            "total_tokens": total_tokens,
            "feature_dim": feature_dim,
            "shard_size_max_tokens": shard_size,
            "shards": shards_info
        }, f, indent=2)

    # === Final Summary ===
    print(f"\nâœ… Done! Features saved to: {output_shard_dir}/")
    print(f"ğŸ“„ Index file: {shards_json_path}")
    print(f"ğŸ”¢ Total shards: {len(shards_info)}")
    print(f"ğŸ§© Each sample contributes {T} tokens of dim {feature_dim}")


if __name__ == "__main__":
    """
    å‘½ä»¤è¡Œå…¥å£ï¼šè§£æå‚æ•°å¹¶å¯åŠ¨ç‰¹å¾æå–
    """
    parser = argparse.ArgumentParser(
        description="Extract token-level CNN features from nanopore signal shards."
    )
    parser.add_argument("--input_shards_dir", type=str, required=True,
                        help="Directory containing input signal shards (NanoporeSignalDataset format)")
    parser.add_argument("--output_shard_dir", type=str, required=True,
                        help="Output directory for feature shards")
    parser.add_argument("--checkpoint_path", type=str, required=True,
                        help="Path to trained CNN checkpoint (.pth)")
    parser.add_argument("--shard_size", type=int, default=1_000_000,
                        help="Max number of tokens per output shard (default: 1M)")
    parser.add_argument("--feature_dim", type=int, default=64,
                        help="Expected feature dimension (must match model output channels)")
    parser.add_argument("--batch_size", type=int, default=128,
                        help="Inference batch size (adjust based on GPU memory)")
    parser.add_argument("--num_workers", type=int, default=8,
                        help="Number of DataLoader workers for I/O parallelism")
    parser.add_argument("--cnn_type", type=int, default=1,
                        help="Model configuration identifier (passed to NanoporeCNNModel)")
    parser.add_argument("--device", type=str, default="cuda",
                        help="Device for inference ('cuda' or 'cpu')")

    args = parser.parse_args()
    cnn_eval(**vars(args))
